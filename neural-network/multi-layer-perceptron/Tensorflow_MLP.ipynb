{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = bytes(\"<stripped %d bytes>\"%size, 'utf-8')\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, graph, dim_input, dim_hiddens, dim_output):\n",
    "        self.graph = graph\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_hiddens = dim_hiddens\n",
    "        self.dim_output = dim_output\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        self._x = None\n",
    "        self._t = None\n",
    "        self._y = None\n",
    "\n",
    "        self._loss = None\n",
    "        self._train_step = None\n",
    "        self._accuracy = None\n",
    "        \n",
    "        self._sess = None\n",
    "        \n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "    \n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape)\n",
    "        return tf.Variable(initial, name='W')\n",
    "    \n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial, name='b')\n",
    "    \n",
    "    def inference(self, x):\n",
    "        for i, dim_hidden in enumerate(self.dim_hiddens):\n",
    "            with tf.name_scope('Hidden_Layer_{0}'.format(i+1)):\n",
    "                if i == 0:\n",
    "                    input = x\n",
    "                    dim_input = self.dim_input\n",
    "                else:\n",
    "                    input = output\n",
    "                    dim_input = self.dim_hiddens[i-1]\n",
    "\n",
    "                self.weights.append(self.weight_variable([dim_input, dim_hidden]))\n",
    "                self.biases.append(self.bias_variable([dim_hidden]))\n",
    "\n",
    "                output = tf.nn.sigmoid(tf.matmul(input, self.weights[-1]) + self.biases[-1])\n",
    "\n",
    "        with tf.name_scope('Output_Layer'):\n",
    "            self.weights.append(self.weight_variable([self.dim_hiddens[-1], self.dim_output]))\n",
    "            self.biases.append(self.bias_variable([self.dim_output]))\n",
    "\n",
    "            y = tf.nn.sigmoid(tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "\n",
    "        return y\n",
    "\n",
    "    def loss(self, y, t):\n",
    "        with tf.name_scope('Loss'):\n",
    "            cross_entropy = - tf.reduce_sum(t * tf.log(y) + (1 - t) * tf.log(1 - y))\n",
    "        \n",
    "            return cross_entropy\n",
    "    \n",
    "    def training(self, loss, lr=0.1):\n",
    "        with tf.name_scope('Train'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            train_step = optimizer.minimize(loss)\n",
    "            return train_step\n",
    "    \n",
    "    def accuracy(self, y, t):\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            correct_prediction = tf.equal(tf.to_float(tf.greater(y, 0.5)), t)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            return accuracy\n",
    " \n",
    "    def compile(self, lr=0.1):\n",
    "        with self.graph.as_default():\n",
    "            x = tf.placeholder(tf.float32, shape=[None, self.dim_input], name='x')\n",
    "            t = tf.placeholder(tf.float32, shape=[None, self.dim_output], name='t')\n",
    "\n",
    "            y = self.inference(x)\n",
    "\n",
    "            loss = self.loss(y, t)\n",
    "            train_step = self.training(loss, lr)\n",
    "            accuracy = self.accuracy(y, t)\n",
    "        \n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._y = y\n",
    "        \n",
    "        self._loss = loss\n",
    "        self._train_step = train_step\n",
    "        self._accuracy = accuracy\n",
    "\n",
    "    def fit(self, X_train, Y_train, epochs=100, batch_size=100, verbose=1):\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "\n",
    "            self._sess = sess\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "                for i in range(n_batches):\n",
    "                    start = i * batch_size\n",
    "                    end = start + batch_size\n",
    "\n",
    "                    sess.run(\n",
    "                        self._train_step,\n",
    "                        feed_dict={\n",
    "                            self._x: X_[start:end],\n",
    "                            self._t: Y_[start:end]\n",
    "                        })\n",
    "\n",
    "                loss_ = self._loss.eval(\n",
    "                            session=sess,\n",
    "                            feed_dict={\n",
    "                                self._x: X_train,\n",
    "                                self._t: Y_train\n",
    "                            })\n",
    "\n",
    "                accuracy_ = self._accuracy.eval(\n",
    "                                session=sess,\n",
    "                                feed_dict={\n",
    "                                    self._x: X_train,\n",
    "                                    self._t: Y_train\n",
    "                                })\n",
    "\n",
    "                self._history['loss'].append(loss_)\n",
    "                self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"epoch: {0} / loss: {1} / accuracy: {2}\".format(epoch, loss_, accuracy_))\n",
    "\n",
    "            return self._history\n",
    "        \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        with self.graph.as_default():\n",
    "            return self.accuracy.eval(\n",
    "                        session=self._sess,\n",
    "                        feed_dict={\n",
    "                            self._x: X_test,\n",
    "                            self._t: Y_test\n",
    "                        })\n",
    "    \n",
    "    def output(self, X_test):\n",
    "        with self.graph.as_default():\n",
    "            return self._y.eval(session=self._sess,\n",
    "                                feed_dict={\n",
    "                                    self._x: X_test\n",
    "                                })\n",
    "    \n",
    "    def show_graph_model(self):\n",
    "        with self.graph.as_default():\n",
    "            show_graph(tf.Session().graph_def)\n",
    "            \n",
    "    def show_graph_loss(self):\n",
    "        fig = plt.figure(figsize=(16, 4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(self._history['loss'])\n",
    "        \n",
    "        ax.set_title('Loss')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Loss')\n",
    "\n",
    "        plt.show() \n",
    "\n",
    "    def show_graph_accuracy(self):\n",
    "        fig = plt.figure(figsize=(16, 4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(self._history['accuracy'])\n",
    "        \n",
    "        ax.set_title('Accuracy')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        \n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array([0, 1, 1, 0])[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLP(tf.Graph(),\n",
    "            dim_input=2,\n",
    "            dim_hiddens=[2],\n",
    "            dim_output=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.349062517670923&quot;).pbtxt = 'node {\\n  name: &quot;x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;t&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;Hidden_Layer_1/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Hidden_Layer_1/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;Hidden_Layer_1/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Hidden_Layer_1/truncated_normal/mul&quot;\\n  input: &quot;Hidden_Layer_1/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 2\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Hidden_Layer_1/W&quot;\\n  input: &quot;Hidden_Layer_1/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Hidden_Layer_1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Hidden_Layer_1/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Hidden_Layer_1/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Hidden_Layer_1/b&quot;\\n  input: &quot;Hidden_Layer_1/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Hidden_Layer_1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Hidden_Layer_1/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Hidden_Layer_1/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;x&quot;\\n  input: &quot;Hidden_Layer_1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Hidden_Layer_1/MatMul&quot;\\n  input: &quot;Hidden_Layer_1/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Hidden_Layer_1/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Hidden_Layer_1/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;Output_Layer/truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Output_Layer/truncated_normal/TruncatedNormal&quot;\\n  input: &quot;Output_Layer/truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Output_Layer/truncated_normal/mul&quot;\\n  input: &quot;Output_Layer/truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/W&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 2\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Output_Layer/W&quot;\\n  input: &quot;Output_Layer/truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Output_Layer/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Output_Layer/W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Output_Layer/W&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/b&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/b/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Output_Layer/b&quot;\\n  input: &quot;Output_Layer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Output_Layer/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/b/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Output_Layer/b&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Output_Layer/b&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Hidden_Layer_1/Sigmoid&quot;\\n  input: &quot;Output_Layer/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Output_Layer/MatMul&quot;\\n  input: &quot;Output_Layer/b/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Output_Layer/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Output_Layer/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/Log&quot;\\n  op: &quot;Log&quot;\\n  input: &quot;Output_Layer/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;t&quot;\\n  input: &quot;Loss/Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/sub/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Loss/sub/x&quot;\\n  input: &quot;t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/sub_1/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Loss/sub_1/x&quot;\\n  input: &quot;Output_Layer/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/Log_1&quot;\\n  op: &quot;Log&quot;\\n  input: &quot;Loss/sub_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Loss/sub&quot;\\n  input: &quot;Loss/Log_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Loss/mul&quot;\\n  input: &quot;Loss/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Loss/add&quot;\\n  input: &quot;Loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Loss/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;Loss/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;Train/gradients/Shape&quot;\\n  input: &quot;Train/gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Neg_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;Train/gradients/Fill&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Sum_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/Neg_grad/Neg&quot;\\n  input: &quot;Train/gradients/Loss/Sum_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Loss/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;Train/gradients/Loss/Sum_grad/Reshape&quot;\\n  input: &quot;Train/gradients/Loss/Sum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Loss/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Loss/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Shape&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/Sum_grad/Tile&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Sum&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/Sum_grad/Tile&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Sum_1&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Loss/add_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/Reshape_1&quot;\\n  input: &quot;^Train/gradients/Loss/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Loss/Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Shape&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/tuple/control_dependency&quot;\\n  input: &quot;Loss/Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/mul&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Sum&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;t&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/mul_1&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Sum_1&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Loss/mul_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/Reshape_1&quot;\\n  input: &quot;^Train/gradients/Loss/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Loss/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Loss/Log_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Shape&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Loss/Log_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/mul&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Sum&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Loss/sub&quot;\\n  input: &quot;Train/gradients/Loss/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/mul_1&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Sum_1&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Loss/mul_1_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^Train/gradients/Loss/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Log_grad/Reciprocal&quot;\\n  op: &quot;Reciprocal&quot;\\n  input: &quot;Output_Layer/Sigmoid&quot;\\n  input: &quot;^Train/gradients/Loss/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Log_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Train/gradients/Loss/mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Train/gradients/Loss/Log_grad/Reciprocal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Log_1_grad/Reciprocal&quot;\\n  op: &quot;Reciprocal&quot;\\n  input: &quot;Loss/sub_1&quot;\\n  input: &quot;^Train/gradients/Loss/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/Log_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Train/gradients/Loss/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Train/gradients/Loss/Log_1_grad/Reciprocal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Output_Layer/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Shape&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/Log_1_grad/mul&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Sum&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Loss/Log_1_grad/mul&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Neg&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Loss/sub_1_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/sub_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Loss/sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/sub_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Loss/sub_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/Reshape_1&quot;\\n  input: &quot;^Train/gradients/Loss/sub_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/sub_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/AddN&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;Train/gradients/Loss/Log_grad/mul&quot;\\n  input: &quot;Train/gradients/Loss/sub_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Loss/Log_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Output_Layer/Sigmoid&quot;\\n  input: &quot;Train/gradients/AddN&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Output_Layer/MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Shape&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Output_Layer/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Sum&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Output_Layer/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Sum_1&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Output_Layer/add_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Output_Layer/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Output_Layer/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Output_Layer/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/Reshape_1&quot;\\n  input: &quot;^Train/gradients/Output_Layer/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Output_Layer/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/tuple/control_dependency&quot;\\n  input: &quot;Output_Layer/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Hidden_Layer_1/Sigmoid&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Output_Layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^Train/gradients/Output_Layer/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Output_Layer/MatMul_grad/MatMul&quot;\\n  input: &quot;^Train/gradients/Output_Layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Output_Layer/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Output_Layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Output_Layer/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^Train/gradients/Output_Layer/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Output_Layer/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Hidden_Layer_1/Sigmoid&quot;\\n  input: &quot;Train/gradients/Output_Layer/MatMul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Hidden_Layer_1/MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Shape&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Sum&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Sum_1&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/add_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Reshape&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Hidden_Layer_1/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/Reshape_1&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Hidden_Layer_1/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/tuple/control_dependency&quot;\\n  input: &quot;Hidden_Layer_1/W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;x&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^Train/gradients/Hidden_Layer_1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Train/gradients/Hidden_Layer_1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149011612\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/GradientDescent/update_Hidden_Layer_1/W/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;Hidden_Layer_1/W&quot;\\n  input: &quot;Train/GradientDescent/learning_rate&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Hidden_Layer_1/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/GradientDescent/update_Hidden_Layer_1/b/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;Hidden_Layer_1/b&quot;\\n  input: &quot;Train/GradientDescent/learning_rate&quot;\\n  input: &quot;Train/gradients/Hidden_Layer_1/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Hidden_Layer_1/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/GradientDescent/update_Output_Layer/W/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;Output_Layer/W&quot;\\n  input: &quot;Train/GradientDescent/learning_rate&quot;\\n  input: &quot;Train/gradients/Output_Layer/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Output_Layer/W&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/GradientDescent/update_Output_Layer/b/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;Output_Layer/b&quot;\\n  input: &quot;Train/GradientDescent/learning_rate&quot;\\n  input: &quot;Train/gradients/Output_Layer/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Output_Layer/b&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Train/GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Train/GradientDescent/update_Hidden_Layer_1/W/ApplyGradientDescent&quot;\\n  input: &quot;^Train/GradientDescent/update_Hidden_Layer_1/b/ApplyGradientDescent&quot;\\n  input: &quot;^Train/GradientDescent/update_Output_Layer/W/ApplyGradientDescent&quot;\\n  input: &quot;^Train/GradientDescent/update_Output_Layer/b/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;Accuracy/Greater/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Accuracy/Greater&quot;\\n  op: &quot;Greater&quot;\\n  input: &quot;Output_Layer/Sigmoid&quot;\\n  input: &quot;Accuracy/Greater/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Accuracy/ToFloat&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Accuracy/Greater&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Accuracy/Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;Accuracy/ToFloat&quot;\\n  input: &quot;t&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Accuracy/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Accuracy/Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Accuracy/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Accuracy/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Accuracy/Cast&quot;\\n  input: &quot;Accuracy/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.349062517670923&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.show_graph_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 / loss: 2.9576663970947266 / accuracy: 0.5\n",
      "epoch: 1 / loss: 2.9125499725341797 / accuracy: 0.5\n",
      "epoch: 2 / loss: 2.8783907890319824 / accuracy: 0.5\n",
      "epoch: 3 / loss: 2.8524868488311768 / accuracy: 0.5\n",
      "epoch: 4 / loss: 2.8327736854553223 / accuracy: 0.5\n",
      "epoch: 5 / loss: 2.8176870346069336 / accuracy: 0.25\n",
      "epoch: 6 / loss: 2.8060522079467773 / accuracy: 0.5\n",
      "epoch: 7 / loss: 2.7969884872436523 / accuracy: 0.5\n",
      "epoch: 8 / loss: 2.789839267730713 / accuracy: 0.5\n",
      "epoch: 9 / loss: 2.7841148376464844 / accuracy: 0.5\n",
      "epoch: 10 / loss: 2.779451847076416 / accuracy: 0.5\n",
      "epoch: 11 / loss: 2.775578498840332 / accuracy: 0.5\n",
      "epoch: 12 / loss: 2.772294282913208 / accuracy: 0.5\n",
      "epoch: 13 / loss: 2.7694485187530518 / accuracy: 0.5\n",
      "epoch: 14 / loss: 2.766930103302002 / accuracy: 0.5\n",
      "epoch: 15 / loss: 2.7646560668945312 / accuracy: 0.5\n",
      "epoch: 16 / loss: 2.7625646591186523 / accuracy: 0.5\n",
      "epoch: 17 / loss: 2.7606101036071777 / accuracy: 0.5\n",
      "epoch: 18 / loss: 2.7587571144104004 / accuracy: 0.5\n",
      "epoch: 19 / loss: 2.756981372833252 / accuracy: 0.5\n",
      "epoch: 20 / loss: 2.7552623748779297 / accuracy: 0.5\n",
      "epoch: 21 / loss: 2.753586530685425 / accuracy: 0.5\n",
      "epoch: 22 / loss: 2.7519431114196777 / accuracy: 0.5\n",
      "epoch: 23 / loss: 2.750324249267578 / accuracy: 0.5\n",
      "epoch: 24 / loss: 2.7487235069274902 / accuracy: 0.5\n",
      "epoch: 25 / loss: 2.747136354446411 / accuracy: 0.5\n",
      "epoch: 26 / loss: 2.7455601692199707 / accuracy: 0.5\n",
      "epoch: 27 / loss: 2.7439916133880615 / accuracy: 0.5\n",
      "epoch: 28 / loss: 2.74242901802063 / accuracy: 0.5\n",
      "epoch: 29 / loss: 2.7408714294433594 / accuracy: 0.5\n",
      "epoch: 30 / loss: 2.739316940307617 / accuracy: 0.5\n",
      "epoch: 31 / loss: 2.737765312194824 / accuracy: 0.5\n",
      "epoch: 32 / loss: 2.736215591430664 / accuracy: 0.5\n",
      "epoch: 33 / loss: 2.7346677780151367 / accuracy: 0.5\n",
      "epoch: 34 / loss: 2.733121156692505 / accuracy: 0.5\n",
      "epoch: 35 / loss: 2.7315759658813477 / accuracy: 0.5\n",
      "epoch: 36 / loss: 2.7300310134887695 / accuracy: 0.5\n",
      "epoch: 37 / loss: 2.7284865379333496 / accuracy: 0.5\n",
      "epoch: 38 / loss: 2.726943016052246 / accuracy: 0.5\n",
      "epoch: 39 / loss: 2.7253994941711426 / accuracy: 0.5\n",
      "epoch: 40 / loss: 2.7238569259643555 / accuracy: 0.5\n",
      "epoch: 41 / loss: 2.72231388092041 / accuracy: 0.5\n",
      "epoch: 42 / loss: 2.720771551132202 / accuracy: 0.5\n",
      "epoch: 43 / loss: 2.719228982925415 / accuracy: 0.5\n",
      "epoch: 44 / loss: 2.717686653137207 / accuracy: 0.5\n",
      "epoch: 45 / loss: 2.716144561767578 / accuracy: 0.5\n",
      "epoch: 46 / loss: 2.7146029472351074 / accuracy: 0.5\n",
      "epoch: 47 / loss: 2.7130608558654785 / accuracy: 0.5\n",
      "epoch: 48 / loss: 2.7115187644958496 / accuracy: 0.5\n",
      "epoch: 49 / loss: 2.7099766731262207 / accuracy: 0.5\n",
      "epoch: 50 / loss: 2.708434820175171 / accuracy: 0.5\n",
      "epoch: 51 / loss: 2.706892490386963 / accuracy: 0.5\n",
      "epoch: 52 / loss: 2.705350399017334 / accuracy: 0.5\n",
      "epoch: 53 / loss: 2.7038075923919678 / accuracy: 0.5\n",
      "epoch: 54 / loss: 2.7022647857666016 / accuracy: 0.5\n",
      "epoch: 55 / loss: 2.700721502304077 / accuracy: 0.5\n",
      "epoch: 56 / loss: 2.6991782188415527 / accuracy: 0.5\n",
      "epoch: 57 / loss: 2.697634220123291 / accuracy: 0.5\n",
      "epoch: 58 / loss: 2.6960902214050293 / accuracy: 0.5\n",
      "epoch: 59 / loss: 2.6945457458496094 / accuracy: 0.75\n",
      "epoch: 60 / loss: 2.6930007934570312 / accuracy: 0.75\n",
      "epoch: 61 / loss: 2.6914546489715576 / accuracy: 0.75\n",
      "epoch: 62 / loss: 2.689908504486084 / accuracy: 0.75\n",
      "epoch: 63 / loss: 2.688361167907715 / accuracy: 0.75\n",
      "epoch: 64 / loss: 2.6868133544921875 / accuracy: 0.75\n",
      "epoch: 65 / loss: 2.6852645874023438 / accuracy: 0.75\n",
      "epoch: 66 / loss: 2.6837151050567627 / accuracy: 0.75\n",
      "epoch: 67 / loss: 2.682164192199707 / accuracy: 0.75\n",
      "epoch: 68 / loss: 2.680612802505493 / accuracy: 0.75\n",
      "epoch: 69 / loss: 2.6790599822998047 / accuracy: 0.75\n",
      "epoch: 70 / loss: 2.6775059700012207 / accuracy: 0.75\n",
      "epoch: 71 / loss: 2.675950527191162 / accuracy: 0.75\n",
      "epoch: 72 / loss: 2.674393653869629 / accuracy: 0.75\n",
      "epoch: 73 / loss: 2.672835350036621 / accuracy: 0.75\n",
      "epoch: 74 / loss: 2.6712756156921387 / accuracy: 0.75\n",
      "epoch: 75 / loss: 2.6697139739990234 / accuracy: 0.75\n",
      "epoch: 76 / loss: 2.6681511402130127 / accuracy: 0.75\n",
      "epoch: 77 / loss: 2.6665854454040527 / accuracy: 0.75\n",
      "epoch: 78 / loss: 2.6650185585021973 / accuracy: 0.75\n",
      "epoch: 79 / loss: 2.66344952583313 / accuracy: 0.75\n",
      "epoch: 80 / loss: 2.6618783473968506 / accuracy: 0.75\n",
      "epoch: 81 / loss: 2.6603047847747803 / accuracy: 0.75\n",
      "epoch: 82 / loss: 2.658728837966919 / accuracy: 0.75\n",
      "epoch: 83 / loss: 2.6571507453918457 / accuracy: 0.75\n",
      "epoch: 84 / loss: 2.6555700302124023 / accuracy: 0.75\n",
      "epoch: 85 / loss: 2.6539862155914307 / accuracy: 0.75\n",
      "epoch: 86 / loss: 2.652400016784668 / accuracy: 0.75\n",
      "epoch: 87 / loss: 2.650810956954956 / accuracy: 0.75\n",
      "epoch: 88 / loss: 2.649218797683716 / accuracy: 0.75\n",
      "epoch: 89 / loss: 2.6476235389709473 / accuracy: 0.75\n",
      "epoch: 90 / loss: 2.6460251808166504 / accuracy: 0.75\n",
      "epoch: 91 / loss: 2.644423723220825 / accuracy: 0.75\n",
      "epoch: 92 / loss: 2.6428189277648926 / accuracy: 0.75\n",
      "epoch: 93 / loss: 2.6412100791931152 / accuracy: 0.75\n",
      "epoch: 94 / loss: 2.6395978927612305 / accuracy: 0.75\n",
      "epoch: 95 / loss: 2.6379823684692383 / accuracy: 0.75\n",
      "epoch: 96 / loss: 2.6363630294799805 / accuracy: 0.75\n",
      "epoch: 97 / loss: 2.634739398956299 / accuracy: 0.75\n",
      "epoch: 98 / loss: 2.6331114768981934 / accuracy: 0.75\n",
      "epoch: 99 / loss: 2.6314802169799805 / accuracy: 0.75\n",
      "epoch: 100 / loss: 2.6298441886901855 / accuracy: 0.75\n",
      "epoch: 101 / loss: 2.6282036304473877 / accuracy: 0.75\n",
      "epoch: 102 / loss: 2.626559257507324 / accuracy: 0.75\n",
      "epoch: 103 / loss: 2.6249098777770996 / accuracy: 0.75\n",
      "epoch: 104 / loss: 2.623256206512451 / accuracy: 0.75\n",
      "epoch: 105 / loss: 2.6215975284576416 / accuracy: 0.75\n",
      "epoch: 106 / loss: 2.61993408203125 / accuracy: 0.75\n",
      "epoch: 107 / loss: 2.6182656288146973 / accuracy: 0.75\n",
      "epoch: 108 / loss: 2.6165924072265625 / accuracy: 0.75\n",
      "epoch: 109 / loss: 2.6149144172668457 / accuracy: 0.75\n",
      "epoch: 110 / loss: 2.6132302284240723 / accuracy: 0.75\n",
      "epoch: 111 / loss: 2.611541509628296 / accuracy: 0.75\n",
      "epoch: 112 / loss: 2.609847068786621 / accuracy: 0.75\n",
      "epoch: 113 / loss: 2.608147621154785 / accuracy: 0.75\n",
      "epoch: 114 / loss: 2.6064419746398926 / accuracy: 0.75\n",
      "epoch: 115 / loss: 2.604731321334839 / accuracy: 0.75\n",
      "epoch: 116 / loss: 2.6030144691467285 / accuracy: 0.75\n",
      "epoch: 117 / loss: 2.601292133331299 / accuracy: 0.75\n",
      "epoch: 118 / loss: 2.5995635986328125 / accuracy: 0.75\n",
      "epoch: 119 / loss: 2.5978293418884277 / accuracy: 0.75\n",
      "epoch: 120 / loss: 2.5960893630981445 / accuracy: 0.75\n",
      "epoch: 121 / loss: 2.5943427085876465 / accuracy: 0.75\n",
      "epoch: 122 / loss: 2.59259033203125 / accuracy: 0.75\n",
      "epoch: 123 / loss: 2.590831756591797 / accuracy: 0.75\n",
      "epoch: 124 / loss: 2.589066743850708 / accuracy: 0.75\n",
      "epoch: 125 / loss: 2.5872952938079834 / accuracy: 0.75\n",
      "epoch: 126 / loss: 2.585517406463623 / accuracy: 0.75\n",
      "epoch: 127 / loss: 2.583733320236206 / accuracy: 0.75\n",
      "epoch: 128 / loss: 2.581942558288574 / accuracy: 0.75\n",
      "epoch: 129 / loss: 2.5801448822021484 / accuracy: 0.75\n",
      "epoch: 130 / loss: 2.578341007232666 / accuracy: 0.75\n",
      "epoch: 131 / loss: 2.5765304565429688 / accuracy: 0.75\n",
      "epoch: 132 / loss: 2.5747127532958984 / accuracy: 0.75\n",
      "epoch: 133 / loss: 2.5728888511657715 / accuracy: 0.75\n",
      "epoch: 134 / loss: 2.5710580348968506 / accuracy: 0.75\n",
      "epoch: 135 / loss: 2.5692200660705566 / accuracy: 0.75\n",
      "epoch: 136 / loss: 2.567375659942627 / accuracy: 0.75\n",
      "epoch: 137 / loss: 2.565524101257324 / accuracy: 0.75\n",
      "epoch: 138 / loss: 2.5636653900146484 / accuracy: 0.75\n",
      "epoch: 139 / loss: 2.561800003051758 / accuracy: 0.75\n",
      "epoch: 140 / loss: 2.559927463531494 / accuracy: 0.75\n",
      "epoch: 141 / loss: 2.5580480098724365 / accuracy: 0.75\n",
      "epoch: 142 / loss: 2.556161403656006 / accuracy: 0.75\n",
      "epoch: 143 / loss: 2.5542678833007812 / accuracy: 0.75\n",
      "epoch: 144 / loss: 2.5523672103881836 / accuracy: 0.75\n",
      "epoch: 145 / loss: 2.550459384918213 / accuracy: 0.75\n",
      "epoch: 146 / loss: 2.5485446453094482 / accuracy: 0.75\n",
      "epoch: 147 / loss: 2.5466229915618896 / accuracy: 0.75\n",
      "epoch: 148 / loss: 2.544693946838379 / accuracy: 0.75\n",
      "epoch: 149 / loss: 2.542757987976074 / accuracy: 0.75\n",
      "epoch: 150 / loss: 2.5408148765563965 / accuracy: 0.75\n",
      "epoch: 151 / loss: 2.5388646125793457 / accuracy: 0.75\n",
      "epoch: 152 / loss: 2.536907196044922 / accuracy: 0.75\n",
      "epoch: 153 / loss: 2.534942626953125 / accuracy: 0.75\n",
      "epoch: 154 / loss: 2.5329713821411133 / accuracy: 0.75\n",
      "epoch: 155 / loss: 2.5309929847717285 / accuracy: 0.75\n",
      "epoch: 156 / loss: 2.5290074348449707 / accuracy: 0.75\n",
      "epoch: 157 / loss: 2.527015209197998 / accuracy: 0.75\n",
      "epoch: 158 / loss: 2.5250158309936523 / accuracy: 0.75\n",
      "epoch: 159 / loss: 2.5230095386505127 / accuracy: 0.75\n",
      "epoch: 160 / loss: 2.52099609375 / accuracy: 0.75\n",
      "epoch: 161 / loss: 2.5189759731292725 / accuracy: 0.75\n",
      "epoch: 162 / loss: 2.516948938369751 / accuracy: 0.75\n",
      "epoch: 163 / loss: 2.5149149894714355 / accuracy: 0.75\n",
      "epoch: 164 / loss: 2.5128746032714844 / accuracy: 0.75\n",
      "epoch: 165 / loss: 2.5108275413513184 / accuracy: 0.75\n",
      "epoch: 166 / loss: 2.5087733268737793 / accuracy: 0.75\n",
      "epoch: 167 / loss: 2.5067129135131836 / accuracy: 0.75\n",
      "epoch: 168 / loss: 2.504645824432373 / accuracy: 0.75\n",
      "epoch: 169 / loss: 2.5025720596313477 / accuracy: 0.75\n",
      "epoch: 170 / loss: 2.5004920959472656 / accuracy: 0.75\n",
      "epoch: 171 / loss: 2.4984054565429688 / accuracy: 0.75\n",
      "epoch: 172 / loss: 2.4963126182556152 / accuracy: 0.75\n",
      "epoch: 173 / loss: 2.494213581085205 / accuracy: 0.75\n",
      "epoch: 174 / loss: 2.49210786819458 / accuracy: 0.75\n",
      "epoch: 175 / loss: 2.4899964332580566 / accuracy: 0.75\n",
      "epoch: 176 / loss: 2.4878787994384766 / accuracy: 0.75\n",
      "epoch: 177 / loss: 2.48575496673584 / accuracy: 0.75\n",
      "epoch: 178 / loss: 2.4836254119873047 / accuracy: 0.75\n",
      "epoch: 179 / loss: 2.481489658355713 / accuracy: 0.75\n",
      "epoch: 180 / loss: 2.4793484210968018 / accuracy: 0.75\n",
      "epoch: 181 / loss: 2.477201461791992 / accuracy: 0.75\n",
      "epoch: 182 / loss: 2.475048780441284 / accuracy: 0.75\n",
      "epoch: 183 / loss: 2.472890615463257 / accuracy: 0.75\n",
      "epoch: 184 / loss: 2.47072696685791 / accuracy: 0.75\n",
      "epoch: 185 / loss: 2.468557834625244 / accuracy: 0.75\n",
      "epoch: 186 / loss: 2.466383457183838 / accuracy: 0.75\n",
      "epoch: 187 / loss: 2.4642038345336914 / accuracy: 0.75\n",
      "epoch: 188 / loss: 2.462019205093384 / accuracy: 0.75\n",
      "epoch: 189 / loss: 2.459829330444336 / accuracy: 0.75\n",
      "epoch: 190 / loss: 2.457634449005127 / accuracy: 0.75\n",
      "epoch: 191 / loss: 2.455434799194336 / accuracy: 0.75\n",
      "epoch: 192 / loss: 2.453230857849121 / accuracy: 0.75\n",
      "epoch: 193 / loss: 2.451021909713745 / accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 194 / loss: 2.448808193206787 / accuracy: 0.75\n",
      "epoch: 195 / loss: 2.4465904235839844 / accuracy: 0.75\n",
      "epoch: 196 / loss: 2.444368362426758 / accuracy: 0.75\n",
      "epoch: 197 / loss: 2.4421417713165283 / accuracy: 0.75\n",
      "epoch: 198 / loss: 2.439910888671875 / accuracy: 0.75\n",
      "epoch: 199 / loss: 2.437676191329956 / accuracy: 0.75\n",
      "epoch: 200 / loss: 2.4354376792907715 / accuracy: 0.75\n",
      "epoch: 201 / loss: 2.433195114135742 / accuracy: 0.75\n",
      "epoch: 202 / loss: 2.4309489727020264 / accuracy: 0.75\n",
      "epoch: 203 / loss: 2.428699016571045 / accuracy: 0.75\n",
      "epoch: 204 / loss: 2.426445484161377 / accuracy: 0.75\n",
      "epoch: 205 / loss: 2.4241890907287598 / accuracy: 0.75\n",
      "epoch: 206 / loss: 2.421928644180298 / accuracy: 0.75\n",
      "epoch: 207 / loss: 2.4196653366088867 / accuracy: 0.75\n",
      "epoch: 208 / loss: 2.4173989295959473 / accuracy: 0.75\n",
      "epoch: 209 / loss: 2.4151296615600586 / accuracy: 0.75\n",
      "epoch: 210 / loss: 2.4128572940826416 / accuracy: 0.75\n",
      "epoch: 211 / loss: 2.4105820655822754 / accuracy: 0.75\n",
      "epoch: 212 / loss: 2.4083046913146973 / accuracy: 0.75\n",
      "epoch: 213 / loss: 2.4060239791870117 / accuracy: 0.75\n",
      "epoch: 214 / loss: 2.4037415981292725 / accuracy: 0.75\n",
      "epoch: 215 / loss: 2.401456594467163 / accuracy: 0.75\n",
      "epoch: 216 / loss: 2.3991692066192627 / accuracy: 0.75\n",
      "epoch: 217 / loss: 2.3968799114227295 / accuracy: 0.75\n",
      "epoch: 218 / loss: 2.3945882320404053 / accuracy: 0.75\n",
      "epoch: 219 / loss: 2.3922951221466064 / accuracy: 0.75\n",
      "epoch: 220 / loss: 2.3899998664855957 / accuracy: 0.75\n",
      "epoch: 221 / loss: 2.3877029418945312 / accuracy: 0.75\n",
      "epoch: 222 / loss: 2.385404586791992 / accuracy: 0.75\n",
      "epoch: 223 / loss: 2.3831048011779785 / accuracy: 0.75\n",
      "epoch: 224 / loss: 2.380803108215332 / accuracy: 0.75\n",
      "epoch: 225 / loss: 2.3785009384155273 / accuracy: 0.75\n",
      "epoch: 226 / loss: 2.37619686126709 / accuracy: 0.75\n",
      "epoch: 227 / loss: 2.373892068862915 / accuracy: 0.75\n",
      "epoch: 228 / loss: 2.371586322784424 / accuracy: 0.75\n",
      "epoch: 229 / loss: 2.369279623031616 / accuracy: 0.75\n",
      "epoch: 230 / loss: 2.366971969604492 / accuracy: 0.75\n",
      "epoch: 231 / loss: 2.36466383934021 / accuracy: 0.75\n",
      "epoch: 232 / loss: 2.3623552322387695 / accuracy: 0.75\n",
      "epoch: 233 / loss: 2.360045909881592 / accuracy: 0.75\n",
      "epoch: 234 / loss: 2.357736587524414 / accuracy: 0.75\n",
      "epoch: 235 / loss: 2.355426549911499 / accuracy: 0.75\n",
      "epoch: 236 / loss: 2.353116273880005 / accuracy: 0.75\n",
      "epoch: 237 / loss: 2.35080623626709 / accuracy: 0.75\n",
      "epoch: 238 / loss: 2.3484959602355957 / accuracy: 0.75\n",
      "epoch: 239 / loss: 2.3461861610412598 / accuracy: 0.75\n",
      "epoch: 240 / loss: 2.3438761234283447 / accuracy: 0.75\n",
      "epoch: 241 / loss: 2.341566324234009 / accuracy: 0.75\n",
      "epoch: 242 / loss: 2.339257001876831 / accuracy: 0.75\n",
      "epoch: 243 / loss: 2.3369479179382324 / accuracy: 0.75\n",
      "epoch: 244 / loss: 2.33463978767395 / accuracy: 0.75\n",
      "epoch: 245 / loss: 2.332331657409668 / accuracy: 0.75\n",
      "epoch: 246 / loss: 2.3300247192382812 / accuracy: 0.75\n",
      "epoch: 247 / loss: 2.3277182579040527 / accuracy: 0.75\n",
      "epoch: 248 / loss: 2.3254125118255615 / accuracy: 0.75\n",
      "epoch: 249 / loss: 2.3231077194213867 / accuracy: 0.75\n",
      "epoch: 250 / loss: 2.3208043575286865 / accuracy: 0.75\n",
      "epoch: 251 / loss: 2.3185017108917236 / accuracy: 0.75\n",
      "epoch: 252 / loss: 2.3162002563476562 / accuracy: 0.75\n",
      "epoch: 253 / loss: 2.3138999938964844 / accuracy: 0.75\n",
      "epoch: 254 / loss: 2.311601161956787 / accuracy: 0.75\n",
      "epoch: 255 / loss: 2.3093032836914062 / accuracy: 0.75\n",
      "epoch: 256 / loss: 2.3070068359375 / accuracy: 0.75\n",
      "epoch: 257 / loss: 2.3047122955322266 / accuracy: 0.75\n",
      "epoch: 258 / loss: 2.3024187088012695 / accuracy: 0.75\n",
      "epoch: 259 / loss: 2.3001275062561035 / accuracy: 0.75\n",
      "epoch: 260 / loss: 2.297837495803833 / accuracy: 0.75\n",
      "epoch: 261 / loss: 2.295548915863037 / accuracy: 0.75\n",
      "epoch: 262 / loss: 2.293262481689453 / accuracy: 0.75\n",
      "epoch: 263 / loss: 2.290977954864502 / accuracy: 0.75\n",
      "epoch: 264 / loss: 2.2886948585510254 / accuracy: 0.75\n",
      "epoch: 265 / loss: 2.286414384841919 / accuracy: 0.75\n",
      "epoch: 266 / loss: 2.284135580062866 / accuracy: 0.75\n",
      "epoch: 267 / loss: 2.2818586826324463 / accuracy: 0.75\n",
      "epoch: 268 / loss: 2.2795844078063965 / accuracy: 0.75\n",
      "epoch: 269 / loss: 2.2773115634918213 / accuracy: 0.75\n",
      "epoch: 270 / loss: 2.275041103363037 / accuracy: 0.75\n",
      "epoch: 271 / loss: 2.272773265838623 / accuracy: 0.75\n",
      "epoch: 272 / loss: 2.270507335662842 / accuracy: 0.75\n",
      "epoch: 273 / loss: 2.2682440280914307 / accuracy: 0.75\n",
      "epoch: 274 / loss: 2.2659826278686523 / accuracy: 0.75\n",
      "epoch: 275 / loss: 2.263723850250244 / accuracy: 0.75\n",
      "epoch: 276 / loss: 2.261467695236206 / accuracy: 0.75\n",
      "epoch: 277 / loss: 2.259213924407959 / accuracy: 0.75\n",
      "epoch: 278 / loss: 2.256962776184082 / accuracy: 0.75\n",
      "epoch: 279 / loss: 2.254713773727417 / accuracy: 0.75\n",
      "epoch: 280 / loss: 2.252467632293701 / accuracy: 0.75\n",
      "epoch: 281 / loss: 2.2502241134643555 / accuracy: 0.75\n",
      "epoch: 282 / loss: 2.24798321723938 / accuracy: 0.75\n",
      "epoch: 283 / loss: 2.2457447052001953 / accuracy: 0.75\n",
      "epoch: 284 / loss: 2.243509292602539 / accuracy: 0.75\n",
      "epoch: 285 / loss: 2.241276264190674 / accuracy: 0.75\n",
      "epoch: 286 / loss: 2.239046096801758 / accuracy: 0.75\n",
      "epoch: 287 / loss: 2.236818790435791 / accuracy: 0.75\n",
      "epoch: 288 / loss: 2.2345941066741943 / accuracy: 0.75\n",
      "epoch: 289 / loss: 2.232372283935547 / accuracy: 0.75\n",
      "epoch: 290 / loss: 2.2301533222198486 / accuracy: 0.75\n",
      "epoch: 291 / loss: 2.2279369831085205 / accuracy: 0.75\n",
      "epoch: 292 / loss: 2.2257237434387207 / accuracy: 0.75\n",
      "epoch: 293 / loss: 2.223513603210449 / accuracy: 0.75\n",
      "epoch: 294 / loss: 2.2213058471679688 / accuracy: 0.75\n",
      "epoch: 295 / loss: 2.2191009521484375 / accuracy: 0.75\n",
      "epoch: 296 / loss: 2.2168989181518555 / accuracy: 0.75\n",
      "epoch: 297 / loss: 2.214700222015381 / accuracy: 0.75\n",
      "epoch: 298 / loss: 2.2125041484832764 / accuracy: 0.75\n",
      "epoch: 299 / loss: 2.2103114128112793 / accuracy: 0.75\n",
      "epoch: 300 / loss: 2.2081212997436523 / accuracy: 0.75\n",
      "epoch: 301 / loss: 2.2059338092803955 / accuracy: 0.75\n",
      "epoch: 302 / loss: 2.203749656677246 / accuracy: 0.75\n",
      "epoch: 303 / loss: 2.201568603515625 / accuracy: 0.75\n",
      "epoch: 304 / loss: 2.199389934539795 / accuracy: 0.75\n",
      "epoch: 305 / loss: 2.1972146034240723 / accuracy: 0.75\n",
      "epoch: 306 / loss: 2.195042371749878 / accuracy: 0.75\n",
      "epoch: 307 / loss: 2.192873001098633 / accuracy: 0.75\n",
      "epoch: 308 / loss: 2.190706491470337 / accuracy: 0.75\n",
      "epoch: 309 / loss: 2.1885428428649902 / accuracy: 0.75\n",
      "epoch: 310 / loss: 2.1863820552825928 / accuracy: 0.75\n",
      "epoch: 311 / loss: 2.1842246055603027 / accuracy: 0.75\n",
      "epoch: 312 / loss: 2.1820695400238037 / accuracy: 0.75\n",
      "epoch: 313 / loss: 2.179917812347412 / accuracy: 0.75\n",
      "epoch: 314 / loss: 2.1777687072753906 / accuracy: 0.75\n",
      "epoch: 315 / loss: 2.1756229400634766 / accuracy: 0.75\n",
      "epoch: 316 / loss: 2.1734795570373535 / accuracy: 0.75\n",
      "epoch: 317 / loss: 2.171339273452759 / accuracy: 0.75\n",
      "epoch: 318 / loss: 2.1692020893096924 / accuracy: 0.75\n",
      "epoch: 319 / loss: 2.167067527770996 / accuracy: 0.75\n",
      "epoch: 320 / loss: 2.16493558883667 / accuracy: 0.75\n",
      "epoch: 321 / loss: 2.162806749343872 / accuracy: 0.75\n",
      "epoch: 322 / loss: 2.1606810092926025 / accuracy: 0.75\n",
      "epoch: 323 / loss: 2.158557653427124 / accuracy: 0.75\n",
      "epoch: 324 / loss: 2.156437397003174 / accuracy: 0.75\n",
      "epoch: 325 / loss: 2.1543197631835938 / accuracy: 0.75\n",
      "epoch: 326 / loss: 2.1522045135498047 / accuracy: 0.75\n",
      "epoch: 327 / loss: 2.150092363357544 / accuracy: 0.75\n",
      "epoch: 328 / loss: 2.1479828357696533 / accuracy: 0.75\n",
      "epoch: 329 / loss: 2.145875930786133 / accuracy: 0.75\n",
      "epoch: 330 / loss: 2.1437721252441406 / accuracy: 0.75\n",
      "epoch: 331 / loss: 2.1416702270507812 / accuracy: 0.75\n",
      "epoch: 332 / loss: 2.1395716667175293 / accuracy: 0.75\n",
      "epoch: 333 / loss: 2.1374754905700684 / accuracy: 0.75\n",
      "epoch: 334 / loss: 2.1353819370269775 / accuracy: 0.75\n",
      "epoch: 335 / loss: 2.1332907676696777 / accuracy: 0.75\n",
      "epoch: 336 / loss: 2.13120174407959 / accuracy: 0.75\n",
      "epoch: 337 / loss: 2.1291158199310303 / accuracy: 0.75\n",
      "epoch: 338 / loss: 2.1270322799682617 / accuracy: 0.75\n",
      "epoch: 339 / loss: 2.124950885772705 / accuracy: 0.75\n",
      "epoch: 340 / loss: 2.1228721141815186 / accuracy: 0.75\n",
      "epoch: 341 / loss: 2.120795726776123 / accuracy: 0.75\n",
      "epoch: 342 / loss: 2.1187219619750977 / accuracy: 0.75\n",
      "epoch: 343 / loss: 2.116650104522705 / accuracy: 0.75\n",
      "epoch: 344 / loss: 2.1145806312561035 / accuracy: 0.75\n",
      "epoch: 345 / loss: 2.112513303756714 / accuracy: 0.75\n",
      "epoch: 346 / loss: 2.1104483604431152 / accuracy: 0.75\n",
      "epoch: 347 / loss: 2.1083855628967285 / accuracy: 0.75\n",
      "epoch: 348 / loss: 2.106325149536133 / accuracy: 0.75\n",
      "epoch: 349 / loss: 2.10426664352417 / accuracy: 0.75\n",
      "epoch: 350 / loss: 2.102210283279419 / accuracy: 0.75\n",
      "epoch: 351 / loss: 2.10015606880188 / accuracy: 0.75\n",
      "epoch: 352 / loss: 2.0981035232543945 / accuracy: 0.75\n",
      "epoch: 353 / loss: 2.0960533618927 / accuracy: 0.75\n",
      "epoch: 354 / loss: 2.0940046310424805 / accuracy: 0.75\n",
      "epoch: 355 / loss: 2.0919580459594727 / accuracy: 0.75\n",
      "epoch: 356 / loss: 2.0899133682250977 / accuracy: 0.75\n",
      "epoch: 357 / loss: 2.0878705978393555 / accuracy: 0.75\n",
      "epoch: 358 / loss: 2.085829734802246 / accuracy: 0.75\n",
      "epoch: 359 / loss: 2.0837903022766113 / accuracy: 0.75\n",
      "epoch: 360 / loss: 2.0817525386810303 / accuracy: 0.75\n",
      "epoch: 361 / loss: 2.079716444015503 / accuracy: 0.75\n",
      "epoch: 362 / loss: 2.0776820182800293 / accuracy: 0.75\n",
      "epoch: 363 / loss: 2.0756490230560303 / accuracy: 0.75\n",
      "epoch: 364 / loss: 2.073617696762085 / accuracy: 0.75\n",
      "epoch: 365 / loss: 2.071587562561035 / accuracy: 0.75\n",
      "epoch: 366 / loss: 2.069559335708618 / accuracy: 0.75\n",
      "epoch: 367 / loss: 2.0675320625305176 / accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 368 / loss: 2.0655064582824707 / accuracy: 0.75\n",
      "epoch: 369 / loss: 2.0634818077087402 / accuracy: 0.75\n",
      "epoch: 370 / loss: 2.0614585876464844 / accuracy: 0.75\n",
      "epoch: 371 / loss: 2.059436321258545 / accuracy: 0.75\n",
      "epoch: 372 / loss: 2.057415008544922 / accuracy: 0.75\n",
      "epoch: 373 / loss: 2.0553951263427734 / accuracy: 0.75\n",
      "epoch: 374 / loss: 2.0533761978149414 / accuracy: 0.75\n",
      "epoch: 375 / loss: 2.051358222961426 / accuracy: 0.75\n",
      "epoch: 376 / loss: 2.0493412017822266 / accuracy: 0.75\n",
      "epoch: 377 / loss: 2.0473246574401855 / accuracy: 0.75\n",
      "epoch: 378 / loss: 2.045309066772461 / accuracy: 0.75\n",
      "epoch: 379 / loss: 2.0432944297790527 / accuracy: 0.75\n",
      "epoch: 380 / loss: 2.0412802696228027 / accuracy: 0.75\n",
      "epoch: 381 / loss: 2.039266586303711 / accuracy: 0.75\n",
      "epoch: 382 / loss: 2.0372538566589355 / accuracy: 0.75\n",
      "epoch: 383 / loss: 2.03524112701416 / accuracy: 0.75\n",
      "epoch: 384 / loss: 2.033229351043701 / accuracy: 0.75\n",
      "epoch: 385 / loss: 2.031217575073242 / accuracy: 0.75\n",
      "epoch: 386 / loss: 2.0292062759399414 / accuracy: 0.75\n",
      "epoch: 387 / loss: 2.0271947383880615 / accuracy: 0.75\n",
      "epoch: 388 / loss: 2.025183916091919 / accuracy: 0.75\n",
      "epoch: 389 / loss: 2.0231730937957764 / accuracy: 0.75\n",
      "epoch: 390 / loss: 2.0211620330810547 / accuracy: 0.75\n",
      "epoch: 391 / loss: 2.019151210784912 / accuracy: 0.75\n",
      "epoch: 392 / loss: 2.0171399116516113 / accuracy: 0.75\n",
      "epoch: 393 / loss: 2.0151286125183105 / accuracy: 0.75\n",
      "epoch: 394 / loss: 2.0131168365478516 / accuracy: 0.75\n",
      "epoch: 395 / loss: 2.0111050605773926 / accuracy: 0.75\n",
      "epoch: 396 / loss: 2.0090925693511963 / accuracy: 0.75\n",
      "epoch: 397 / loss: 2.007079839706421 / accuracy: 0.75\n",
      "epoch: 398 / loss: 2.005066394805908 / accuracy: 0.75\n",
      "epoch: 399 / loss: 2.0030527114868164 / accuracy: 0.75\n",
      "epoch: 400 / loss: 2.001037836074829 / accuracy: 0.75\n",
      "epoch: 401 / loss: 1.999022364616394 / accuracy: 0.75\n",
      "epoch: 402 / loss: 1.9970057010650635 / accuracy: 0.75\n",
      "epoch: 403 / loss: 1.9949884414672852 / accuracy: 0.75\n",
      "epoch: 404 / loss: 1.9929697513580322 / accuracy: 0.75\n",
      "epoch: 405 / loss: 1.9909502267837524 / accuracy: 0.75\n",
      "epoch: 406 / loss: 1.988929271697998 / accuracy: 0.75\n",
      "epoch: 407 / loss: 1.9869070053100586 / accuracy: 0.75\n",
      "epoch: 408 / loss: 1.9848830699920654 / accuracy: 0.75\n",
      "epoch: 409 / loss: 1.9828579425811768 / accuracy: 0.75\n",
      "epoch: 410 / loss: 1.9808311462402344 / accuracy: 0.75\n",
      "epoch: 411 / loss: 1.9788024425506592 / accuracy: 0.75\n",
      "epoch: 412 / loss: 1.9767719507217407 / accuracy: 0.75\n",
      "epoch: 413 / loss: 1.974739670753479 / accuracy: 0.75\n",
      "epoch: 414 / loss: 1.972705364227295 / accuracy: 0.75\n",
      "epoch: 415 / loss: 1.9706687927246094 / accuracy: 0.75\n",
      "epoch: 416 / loss: 1.968630075454712 / accuracy: 0.75\n",
      "epoch: 417 / loss: 1.9665887355804443 / accuracy: 0.75\n",
      "epoch: 418 / loss: 1.9645452499389648 / accuracy: 0.75\n",
      "epoch: 419 / loss: 1.9624993801116943 / accuracy: 0.75\n",
      "epoch: 420 / loss: 1.9604504108428955 / accuracy: 0.75\n",
      "epoch: 421 / loss: 1.9583988189697266 / accuracy: 0.75\n",
      "epoch: 422 / loss: 1.9563446044921875 / accuracy: 0.75\n",
      "epoch: 423 / loss: 1.954287052154541 / accuracy: 0.75\n",
      "epoch: 424 / loss: 1.9522262811660767 / accuracy: 0.75\n",
      "epoch: 425 / loss: 1.950162410736084 / accuracy: 0.75\n",
      "epoch: 426 / loss: 1.9480950832366943 / accuracy: 0.75\n",
      "epoch: 427 / loss: 1.9460242986679077 / accuracy: 0.75\n",
      "epoch: 428 / loss: 1.943949580192566 / accuracy: 0.75\n",
      "epoch: 429 / loss: 1.9418714046478271 / accuracy: 0.75\n",
      "epoch: 430 / loss: 1.9397886991500854 / accuracy: 0.75\n",
      "epoch: 431 / loss: 1.9377026557922363 / accuracy: 0.75\n",
      "epoch: 432 / loss: 1.9356119632720947 / accuracy: 0.75\n",
      "epoch: 433 / loss: 1.9335169792175293 / accuracy: 0.75\n",
      "epoch: 434 / loss: 1.931417465209961 / accuracy: 0.75\n",
      "epoch: 435 / loss: 1.9293133020401 / accuracy: 0.75\n",
      "epoch: 436 / loss: 1.9272042512893677 / accuracy: 0.75\n",
      "epoch: 437 / loss: 1.9250905513763428 / accuracy: 0.75\n",
      "epoch: 438 / loss: 1.922971487045288 / accuracy: 0.75\n",
      "epoch: 439 / loss: 1.9208474159240723 / accuracy: 0.75\n",
      "epoch: 440 / loss: 1.918717861175537 / accuracy: 0.75\n",
      "epoch: 441 / loss: 1.9165825843811035 / accuracy: 0.75\n",
      "epoch: 442 / loss: 1.9144415855407715 / accuracy: 0.75\n",
      "epoch: 443 / loss: 1.912294626235962 / accuracy: 0.75\n",
      "epoch: 444 / loss: 1.9101417064666748 / accuracy: 0.75\n",
      "epoch: 445 / loss: 1.9079824686050415 / accuracy: 0.75\n",
      "epoch: 446 / loss: 1.905816674232483 / accuracy: 0.75\n",
      "epoch: 447 / loss: 1.903644323348999 / accuracy: 0.75\n",
      "epoch: 448 / loss: 1.9014651775360107 / accuracy: 0.75\n",
      "epoch: 449 / loss: 1.8992791175842285 / accuracy: 0.75\n",
      "epoch: 450 / loss: 1.8970861434936523 / accuracy: 0.75\n",
      "epoch: 451 / loss: 1.8948850631713867 / accuracy: 0.75\n",
      "epoch: 452 / loss: 1.892676830291748 / accuracy: 0.75\n",
      "epoch: 453 / loss: 1.890460729598999 / accuracy: 0.75\n",
      "epoch: 454 / loss: 1.8882368803024292 / accuracy: 0.75\n",
      "epoch: 455 / loss: 1.8860046863555908 / accuracy: 0.75\n",
      "epoch: 456 / loss: 1.8837642669677734 / accuracy: 0.75\n",
      "epoch: 457 / loss: 1.8815149068832397 / accuracy: 0.75\n",
      "epoch: 458 / loss: 1.8792572021484375 / accuracy: 0.75\n",
      "epoch: 459 / loss: 1.876990556716919 / accuracy: 0.75\n",
      "epoch: 460 / loss: 1.8747141361236572 / accuracy: 0.75\n",
      "epoch: 461 / loss: 1.8724285364151 / accuracy: 0.75\n",
      "epoch: 462 / loss: 1.870133399963379 / accuracy: 0.75\n",
      "epoch: 463 / loss: 1.8678282499313354 / accuracy: 0.75\n",
      "epoch: 464 / loss: 1.8655128479003906 / accuracy: 0.75\n",
      "epoch: 465 / loss: 1.863187551498413 / accuracy: 0.75\n",
      "epoch: 466 / loss: 1.8608512878417969 / accuracy: 0.75\n",
      "epoch: 467 / loss: 1.858504295349121 / accuracy: 0.75\n",
      "epoch: 468 / loss: 1.8561460971832275 / accuracy: 0.75\n",
      "epoch: 469 / loss: 1.8537766933441162 / accuracy: 0.75\n",
      "epoch: 470 / loss: 1.8513957262039185 / accuracy: 0.75\n",
      "epoch: 471 / loss: 1.8490028381347656 / accuracy: 0.75\n",
      "epoch: 472 / loss: 1.8465981483459473 / accuracy: 0.75\n",
      "epoch: 473 / loss: 1.8441810607910156 / accuracy: 0.75\n",
      "epoch: 474 / loss: 1.8417515754699707 / accuracy: 0.75\n",
      "epoch: 475 / loss: 1.839308738708496 / accuracy: 0.75\n",
      "epoch: 476 / loss: 1.8368533849716187 / accuracy: 0.75\n",
      "epoch: 477 / loss: 1.834384560585022 / accuracy: 0.75\n",
      "epoch: 478 / loss: 1.831902027130127 / accuracy: 0.75\n",
      "epoch: 479 / loss: 1.8294055461883545 / accuracy: 0.75\n",
      "epoch: 480 / loss: 1.8268952369689941 / accuracy: 0.75\n",
      "epoch: 481 / loss: 1.8243701457977295 / accuracy: 0.75\n",
      "epoch: 482 / loss: 1.8218307495117188 / accuracy: 0.75\n",
      "epoch: 483 / loss: 1.819276213645935 / accuracy: 0.75\n",
      "epoch: 484 / loss: 1.8167065382003784 / accuracy: 0.75\n",
      "epoch: 485 / loss: 1.8141212463378906 / accuracy: 0.75\n",
      "epoch: 486 / loss: 1.8115205764770508 / accuracy: 0.75\n",
      "epoch: 487 / loss: 1.808903455734253 / accuracy: 0.75\n",
      "epoch: 488 / loss: 1.8062703609466553 / accuracy: 0.75\n",
      "epoch: 489 / loss: 1.803620457649231 / accuracy: 0.75\n",
      "epoch: 490 / loss: 1.8009538650512695 / accuracy: 0.75\n",
      "epoch: 491 / loss: 1.7982702255249023 / accuracy: 0.75\n",
      "epoch: 492 / loss: 1.7955689430236816 / accuracy: 0.75\n",
      "epoch: 493 / loss: 1.7928502559661865 / accuracy: 0.75\n",
      "epoch: 494 / loss: 1.7901134490966797 / accuracy: 0.75\n",
      "epoch: 495 / loss: 1.7873585224151611 / accuracy: 0.75\n",
      "epoch: 496 / loss: 1.7845851182937622 / accuracy: 0.75\n",
      "epoch: 497 / loss: 1.781793236732483 / accuracy: 0.75\n",
      "epoch: 498 / loss: 1.778982162475586 / accuracy: 0.75\n",
      "epoch: 499 / loss: 1.7761516571044922 / accuracy: 0.75\n",
      "epoch: 500 / loss: 1.7733018398284912 / accuracy: 0.75\n",
      "epoch: 501 / loss: 1.7704323530197144 / accuracy: 0.75\n",
      "epoch: 502 / loss: 1.767542839050293 / accuracy: 0.75\n",
      "epoch: 503 / loss: 1.7646329402923584 / accuracy: 0.75\n",
      "epoch: 504 / loss: 1.7617026567459106 / accuracy: 0.75\n",
      "epoch: 505 / loss: 1.758751392364502 / accuracy: 0.75\n",
      "epoch: 506 / loss: 1.755779504776001 / accuracy: 0.75\n",
      "epoch: 507 / loss: 1.7527862787246704 / accuracy: 0.75\n",
      "epoch: 508 / loss: 1.7497714757919312 / accuracy: 0.75\n",
      "epoch: 509 / loss: 1.7467353343963623 / accuracy: 0.75\n",
      "epoch: 510 / loss: 1.7436771392822266 / accuracy: 0.75\n",
      "epoch: 511 / loss: 1.7405970096588135 / accuracy: 0.75\n",
      "epoch: 512 / loss: 1.7374944686889648 / accuracy: 0.75\n",
      "epoch: 513 / loss: 1.7343696355819702 / accuracy: 0.75\n",
      "epoch: 514 / loss: 1.7312220335006714 / accuracy: 0.75\n",
      "epoch: 515 / loss: 1.7280516624450684 / accuracy: 0.75\n",
      "epoch: 516 / loss: 1.724858283996582 / accuracy: 0.75\n",
      "epoch: 517 / loss: 1.721642017364502 / accuracy: 0.75\n",
      "epoch: 518 / loss: 1.7184022665023804 / accuracy: 0.75\n",
      "epoch: 519 / loss: 1.7151391506195068 / accuracy: 0.75\n",
      "epoch: 520 / loss: 1.7118524312973022 / accuracy: 0.75\n",
      "epoch: 521 / loss: 1.708541989326477 / accuracy: 0.75\n",
      "epoch: 522 / loss: 1.7052078247070312 / accuracy: 0.75\n",
      "epoch: 523 / loss: 1.7018496990203857 / accuracy: 0.75\n",
      "epoch: 524 / loss: 1.6984676122665405 / accuracy: 0.75\n",
      "epoch: 525 / loss: 1.695061445236206 / accuracy: 0.75\n",
      "epoch: 526 / loss: 1.6916310787200928 / accuracy: 0.75\n",
      "epoch: 527 / loss: 1.6881763935089111 / accuracy: 0.75\n",
      "epoch: 528 / loss: 1.6846977472305298 / accuracy: 0.75\n",
      "epoch: 529 / loss: 1.681194543838501 / accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 530 / loss: 1.677667260169983 / accuracy: 0.75\n",
      "epoch: 531 / loss: 1.6741156578063965 / accuracy: 0.75\n",
      "epoch: 532 / loss: 1.6705397367477417 / accuracy: 0.75\n",
      "epoch: 533 / loss: 1.6669392585754395 / accuracy: 0.75\n",
      "epoch: 534 / loss: 1.6633145809173584 / accuracy: 0.75\n",
      "epoch: 535 / loss: 1.6596657037734985 / accuracy: 0.75\n",
      "epoch: 536 / loss: 1.6559927463531494 / accuracy: 0.75\n",
      "epoch: 537 / loss: 1.6522951126098633 / accuracy: 0.75\n",
      "epoch: 538 / loss: 1.6485737562179565 / accuracy: 0.75\n",
      "epoch: 539 / loss: 1.64482843875885 / accuracy: 0.75\n",
      "epoch: 540 / loss: 1.6410590410232544 / accuracy: 0.75\n",
      "epoch: 541 / loss: 1.637265920639038 / accuracy: 0.75\n",
      "epoch: 542 / loss: 1.6334491968154907 / accuracy: 0.75\n",
      "epoch: 543 / loss: 1.6296088695526123 / accuracy: 0.75\n",
      "epoch: 544 / loss: 1.6257450580596924 / accuracy: 0.75\n",
      "epoch: 545 / loss: 1.6218581199645996 / accuracy: 0.75\n",
      "epoch: 546 / loss: 1.617948055267334 / accuracy: 0.75\n",
      "epoch: 547 / loss: 1.6140148639678955 / accuracy: 0.75\n",
      "epoch: 548 / loss: 1.6100594997406006 / accuracy: 0.75\n",
      "epoch: 549 / loss: 1.6060808897018433 / accuracy: 0.75\n",
      "epoch: 550 / loss: 1.6020807027816772 / accuracy: 0.75\n",
      "epoch: 551 / loss: 1.5980579853057861 / accuracy: 0.75\n",
      "epoch: 552 / loss: 1.5940135717391968 / accuracy: 0.75\n",
      "epoch: 553 / loss: 1.5899475812911987 / accuracy: 0.75\n",
      "epoch: 554 / loss: 1.5858601331710815 / accuracy: 0.75\n",
      "epoch: 555 / loss: 1.5817517042160034 / accuracy: 0.75\n",
      "epoch: 556 / loss: 1.5776220560073853 / accuracy: 0.75\n",
      "epoch: 557 / loss: 1.5734723806381226 / accuracy: 0.75\n",
      "epoch: 558 / loss: 1.5693023204803467 / accuracy: 0.75\n",
      "epoch: 559 / loss: 1.5651121139526367 / accuracy: 0.75\n",
      "epoch: 560 / loss: 1.5609023571014404 / accuracy: 0.75\n",
      "epoch: 561 / loss: 1.556673288345337 / accuracy: 0.75\n",
      "epoch: 562 / loss: 1.5524251461029053 / accuracy: 0.75\n",
      "epoch: 563 / loss: 1.5481581687927246 / accuracy: 0.75\n",
      "epoch: 564 / loss: 1.5438728332519531 / accuracy: 0.75\n",
      "epoch: 565 / loss: 1.53956937789917 / accuracy: 0.75\n",
      "epoch: 566 / loss: 1.5352483987808228 / accuracy: 0.75\n",
      "epoch: 567 / loss: 1.530909776687622 / accuracy: 0.75\n",
      "epoch: 568 / loss: 1.5265545845031738 / accuracy: 0.75\n",
      "epoch: 569 / loss: 1.5221822261810303 / accuracy: 0.75\n",
      "epoch: 570 / loss: 1.517793893814087 / accuracy: 0.75\n",
      "epoch: 571 / loss: 1.5133894681930542 / accuracy: 0.75\n",
      "epoch: 572 / loss: 1.5089696645736694 / accuracy: 0.75\n",
      "epoch: 573 / loss: 1.504534363746643 / accuracy: 0.75\n",
      "epoch: 574 / loss: 1.500084400177002 / accuracy: 0.75\n",
      "epoch: 575 / loss: 1.495619773864746 / accuracy: 0.75\n",
      "epoch: 576 / loss: 1.4911413192749023 / accuracy: 0.75\n",
      "epoch: 577 / loss: 1.4866490364074707 / accuracy: 0.75\n",
      "epoch: 578 / loss: 1.482143521308899 / accuracy: 0.75\n",
      "epoch: 579 / loss: 1.4776246547698975 / accuracy: 0.75\n",
      "epoch: 580 / loss: 1.4730935096740723 / accuracy: 0.75\n",
      "epoch: 581 / loss: 1.4685498476028442 / accuracy: 0.75\n",
      "epoch: 582 / loss: 1.4639945030212402 / accuracy: 0.75\n",
      "epoch: 583 / loss: 1.459428071975708 / accuracy: 0.75\n",
      "epoch: 584 / loss: 1.4548499584197998 / accuracy: 0.75\n",
      "epoch: 585 / loss: 1.4502613544464111 / accuracy: 0.75\n",
      "epoch: 586 / loss: 1.445662498474121 / accuracy: 0.75\n",
      "epoch: 587 / loss: 1.4410536289215088 / accuracy: 0.75\n",
      "epoch: 588 / loss: 1.4364351034164429 / accuracy: 0.75\n",
      "epoch: 589 / loss: 1.4318077564239502 / accuracy: 1.0\n",
      "epoch: 590 / loss: 1.427170991897583 / accuracy: 1.0\n",
      "epoch: 591 / loss: 1.422526240348816 / accuracy: 1.0\n",
      "epoch: 592 / loss: 1.4178731441497803 / accuracy: 1.0\n",
      "epoch: 593 / loss: 1.4132126569747925 / accuracy: 1.0\n",
      "epoch: 594 / loss: 1.4085445404052734 / accuracy: 1.0\n",
      "epoch: 595 / loss: 1.4038692712783813 / accuracy: 1.0\n",
      "epoch: 596 / loss: 1.3991875648498535 / accuracy: 1.0\n",
      "epoch: 597 / loss: 1.3944995403289795 / accuracy: 1.0\n",
      "epoch: 598 / loss: 1.389805793762207 / accuracy: 1.0\n",
      "epoch: 599 / loss: 1.3851062059402466 / accuracy: 1.0\n",
      "epoch: 600 / loss: 1.380401611328125 / accuracy: 1.0\n",
      "epoch: 601 / loss: 1.3756920099258423 / accuracy: 1.0\n",
      "epoch: 602 / loss: 1.3709774017333984 / accuracy: 1.0\n",
      "epoch: 603 / loss: 1.366259217262268 / accuracy: 1.0\n",
      "epoch: 604 / loss: 1.361537218093872 / accuracy: 1.0\n",
      "epoch: 605 / loss: 1.356811285018921 / accuracy: 1.0\n",
      "epoch: 606 / loss: 1.3520824909210205 / accuracy: 1.0\n",
      "epoch: 607 / loss: 1.3473505973815918 / accuracy: 1.0\n",
      "epoch: 608 / loss: 1.342616319656372 / accuracy: 1.0\n",
      "epoch: 609 / loss: 1.3378797769546509 / accuracy: 1.0\n",
      "epoch: 610 / loss: 1.3331414461135864 / accuracy: 1.0\n",
      "epoch: 611 / loss: 1.3284014463424683 / accuracy: 1.0\n",
      "epoch: 612 / loss: 1.323660135269165 / accuracy: 1.0\n",
      "epoch: 613 / loss: 1.318917989730835 / accuracy: 1.0\n",
      "epoch: 614 / loss: 1.3141752481460571 / accuracy: 1.0\n",
      "epoch: 615 / loss: 1.309432029724121 / accuracy: 1.0\n",
      "epoch: 616 / loss: 1.3046889305114746 / accuracy: 1.0\n",
      "epoch: 617 / loss: 1.2999458312988281 / accuracy: 1.0\n",
      "epoch: 618 / loss: 1.295203447341919 / accuracy: 1.0\n",
      "epoch: 619 / loss: 1.2904620170593262 / accuracy: 1.0\n",
      "epoch: 620 / loss: 1.2857216596603394 / accuracy: 1.0\n",
      "epoch: 621 / loss: 1.2809829711914062 / accuracy: 1.0\n",
      "epoch: 622 / loss: 1.2762455940246582 / accuracy: 1.0\n",
      "epoch: 623 / loss: 1.2715106010437012 / accuracy: 1.0\n",
      "epoch: 624 / loss: 1.266777515411377 / accuracy: 1.0\n",
      "epoch: 625 / loss: 1.2620474100112915 / accuracy: 1.0\n",
      "epoch: 626 / loss: 1.2573199272155762 / accuracy: 1.0\n",
      "epoch: 627 / loss: 1.2525951862335205 / accuracy: 1.0\n",
      "epoch: 628 / loss: 1.2478742599487305 / accuracy: 1.0\n",
      "epoch: 629 / loss: 1.243156909942627 / accuracy: 1.0\n",
      "epoch: 630 / loss: 1.2384432554244995 / accuracy: 1.0\n",
      "epoch: 631 / loss: 1.233734130859375 / accuracy: 1.0\n",
      "epoch: 632 / loss: 1.2290289402008057 / accuracy: 1.0\n",
      "epoch: 633 / loss: 1.2243289947509766 / accuracy: 1.0\n",
      "epoch: 634 / loss: 1.2196334600448608 / accuracy: 1.0\n",
      "epoch: 635 / loss: 1.2149431705474854 / accuracy: 1.0\n",
      "epoch: 636 / loss: 1.2102584838867188 / accuracy: 1.0\n",
      "epoch: 637 / loss: 1.2055795192718506 / accuracy: 1.0\n",
      "epoch: 638 / loss: 1.2009060382843018 / accuracy: 1.0\n",
      "epoch: 639 / loss: 1.1962388753890991 / accuracy: 1.0\n",
      "epoch: 640 / loss: 1.1915781497955322 / accuracy: 1.0\n",
      "epoch: 641 / loss: 1.1869237422943115 / accuracy: 1.0\n",
      "epoch: 642 / loss: 1.1822763681411743 / accuracy: 1.0\n",
      "epoch: 643 / loss: 1.1776361465454102 / accuracy: 1.0\n",
      "epoch: 644 / loss: 1.173003077507019 / accuracy: 1.0\n",
      "epoch: 645 / loss: 1.1683775186538696 / accuracy: 1.0\n",
      "epoch: 646 / loss: 1.1637595891952515 / accuracy: 1.0\n",
      "epoch: 647 / loss: 1.1591497659683228 / accuracy: 1.0\n",
      "epoch: 648 / loss: 1.154548168182373 / accuracy: 1.0\n",
      "epoch: 649 / loss: 1.1499545574188232 / accuracy: 1.0\n",
      "epoch: 650 / loss: 1.145369529724121 / accuracy: 1.0\n",
      "epoch: 651 / loss: 1.1407934427261353 / accuracy: 1.0\n",
      "epoch: 652 / loss: 1.1362262964248657 / accuracy: 1.0\n",
      "epoch: 653 / loss: 1.131668210029602 / accuracy: 1.0\n",
      "epoch: 654 / loss: 1.127119541168213 / accuracy: 1.0\n",
      "epoch: 655 / loss: 1.1225802898406982 / accuracy: 1.0\n",
      "epoch: 656 / loss: 1.1180509328842163 / accuracy: 1.0\n",
      "epoch: 657 / loss: 1.1135315895080566 / accuracy: 1.0\n",
      "epoch: 658 / loss: 1.1090223789215088 / accuracy: 1.0\n",
      "epoch: 659 / loss: 1.1045234203338623 / accuracy: 1.0\n",
      "epoch: 660 / loss: 1.1000348329544067 / accuracy: 1.0\n",
      "epoch: 661 / loss: 1.0955569744110107 / accuracy: 1.0\n",
      "epoch: 662 / loss: 1.0910899639129639 / accuracy: 1.0\n",
      "epoch: 663 / loss: 1.0866338014602661 / accuracy: 1.0\n",
      "epoch: 664 / loss: 1.0821892023086548 / accuracy: 1.0\n",
      "epoch: 665 / loss: 1.0777556896209717 / accuracy: 1.0\n",
      "epoch: 666 / loss: 1.073333740234375 / accuracy: 1.0\n",
      "epoch: 667 / loss: 1.0689234733581543 / accuracy: 1.0\n",
      "epoch: 668 / loss: 1.0645251274108887 / accuracy: 1.0\n",
      "epoch: 669 / loss: 1.060138463973999 / accuracy: 1.0\n",
      "epoch: 670 / loss: 1.0557641983032227 / accuracy: 1.0\n",
      "epoch: 671 / loss: 1.0514020919799805 / accuracy: 1.0\n",
      "epoch: 672 / loss: 1.0470525026321411 / accuracy: 1.0\n",
      "epoch: 673 / loss: 1.0427155494689941 / accuracy: 1.0\n",
      "epoch: 674 / loss: 1.03839111328125 / accuracy: 1.0\n",
      "epoch: 675 / loss: 1.034079909324646 / accuracy: 1.0\n",
      "epoch: 676 / loss: 1.0297813415527344 / accuracy: 1.0\n",
      "epoch: 677 / loss: 1.0254963636398315 / accuracy: 1.0\n",
      "epoch: 678 / loss: 1.0212244987487793 / accuracy: 1.0\n",
      "epoch: 679 / loss: 1.016965627670288 / accuracy: 1.0\n",
      "epoch: 680 / loss: 1.0127205848693848 / accuracy: 1.0\n",
      "epoch: 681 / loss: 1.0084892511367798 / accuracy: 1.0\n",
      "epoch: 682 / loss: 1.004271388053894 / accuracy: 1.0\n",
      "epoch: 683 / loss: 1.0000675916671753 / accuracy: 1.0\n",
      "epoch: 684 / loss: 0.9958778619766235 / accuracy: 1.0\n",
      "epoch: 685 / loss: 0.9917021989822388 / accuracy: 1.0\n",
      "epoch: 686 / loss: 0.9875407218933105 / accuracy: 1.0\n",
      "epoch: 687 / loss: 0.9833933711051941 / accuracy: 1.0\n",
      "epoch: 688 / loss: 0.9792605638504028 / accuracy: 1.0\n",
      "epoch: 689 / loss: 0.9751424193382263 / accuracy: 1.0\n",
      "epoch: 690 / loss: 0.9710388779640198 / accuracy: 1.0\n",
      "epoch: 691 / loss: 0.9669499397277832 / accuracy: 1.0\n",
      "epoch: 692 / loss: 0.9628758430480957 / accuracy: 1.0\n",
      "epoch: 693 / loss: 0.958816409111023 / accuracy: 1.0\n",
      "epoch: 694 / loss: 0.9547722339630127 / accuracy: 1.0\n",
      "epoch: 695 / loss: 0.9507431387901306 / accuracy: 1.0\n",
      "epoch: 696 / loss: 0.9467291831970215 / accuracy: 1.0\n",
      "epoch: 697 / loss: 0.9427303075790405 / accuracy: 1.0\n",
      "epoch: 698 / loss: 0.9387467503547668 / accuracy: 1.0\n",
      "epoch: 699 / loss: 0.9347785711288452 / accuracy: 1.0\n",
      "epoch: 700 / loss: 0.93082594871521 / accuracy: 1.0\n",
      "epoch: 701 / loss: 0.9268887042999268 / accuracy: 1.0\n",
      "epoch: 702 / loss: 0.9229669570922852 / accuracy: 1.0\n",
      "epoch: 703 / loss: 0.9190611243247986 / accuracy: 1.0\n",
      "epoch: 704 / loss: 0.9151709079742432 / accuracy: 1.0\n",
      "epoch: 705 / loss: 0.9112961292266846 / accuracy: 1.0\n",
      "epoch: 706 / loss: 0.9074375629425049 / accuracy: 1.0\n",
      "epoch: 707 / loss: 0.903594434261322 / accuracy: 1.0\n",
      "epoch: 708 / loss: 0.8997675180435181 / accuracy: 1.0\n",
      "epoch: 709 / loss: 0.8959564566612244 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 710 / loss: 0.8921613693237305 / accuracy: 1.0\n",
      "epoch: 711 / loss: 0.8883822560310364 / accuracy: 1.0\n",
      "epoch: 712 / loss: 0.8846191167831421 / accuracy: 1.0\n",
      "epoch: 713 / loss: 0.8808722496032715 / accuracy: 1.0\n",
      "epoch: 714 / loss: 0.8771417140960693 / accuracy: 1.0\n",
      "epoch: 715 / loss: 0.873427152633667 / accuracy: 1.0\n",
      "epoch: 716 / loss: 0.8697288632392883 / accuracy: 1.0\n",
      "epoch: 717 / loss: 0.8660464286804199 / accuracy: 1.0\n",
      "epoch: 718 / loss: 0.8623805642127991 / accuracy: 1.0\n",
      "epoch: 719 / loss: 0.8587312698364258 / accuracy: 1.0\n",
      "epoch: 720 / loss: 0.855097770690918 / accuracy: 1.0\n",
      "epoch: 721 / loss: 0.8514807224273682 / accuracy: 1.0\n",
      "epoch: 722 / loss: 0.8478801250457764 / accuracy: 1.0\n",
      "epoch: 723 / loss: 0.8442957401275635 / accuracy: 1.0\n",
      "epoch: 724 / loss: 0.8407281637191772 / accuracy: 1.0\n",
      "epoch: 725 / loss: 0.8371764421463013 / accuracy: 1.0\n",
      "epoch: 726 / loss: 0.8336412310600281 / accuracy: 1.0\n",
      "epoch: 727 / loss: 0.8301225900650024 / accuracy: 1.0\n",
      "epoch: 728 / loss: 0.8266201019287109 / accuracy: 1.0\n",
      "epoch: 729 / loss: 0.8231342434883118 / accuracy: 1.0\n",
      "epoch: 730 / loss: 0.819664478302002 / accuracy: 1.0\n",
      "epoch: 731 / loss: 0.8162112236022949 / accuracy: 1.0\n",
      "epoch: 732 / loss: 0.8127743601799011 / accuracy: 1.0\n",
      "epoch: 733 / loss: 0.8093538880348206 / accuracy: 1.0\n",
      "epoch: 734 / loss: 0.8059498071670532 / accuracy: 1.0\n",
      "epoch: 735 / loss: 0.8025617599487305 / accuracy: 1.0\n",
      "epoch: 736 / loss: 0.7991905212402344 / accuracy: 1.0\n",
      "epoch: 737 / loss: 0.7958356142044067 / accuracy: 1.0\n",
      "epoch: 738 / loss: 0.7924966812133789 / accuracy: 1.0\n",
      "epoch: 739 / loss: 0.7891741991043091 / accuracy: 1.0\n",
      "epoch: 740 / loss: 0.7858679890632629 / accuracy: 1.0\n",
      "epoch: 741 / loss: 0.78257817029953 / accuracy: 1.0\n",
      "epoch: 742 / loss: 0.7793046236038208 / accuracy: 1.0\n",
      "epoch: 743 / loss: 0.7760472297668457 / accuracy: 1.0\n",
      "epoch: 744 / loss: 0.7728061676025391 / accuracy: 1.0\n",
      "epoch: 745 / loss: 0.7695809602737427 / accuracy: 1.0\n",
      "epoch: 746 / loss: 0.7663720846176147 / accuracy: 1.0\n",
      "epoch: 747 / loss: 0.763179361820221 / accuracy: 1.0\n",
      "epoch: 748 / loss: 0.7600027918815613 / accuracy: 1.0\n",
      "epoch: 749 / loss: 0.7568422555923462 / accuracy: 1.0\n",
      "epoch: 750 / loss: 0.7536976337432861 / accuracy: 1.0\n",
      "epoch: 751 / loss: 0.7505689859390259 / accuracy: 1.0\n",
      "epoch: 752 / loss: 0.7474563717842102 / accuracy: 1.0\n",
      "epoch: 753 / loss: 0.7443599700927734 / accuracy: 1.0\n",
      "epoch: 754 / loss: 0.7412790060043335 / accuracy: 1.0\n",
      "epoch: 755 / loss: 0.7382139563560486 / accuracy: 1.0\n",
      "epoch: 756 / loss: 0.7351649403572083 / accuracy: 1.0\n",
      "epoch: 757 / loss: 0.7321314811706543 / accuracy: 1.0\n",
      "epoch: 758 / loss: 0.7291138172149658 / accuracy: 1.0\n",
      "epoch: 759 / loss: 0.7261121273040771 / accuracy: 1.0\n",
      "epoch: 760 / loss: 0.723125696182251 / accuracy: 1.0\n",
      "epoch: 761 / loss: 0.720154881477356 / accuracy: 1.0\n",
      "epoch: 762 / loss: 0.7171997427940369 / accuracy: 1.0\n",
      "epoch: 763 / loss: 0.7142601013183594 / accuracy: 1.0\n",
      "epoch: 764 / loss: 0.7113358974456787 / accuracy: 1.0\n",
      "epoch: 765 / loss: 0.7084271907806396 / accuracy: 1.0\n",
      "epoch: 766 / loss: 0.7055338621139526 / accuracy: 1.0\n",
      "epoch: 767 / loss: 0.7026556730270386 / accuracy: 1.0\n",
      "epoch: 768 / loss: 0.6997929811477661 / accuracy: 1.0\n",
      "epoch: 769 / loss: 0.696945309638977 / accuracy: 1.0\n",
      "epoch: 770 / loss: 0.6941128373146057 / accuracy: 1.0\n",
      "epoch: 771 / loss: 0.6912952661514282 / accuracy: 1.0\n",
      "epoch: 772 / loss: 0.6884930729866028 / accuracy: 1.0\n",
      "epoch: 773 / loss: 0.6857054829597473 / accuracy: 1.0\n",
      "epoch: 774 / loss: 0.6829333305358887 / accuracy: 1.0\n",
      "epoch: 775 / loss: 0.6801756620407104 / accuracy: 1.0\n",
      "epoch: 776 / loss: 0.6774330139160156 / accuracy: 1.0\n",
      "epoch: 777 / loss: 0.6747051477432251 / accuracy: 1.0\n",
      "epoch: 778 / loss: 0.6719920039176941 / accuracy: 1.0\n",
      "epoch: 779 / loss: 0.669293224811554 / accuracy: 1.0\n",
      "epoch: 780 / loss: 0.6666092872619629 / accuracy: 1.0\n",
      "epoch: 781 / loss: 0.6639398336410522 / accuracy: 1.0\n",
      "epoch: 782 / loss: 0.6612849831581116 / accuracy: 1.0\n",
      "epoch: 783 / loss: 0.6586441993713379 / accuracy: 1.0\n",
      "epoch: 784 / loss: 0.6560181379318237 / accuracy: 1.0\n",
      "epoch: 785 / loss: 0.653406023979187 / accuracy: 1.0\n",
      "epoch: 786 / loss: 0.6508085131645203 / accuracy: 1.0\n",
      "epoch: 787 / loss: 0.648224949836731 / accuracy: 1.0\n",
      "epoch: 788 / loss: 0.645655632019043 / accuracy: 1.0\n",
      "epoch: 789 / loss: 0.643100380897522 / accuracy: 1.0\n",
      "epoch: 790 / loss: 0.6405588388442993 / accuracy: 1.0\n",
      "epoch: 791 / loss: 0.638031542301178 / accuracy: 1.0\n",
      "epoch: 792 / loss: 0.6355177164077759 / accuracy: 1.0\n",
      "epoch: 793 / loss: 0.6330180764198303 / accuracy: 1.0\n",
      "epoch: 794 / loss: 0.630531907081604 / accuracy: 1.0\n",
      "epoch: 795 / loss: 0.6280591487884521 / accuracy: 1.0\n",
      "epoch: 796 / loss: 0.6256006360054016 / accuracy: 1.0\n",
      "epoch: 797 / loss: 0.6231551766395569 / accuracy: 1.0\n",
      "epoch: 798 / loss: 0.620723307132721 / accuracy: 1.0\n",
      "epoch: 799 / loss: 0.6183046102523804 / accuracy: 1.0\n",
      "epoch: 800 / loss: 0.6158996820449829 / accuracy: 1.0\n",
      "epoch: 801 / loss: 0.6135077476501465 / accuracy: 1.0\n",
      "epoch: 802 / loss: 0.6111289262771606 / accuracy: 1.0\n",
      "epoch: 803 / loss: 0.6087634563446045 / accuracy: 1.0\n",
      "epoch: 804 / loss: 0.6064109802246094 / accuracy: 1.0\n",
      "epoch: 805 / loss: 0.6040714979171753 / accuracy: 1.0\n",
      "epoch: 806 / loss: 0.601745069026947 / accuracy: 1.0\n",
      "epoch: 807 / loss: 0.5994312763214111 / accuracy: 1.0\n",
      "epoch: 808 / loss: 0.597130537033081 / accuracy: 1.0\n",
      "epoch: 809 / loss: 0.5948423147201538 / accuracy: 1.0\n",
      "epoch: 810 / loss: 0.5925667881965637 / accuracy: 1.0\n",
      "epoch: 811 / loss: 0.5903040170669556 / accuracy: 1.0\n",
      "epoch: 812 / loss: 0.5880537033081055 / accuracy: 1.0\n",
      "epoch: 813 / loss: 0.5858158469200134 / accuracy: 1.0\n",
      "epoch: 814 / loss: 0.5835902690887451 / accuracy: 1.0\n",
      "epoch: 815 / loss: 0.5813771486282349 / accuracy: 1.0\n",
      "epoch: 816 / loss: 0.5791763663291931 / accuracy: 1.0\n",
      "epoch: 817 / loss: 0.5769877433776855 / accuracy: 1.0\n",
      "epoch: 818 / loss: 0.5748111009597778 / accuracy: 1.0\n",
      "epoch: 819 / loss: 0.5726468563079834 / accuracy: 1.0\n",
      "epoch: 820 / loss: 0.5704942345619202 / accuracy: 1.0\n",
      "epoch: 821 / loss: 0.5683538317680359 / accuracy: 1.0\n",
      "epoch: 822 / loss: 0.5662251710891724 / accuracy: 1.0\n",
      "epoch: 823 / loss: 0.5641081929206848 / accuracy: 1.0\n",
      "epoch: 824 / loss: 0.5620032548904419 / accuracy: 1.0\n",
      "epoch: 825 / loss: 0.5599098205566406 / accuracy: 1.0\n",
      "epoch: 826 / loss: 0.557827889919281 / accuracy: 1.0\n",
      "epoch: 827 / loss: 0.5557576417922974 / accuracy: 1.0\n",
      "epoch: 828 / loss: 0.5536989569664001 / accuracy: 1.0\n",
      "epoch: 829 / loss: 0.5516515374183655 / accuracy: 1.0\n",
      "epoch: 830 / loss: 0.5496153831481934 / accuracy: 1.0\n",
      "epoch: 831 / loss: 0.5475906729698181 / accuracy: 1.0\n",
      "epoch: 832 / loss: 0.5455771088600159 / accuracy: 1.0\n",
      "epoch: 833 / loss: 0.5435745716094971 / accuracy: 1.0\n",
      "epoch: 834 / loss: 0.5415833592414856 / accuracy: 1.0\n",
      "epoch: 835 / loss: 0.539603054523468 / accuracy: 1.0\n",
      "epoch: 836 / loss: 0.5376335382461548 / accuracy: 1.0\n",
      "epoch: 837 / loss: 0.5356752276420593 / accuracy: 1.0\n",
      "epoch: 838 / loss: 0.5337273478507996 / accuracy: 1.0\n",
      "epoch: 839 / loss: 0.5317902565002441 / accuracy: 1.0\n",
      "epoch: 840 / loss: 0.5298643112182617 / accuracy: 1.0\n",
      "epoch: 841 / loss: 0.5279487371444702 / accuracy: 1.0\n",
      "epoch: 842 / loss: 0.5260436534881592 / accuracy: 1.0\n",
      "epoch: 843 / loss: 0.5241491794586182 / accuracy: 1.0\n",
      "epoch: 844 / loss: 0.5222652554512024 / accuracy: 1.0\n",
      "epoch: 845 / loss: 0.520391583442688 / accuracy: 1.0\n",
      "epoch: 846 / loss: 0.5185282230377197 / accuracy: 1.0\n",
      "epoch: 847 / loss: 0.5166751742362976 / accuracy: 1.0\n",
      "epoch: 848 / loss: 0.5148323774337769 / accuracy: 1.0\n",
      "epoch: 849 / loss: 0.5129995942115784 / accuracy: 1.0\n",
      "epoch: 850 / loss: 0.5111767649650574 / accuracy: 1.0\n",
      "epoch: 851 / loss: 0.5093640089035034 / accuracy: 1.0\n",
      "epoch: 852 / loss: 0.5075615644454956 / accuracy: 1.0\n",
      "epoch: 853 / loss: 0.5057686567306519 / accuracy: 1.0\n",
      "epoch: 854 / loss: 0.5039855241775513 / accuracy: 1.0\n",
      "epoch: 855 / loss: 0.5022123456001282 / accuracy: 1.0\n",
      "epoch: 856 / loss: 0.5004487633705139 / accuracy: 1.0\n",
      "epoch: 857 / loss: 0.4986949563026428 / accuracy: 1.0\n",
      "epoch: 858 / loss: 0.49695059657096863 / accuracy: 1.0\n",
      "epoch: 859 / loss: 0.4952160120010376 / accuracy: 1.0\n",
      "epoch: 860 / loss: 0.4934905469417572 / accuracy: 1.0\n",
      "epoch: 861 / loss: 0.4917747974395752 / accuracy: 1.0\n",
      "epoch: 862 / loss: 0.4900682270526886 / accuracy: 1.0\n",
      "epoch: 863 / loss: 0.48837101459503174 / accuracy: 1.0\n",
      "epoch: 864 / loss: 0.4866830110549927 / accuracy: 1.0\n",
      "epoch: 865 / loss: 0.485004186630249 / accuracy: 1.0\n",
      "epoch: 866 / loss: 0.48333442211151123 / accuracy: 1.0\n",
      "epoch: 867 / loss: 0.481673926115036 / accuracy: 1.0\n",
      "epoch: 868 / loss: 0.48002225160598755 / accuracy: 1.0\n",
      "epoch: 869 / loss: 0.4783795177936554 / accuracy: 1.0\n",
      "epoch: 870 / loss: 0.47674596309661865 / accuracy: 1.0\n",
      "epoch: 871 / loss: 0.47512078285217285 / accuracy: 1.0\n",
      "epoch: 872 / loss: 0.4735047519207001 / accuracy: 1.0\n",
      "epoch: 873 / loss: 0.4718972444534302 / accuracy: 1.0\n",
      "epoch: 874 / loss: 0.47029855847358704 / accuracy: 1.0\n",
      "epoch: 875 / loss: 0.4687085449695587 / accuracy: 1.0\n",
      "epoch: 876 / loss: 0.4671269655227661 / accuracy: 1.0\n",
      "epoch: 877 / loss: 0.46555382013320923 / accuracy: 1.0\n",
      "epoch: 878 / loss: 0.4639892280101776 / accuracy: 1.0\n",
      "epoch: 879 / loss: 0.46243301033973694 / accuracy: 1.0\n",
      "epoch: 880 / loss: 0.4608851969242096 / accuracy: 1.0\n",
      "epoch: 881 / loss: 0.45934563875198364 / accuracy: 1.0\n",
      "epoch: 882 / loss: 0.45781442523002625 / accuracy: 1.0\n",
      "epoch: 883 / loss: 0.4562913179397583 / accuracy: 1.0\n",
      "epoch: 884 / loss: 0.4547763466835022 / accuracy: 1.0\n",
      "epoch: 885 / loss: 0.45326951146125793 / accuracy: 1.0\n",
      "epoch: 886 / loss: 0.4517707824707031 / accuracy: 1.0\n",
      "epoch: 887 / loss: 0.45027992129325867 / accuracy: 1.0\n",
      "epoch: 888 / loss: 0.44879674911499023 / accuracy: 1.0\n",
      "epoch: 889 / loss: 0.44732195138931274 / accuracy: 1.0\n",
      "epoch: 890 / loss: 0.4458546042442322 / accuracy: 1.0\n",
      "epoch: 891 / loss: 0.4443954527378082 / accuracy: 1.0\n",
      "epoch: 892 / loss: 0.4429437816143036 / accuracy: 1.0\n",
      "epoch: 893 / loss: 0.44149982929229736 / accuracy: 1.0\n",
      "epoch: 894 / loss: 0.44006359577178955 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 895 / loss: 0.4386346936225891 / accuracy: 1.0\n",
      "epoch: 896 / loss: 0.4372135400772095 / accuracy: 1.0\n",
      "epoch: 897 / loss: 0.435799777507782 / accuracy: 1.0\n",
      "epoch: 898 / loss: 0.4343934953212738 / accuracy: 1.0\n",
      "epoch: 899 / loss: 0.4329947233200073 / accuracy: 1.0\n",
      "epoch: 900 / loss: 0.43160316348075867 / accuracy: 1.0\n",
      "epoch: 901 / loss: 0.43021902441978455 / accuracy: 1.0\n",
      "epoch: 902 / loss: 0.42884206771850586 / accuracy: 1.0\n",
      "epoch: 903 / loss: 0.4274722635746002 / accuracy: 1.0\n",
      "epoch: 904 / loss: 0.42610964179039 / accuracy: 1.0\n",
      "epoch: 905 / loss: 0.4247540831565857 / accuracy: 1.0\n",
      "epoch: 906 / loss: 0.42340582609176636 / accuracy: 1.0\n",
      "epoch: 907 / loss: 0.42206448316574097 / accuracy: 1.0\n",
      "epoch: 908 / loss: 0.4207301139831543 / accuracy: 1.0\n",
      "epoch: 909 / loss: 0.4194026589393616 / accuracy: 1.0\n",
      "epoch: 910 / loss: 0.4180818796157837 / accuracy: 1.0\n",
      "epoch: 911 / loss: 0.41676831245422363 / accuracy: 1.0\n",
      "epoch: 912 / loss: 0.41546130180358887 / accuracy: 1.0\n",
      "epoch: 913 / loss: 0.41416117548942566 / accuracy: 1.0\n",
      "epoch: 914 / loss: 0.412867933511734 / accuracy: 1.0\n",
      "epoch: 915 / loss: 0.4115810990333557 / accuracy: 1.0\n",
      "epoch: 916 / loss: 0.4103008806705475 / accuracy: 1.0\n",
      "epoch: 917 / loss: 0.4090273976325989 / accuracy: 1.0\n",
      "epoch: 918 / loss: 0.40776029229164124 / accuracy: 1.0\n",
      "epoch: 919 / loss: 0.4064997732639313 / accuracy: 1.0\n",
      "epoch: 920 / loss: 0.40524590015411377 / accuracy: 1.0\n",
      "epoch: 921 / loss: 0.40399813652038574 / accuracy: 1.0\n",
      "epoch: 922 / loss: 0.402756929397583 / accuracy: 1.0\n",
      "epoch: 923 / loss: 0.40152209997177124 / accuracy: 1.0\n",
      "epoch: 924 / loss: 0.40029361844062805 / accuracy: 1.0\n",
      "epoch: 925 / loss: 0.39907121658325195 / accuracy: 1.0\n",
      "epoch: 926 / loss: 0.3978552222251892 / accuracy: 1.0\n",
      "epoch: 927 / loss: 0.396645188331604 / accuracy: 1.0\n",
      "epoch: 928 / loss: 0.3954413831233978 / accuracy: 1.0\n",
      "epoch: 929 / loss: 0.39424383640289307 / accuracy: 1.0\n",
      "epoch: 930 / loss: 0.3930521607398987 / accuracy: 1.0\n",
      "epoch: 931 / loss: 0.39186644554138184 / accuracy: 1.0\n",
      "epoch: 932 / loss: 0.39068686962127686 / accuracy: 1.0\n",
      "epoch: 933 / loss: 0.38951319456100464 / accuracy: 1.0\n",
      "epoch: 934 / loss: 0.3883454203605652 / accuracy: 1.0\n",
      "epoch: 935 / loss: 0.3871835768222809 / accuracy: 1.0\n",
      "epoch: 936 / loss: 0.3860274851322174 / accuracy: 1.0\n",
      "epoch: 937 / loss: 0.3848772943019867 / accuracy: 1.0\n",
      "epoch: 938 / loss: 0.3837326765060425 / accuracy: 1.0\n",
      "epoch: 939 / loss: 0.3825939893722534 / accuracy: 1.0\n",
      "epoch: 940 / loss: 0.3814607858657837 / accuracy: 1.0\n",
      "epoch: 941 / loss: 0.3803333044052124 / accuracy: 1.0\n",
      "epoch: 942 / loss: 0.37921157479286194 / accuracy: 1.0\n",
      "epoch: 943 / loss: 0.3780952990055084 / accuracy: 1.0\n",
      "epoch: 944 / loss: 0.3769846558570862 / accuracy: 1.0\n",
      "epoch: 945 / loss: 0.3758794069290161 / accuracy: 1.0\n",
      "epoch: 946 / loss: 0.37477967143058777 / accuracy: 1.0\n",
      "epoch: 947 / loss: 0.3736852705478668 / accuracy: 1.0\n",
      "epoch: 948 / loss: 0.3725963830947876 / accuracy: 1.0\n",
      "epoch: 949 / loss: 0.3715130090713501 / accuracy: 1.0\n",
      "epoch: 950 / loss: 0.37043485045433044 / accuracy: 1.0\n",
      "epoch: 951 / loss: 0.3693618178367615 / accuracy: 1.0\n",
      "epoch: 952 / loss: 0.36829423904418945 / accuracy: 1.0\n",
      "epoch: 953 / loss: 0.3672318458557129 / accuracy: 1.0\n",
      "epoch: 954 / loss: 0.3661746084690094 / accuracy: 1.0\n",
      "epoch: 955 / loss: 0.3651227355003357 / accuracy: 1.0\n",
      "epoch: 956 / loss: 0.36407583951950073 / accuracy: 1.0\n",
      "epoch: 957 / loss: 0.36303406953811646 / accuracy: 1.0\n",
      "epoch: 958 / loss: 0.3619972765445709 / accuracy: 1.0\n",
      "epoch: 959 / loss: 0.36096566915512085 / accuracy: 1.0\n",
      "epoch: 960 / loss: 0.3599390983581543 / accuracy: 1.0\n",
      "epoch: 961 / loss: 0.3589174151420593 / accuracy: 1.0\n",
      "epoch: 962 / loss: 0.3579007387161255 / accuracy: 1.0\n",
      "epoch: 963 / loss: 0.3568888306617737 / accuracy: 1.0\n",
      "epoch: 964 / loss: 0.355881929397583 / accuracy: 1.0\n",
      "epoch: 965 / loss: 0.35487979650497437 / accuracy: 1.0\n",
      "epoch: 966 / loss: 0.35388243198394775 / accuracy: 1.0\n",
      "epoch: 967 / loss: 0.3528900742530823 / accuracy: 1.0\n",
      "epoch: 968 / loss: 0.3519023656845093 / accuracy: 1.0\n",
      "epoch: 969 / loss: 0.3509194552898407 / accuracy: 1.0\n",
      "epoch: 970 / loss: 0.3499411642551422 / accuracy: 1.0\n",
      "epoch: 971 / loss: 0.34896767139434814 / accuracy: 1.0\n",
      "epoch: 972 / loss: 0.34799858927726746 / accuracy: 1.0\n",
      "epoch: 973 / loss: 0.34703439474105835 / accuracy: 1.0\n",
      "epoch: 974 / loss: 0.34607452154159546 / accuracy: 1.0\n",
      "epoch: 975 / loss: 0.3451192378997803 / accuracy: 1.0\n",
      "epoch: 976 / loss: 0.34416869282722473 / accuracy: 1.0\n",
      "epoch: 977 / loss: 0.3432224988937378 / accuracy: 1.0\n",
      "epoch: 978 / loss: 0.34228086471557617 / accuracy: 1.0\n",
      "epoch: 979 / loss: 0.34134364128112793 / accuracy: 1.0\n",
      "epoch: 980 / loss: 0.3404107689857483 / accuracy: 1.0\n",
      "epoch: 981 / loss: 0.33948230743408203 / accuracy: 1.0\n",
      "epoch: 982 / loss: 0.33855828642845154 / accuracy: 1.0\n",
      "epoch: 983 / loss: 0.33763861656188965 / accuracy: 1.0\n",
      "epoch: 984 / loss: 0.3367232084274292 / accuracy: 1.0\n",
      "epoch: 985 / loss: 0.3358120918273926 / accuracy: 1.0\n",
      "epoch: 986 / loss: 0.3349051773548126 / accuracy: 1.0\n",
      "epoch: 987 / loss: 0.3340025544166565 / accuracy: 1.0\n",
      "epoch: 988 / loss: 0.33310413360595703 / accuracy: 1.0\n",
      "epoch: 989 / loss: 0.3322097063064575 / accuracy: 1.0\n",
      "epoch: 990 / loss: 0.331319659948349 / accuracy: 1.0\n",
      "epoch: 991 / loss: 0.33043378591537476 / accuracy: 1.0\n",
      "epoch: 992 / loss: 0.3295517563819885 / accuracy: 1.0\n",
      "epoch: 993 / loss: 0.3286741077899933 / accuracy: 1.0\n",
      "epoch: 994 / loss: 0.32780036330223083 / accuracy: 1.0\n",
      "epoch: 995 / loss: 0.32693061232566833 / accuracy: 1.0\n",
      "epoch: 996 / loss: 0.32606494426727295 / accuracy: 1.0\n",
      "epoch: 997 / loss: 0.3252032697200775 / accuracy: 1.0\n",
      "epoch: 998 / loss: 0.32434532046318054 / accuracy: 1.0\n",
      "epoch: 999 / loss: 0.323491632938385 / accuracy: 1.0\n",
      "epoch: 1000 / loss: 0.3226417005062103 / accuracy: 1.0\n",
      "epoch: 1001 / loss: 0.32179558277130127 / accuracy: 1.0\n",
      "epoch: 1002 / loss: 0.3209535479545593 / accuracy: 1.0\n",
      "epoch: 1003 / loss: 0.3201150894165039 / accuracy: 1.0\n",
      "epoch: 1004 / loss: 0.3192805051803589 / accuracy: 1.0\n",
      "epoch: 1005 / loss: 0.31844985485076904 / accuracy: 1.0\n",
      "epoch: 1006 / loss: 0.3176227807998657 / accuracy: 1.0\n",
      "epoch: 1007 / loss: 0.31679967045783997 / accuracy: 1.0\n",
      "epoch: 1008 / loss: 0.31598013639450073 / accuracy: 1.0\n",
      "epoch: 1009 / loss: 0.3151642084121704 / accuracy: 1.0\n",
      "epoch: 1010 / loss: 0.3143521547317505 / accuracy: 1.0\n",
      "epoch: 1011 / loss: 0.3135436177253723 / accuracy: 1.0\n",
      "epoch: 1012 / loss: 0.31273865699768066 / accuracy: 1.0\n",
      "epoch: 1013 / loss: 0.31193768978118896 / accuracy: 1.0\n",
      "epoch: 1014 / loss: 0.31113988161087036 / accuracy: 1.0\n",
      "epoch: 1015 / loss: 0.3103458881378174 / accuracy: 1.0\n",
      "epoch: 1016 / loss: 0.3095552921295166 / accuracy: 1.0\n",
      "epoch: 1017 / loss: 0.3087684214115143 / accuracy: 1.0\n",
      "epoch: 1018 / loss: 0.3079848885536194 / accuracy: 1.0\n",
      "epoch: 1019 / loss: 0.3072047233581543 / accuracy: 1.0\n",
      "epoch: 1020 / loss: 0.3064281642436981 / accuracy: 1.0\n",
      "epoch: 1021 / loss: 0.30565494298934937 / accuracy: 1.0\n",
      "epoch: 1022 / loss: 0.30488529801368713 / accuracy: 1.0\n",
      "epoch: 1023 / loss: 0.30411896109580994 / accuracy: 1.0\n",
      "epoch: 1024 / loss: 0.3033560812473297 / accuracy: 1.0\n",
      "epoch: 1025 / loss: 0.3025965690612793 / accuracy: 1.0\n",
      "epoch: 1026 / loss: 0.3018403649330139 / accuracy: 1.0\n",
      "epoch: 1027 / loss: 0.30108746886253357 / accuracy: 1.0\n",
      "epoch: 1028 / loss: 0.30033791065216064 / accuracy: 1.0\n",
      "epoch: 1029 / loss: 0.2995915412902832 / accuracy: 1.0\n",
      "epoch: 1030 / loss: 0.2988486886024475 / accuracy: 1.0\n",
      "epoch: 1031 / loss: 0.298108845949173 / accuracy: 1.0\n",
      "epoch: 1032 / loss: 0.2973722815513611 / accuracy: 1.0\n",
      "epoch: 1033 / loss: 0.29663893580436707 / accuracy: 1.0\n",
      "epoch: 1034 / loss: 0.2959086298942566 / accuracy: 1.0\n",
      "epoch: 1035 / loss: 0.2951817512512207 / accuracy: 1.0\n",
      "epoch: 1036 / loss: 0.29445788264274597 / accuracy: 1.0\n",
      "epoch: 1037 / loss: 0.2937372326850891 / accuracy: 1.0\n",
      "epoch: 1038 / loss: 0.29301953315734863 / accuracy: 1.0\n",
      "epoch: 1039 / loss: 0.2923048734664917 / accuracy: 1.0\n",
      "epoch: 1040 / loss: 0.29159361124038696 / accuracy: 1.0\n",
      "epoch: 1041 / loss: 0.29088521003723145 / accuracy: 1.0\n",
      "epoch: 1042 / loss: 0.2901797890663147 / accuracy: 1.0\n",
      "epoch: 1043 / loss: 0.2894776165485382 / accuracy: 1.0\n",
      "epoch: 1044 / loss: 0.288778156042099 / accuracy: 1.0\n",
      "epoch: 1045 / loss: 0.2880818247795105 / accuracy: 1.0\n",
      "epoch: 1046 / loss: 0.28738853335380554 / accuracy: 1.0\n",
      "epoch: 1047 / loss: 0.28669819235801697 / accuracy: 1.0\n",
      "epoch: 1048 / loss: 0.2860108017921448 / accuracy: 1.0\n",
      "epoch: 1049 / loss: 0.2853263020515442 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1050 / loss: 0.2846447229385376 / accuracy: 1.0\n",
      "epoch: 1051 / loss: 0.28396594524383545 / accuracy: 1.0\n",
      "epoch: 1052 / loss: 0.28328990936279297 / accuracy: 1.0\n",
      "epoch: 1053 / loss: 0.2826169729232788 / accuracy: 1.0\n",
      "epoch: 1054 / loss: 0.28194674849510193 / accuracy: 1.0\n",
      "epoch: 1055 / loss: 0.28127944469451904 / accuracy: 1.0\n",
      "epoch: 1056 / loss: 0.2806147634983063 / accuracy: 1.0\n",
      "epoch: 1057 / loss: 0.2799530625343323 / accuracy: 1.0\n",
      "epoch: 1058 / loss: 0.2792941629886627 / accuracy: 1.0\n",
      "epoch: 1059 / loss: 0.2786378562450409 / accuracy: 1.0\n",
      "epoch: 1060 / loss: 0.27798449993133545 / accuracy: 1.0\n",
      "epoch: 1061 / loss: 0.27733370661735535 / accuracy: 1.0\n",
      "epoch: 1062 / loss: 0.27668559551239014 / accuracy: 1.0\n",
      "epoch: 1063 / loss: 0.2760401964187622 / accuracy: 1.0\n",
      "epoch: 1064 / loss: 0.275397390127182 / accuracy: 1.0\n",
      "epoch: 1065 / loss: 0.2747575342655182 / accuracy: 1.0\n",
      "epoch: 1066 / loss: 0.2741200923919678 / accuracy: 1.0\n",
      "epoch: 1067 / loss: 0.27348557114601135 / accuracy: 1.0\n",
      "epoch: 1068 / loss: 0.2728533446788788 / accuracy: 1.0\n",
      "epoch: 1069 / loss: 0.27222374081611633 / accuracy: 1.0\n",
      "epoch: 1070 / loss: 0.27159690856933594 / accuracy: 1.0\n",
      "epoch: 1071 / loss: 0.2709724009037018 / accuracy: 1.0\n",
      "epoch: 1072 / loss: 0.27035075426101685 / accuracy: 1.0\n",
      "epoch: 1073 / loss: 0.2697315812110901 / accuracy: 1.0\n",
      "epoch: 1074 / loss: 0.26911473274230957 / accuracy: 1.0\n",
      "epoch: 1075 / loss: 0.2685006856918335 / accuracy: 1.0\n",
      "epoch: 1076 / loss: 0.26788875460624695 / accuracy: 1.0\n",
      "epoch: 1077 / loss: 0.26727962493896484 / accuracy: 1.0\n",
      "epoch: 1078 / loss: 0.26667308807373047 / accuracy: 1.0\n",
      "epoch: 1079 / loss: 0.266068696975708 / accuracy: 1.0\n",
      "epoch: 1080 / loss: 0.2654668390750885 / accuracy: 1.0\n",
      "epoch: 1081 / loss: 0.26486751437187195 / accuracy: 1.0\n",
      "epoch: 1082 / loss: 0.2642705738544464 / accuracy: 1.0\n",
      "epoch: 1083 / loss: 0.2636760473251343 / accuracy: 1.0\n",
      "epoch: 1084 / loss: 0.2630839943885803 / accuracy: 1.0\n",
      "epoch: 1085 / loss: 0.26249417662620544 / accuracy: 1.0\n",
      "epoch: 1086 / loss: 0.26190680265426636 / accuracy: 1.0\n",
      "epoch: 1087 / loss: 0.2613215446472168 / accuracy: 1.0\n",
      "epoch: 1088 / loss: 0.26073890924453735 / accuracy: 1.0\n",
      "epoch: 1089 / loss: 0.2601586580276489 / accuracy: 1.0\n",
      "epoch: 1090 / loss: 0.2595805525779724 / accuracy: 1.0\n",
      "epoch: 1091 / loss: 0.25900477170944214 / accuracy: 1.0\n",
      "epoch: 1092 / loss: 0.2584313154220581 / accuracy: 1.0\n",
      "epoch: 1093 / loss: 0.2578601837158203 / accuracy: 1.0\n",
      "epoch: 1094 / loss: 0.25729116797447205 / accuracy: 1.0\n",
      "epoch: 1095 / loss: 0.25672465562820435 / accuracy: 1.0\n",
      "epoch: 1096 / loss: 0.25616011023521423 / accuracy: 1.0\n",
      "epoch: 1097 / loss: 0.25559794902801514 / accuracy: 1.0\n",
      "epoch: 1098 / loss: 0.25503799319267273 / accuracy: 1.0\n",
      "epoch: 1099 / loss: 0.25448018312454224 / accuracy: 1.0\n",
      "epoch: 1100 / loss: 0.25392454862594604 / accuracy: 1.0\n",
      "epoch: 1101 / loss: 0.2533712089061737 / accuracy: 1.0\n",
      "epoch: 1102 / loss: 0.25282013416290283 / accuracy: 1.0\n",
      "epoch: 1103 / loss: 0.2522708475589752 / accuracy: 1.0\n",
      "epoch: 1104 / loss: 0.25172409415245056 / accuracy: 1.0\n",
      "epoch: 1105 / loss: 0.25117921829223633 / accuracy: 1.0\n",
      "epoch: 1106 / loss: 0.2506365478038788 / accuracy: 1.0\n",
      "epoch: 1107 / loss: 0.2500959634780884 / accuracy: 1.0\n",
      "epoch: 1108 / loss: 0.24955765902996063 / accuracy: 1.0\n",
      "epoch: 1109 / loss: 0.24902136623859406 / accuracy: 1.0\n",
      "epoch: 1110 / loss: 0.24848699569702148 / accuracy: 1.0\n",
      "epoch: 1111 / loss: 0.2479548305273056 / accuracy: 1.0\n",
      "epoch: 1112 / loss: 0.2474246770143509 / accuracy: 1.0\n",
      "epoch: 1113 / loss: 0.24689653515815735 / accuracy: 1.0\n",
      "epoch: 1114 / loss: 0.246370330452919 / accuracy: 1.0\n",
      "epoch: 1115 / loss: 0.2458464652299881 / accuracy: 1.0\n",
      "epoch: 1116 / loss: 0.24532431364059448 / accuracy: 1.0\n",
      "epoch: 1117 / loss: 0.24480438232421875 / accuracy: 1.0\n",
      "epoch: 1118 / loss: 0.24428626894950867 / accuracy: 1.0\n",
      "epoch: 1119 / loss: 0.24377021193504333 / accuracy: 1.0\n",
      "epoch: 1120 / loss: 0.24325594305992126 / accuracy: 1.0\n",
      "epoch: 1121 / loss: 0.24274395406246185 / accuracy: 1.0\n",
      "epoch: 1122 / loss: 0.24223369359970093 / accuracy: 1.0\n",
      "epoch: 1123 / loss: 0.2417253851890564 / accuracy: 1.0\n",
      "epoch: 1124 / loss: 0.24121923744678497 / accuracy: 1.0\n",
      "epoch: 1125 / loss: 0.24071475863456726 / accuracy: 1.0\n",
      "epoch: 1126 / loss: 0.2402123510837555 / accuracy: 1.0\n",
      "epoch: 1127 / loss: 0.2397117018699646 / accuracy: 1.0\n",
      "epoch: 1128 / loss: 0.23921296000480652 / accuracy: 1.0\n",
      "epoch: 1129 / loss: 0.23871609568595886 / accuracy: 1.0\n",
      "epoch: 1130 / loss: 0.23822121322155 / accuracy: 1.0\n",
      "epoch: 1131 / loss: 0.23772811889648438 / accuracy: 1.0\n",
      "epoch: 1132 / loss: 0.2372368425130844 / accuracy: 1.0\n",
      "epoch: 1133 / loss: 0.2367473989725113 / accuracy: 1.0\n",
      "epoch: 1134 / loss: 0.23625989258289337 / accuracy: 1.0\n",
      "epoch: 1135 / loss: 0.2357741892337799 / accuracy: 1.0\n",
      "epoch: 1136 / loss: 0.2352900207042694 / accuracy: 1.0\n",
      "epoch: 1137 / loss: 0.23480790853500366 / accuracy: 1.0\n",
      "epoch: 1138 / loss: 0.23432768881320953 / accuracy: 1.0\n",
      "epoch: 1139 / loss: 0.2338489592075348 / accuracy: 1.0\n",
      "epoch: 1140 / loss: 0.23337221145629883 / accuracy: 1.0\n",
      "epoch: 1141 / loss: 0.23289726674556732 / accuracy: 1.0\n",
      "epoch: 1142 / loss: 0.23242396116256714 / accuracy: 1.0\n",
      "epoch: 1143 / loss: 0.23195239901542664 / accuracy: 1.0\n",
      "epoch: 1144 / loss: 0.2314826101064682 / accuracy: 1.0\n",
      "epoch: 1145 / loss: 0.23101460933685303 / accuracy: 1.0\n",
      "epoch: 1146 / loss: 0.2305484116077423 / accuracy: 1.0\n",
      "epoch: 1147 / loss: 0.23008359968662262 / accuracy: 1.0\n",
      "epoch: 1148 / loss: 0.22962069511413574 / accuracy: 1.0\n",
      "epoch: 1149 / loss: 0.22915945947170258 / accuracy: 1.0\n",
      "epoch: 1150 / loss: 0.22869987785816193 / accuracy: 1.0\n",
      "epoch: 1151 / loss: 0.22824208438396454 / accuracy: 1.0\n",
      "epoch: 1152 / loss: 0.22778579592704773 / accuracy: 1.0\n",
      "epoch: 1153 / loss: 0.2273315042257309 / accuracy: 1.0\n",
      "epoch: 1154 / loss: 0.2268783450126648 / accuracy: 1.0\n",
      "epoch: 1155 / loss: 0.2264271080493927 / accuracy: 1.0\n",
      "epoch: 1156 / loss: 0.2259775698184967 / accuracy: 1.0\n",
      "epoch: 1157 / loss: 0.22552961111068726 / accuracy: 1.0\n",
      "epoch: 1158 / loss: 0.22508318722248077 / accuracy: 1.0\n",
      "epoch: 1159 / loss: 0.22463840246200562 / accuracy: 1.0\n",
      "epoch: 1160 / loss: 0.2241952270269394 / accuracy: 1.0\n",
      "epoch: 1161 / loss: 0.22375355660915375 / accuracy: 1.0\n",
      "epoch: 1162 / loss: 0.22331371903419495 / accuracy: 1.0\n",
      "epoch: 1163 / loss: 0.22287523746490479 / accuracy: 1.0\n",
      "epoch: 1164 / loss: 0.2224384993314743 / accuracy: 1.0\n",
      "epoch: 1165 / loss: 0.22200316190719604 / accuracy: 1.0\n",
      "epoch: 1166 / loss: 0.2215694785118103 / accuracy: 1.0\n",
      "epoch: 1167 / loss: 0.2211371660232544 / accuracy: 1.0\n",
      "epoch: 1168 / loss: 0.22070671617984772 / accuracy: 1.0\n",
      "epoch: 1169 / loss: 0.2202775776386261 / accuracy: 1.0\n",
      "epoch: 1170 / loss: 0.21984976530075073 / accuracy: 1.0\n",
      "epoch: 1171 / loss: 0.2194237858057022 / accuracy: 1.0\n",
      "epoch: 1172 / loss: 0.21899937093257904 / accuracy: 1.0\n",
      "epoch: 1173 / loss: 0.2185763716697693 / accuracy: 1.0\n",
      "epoch: 1174 / loss: 0.21815472841262817 / accuracy: 1.0\n",
      "epoch: 1175 / loss: 0.21773454546928406 / accuracy: 1.0\n",
      "epoch: 1176 / loss: 0.21731600165367126 / accuracy: 1.0\n",
      "epoch: 1177 / loss: 0.2168988287448883 / accuracy: 1.0\n",
      "epoch: 1178 / loss: 0.2164832055568695 / accuracy: 1.0\n",
      "epoch: 1179 / loss: 0.21606887876987457 / accuracy: 1.0\n",
      "epoch: 1180 / loss: 0.21565625071525574 / accuracy: 1.0\n",
      "epoch: 1181 / loss: 0.21524487435817719 / accuracy: 1.0\n",
      "epoch: 1182 / loss: 0.21483513712882996 / accuracy: 1.0\n",
      "epoch: 1183 / loss: 0.21442660689353943 / accuracy: 1.0\n",
      "epoch: 1184 / loss: 0.2140195518732071 / accuracy: 1.0\n",
      "epoch: 1185 / loss: 0.21361389756202698 / accuracy: 1.0\n",
      "epoch: 1186 / loss: 0.21320980787277222 / accuracy: 1.0\n",
      "epoch: 1187 / loss: 0.21280691027641296 / accuracy: 1.0\n",
      "epoch: 1188 / loss: 0.21240553259849548 / accuracy: 1.0\n",
      "epoch: 1189 / loss: 0.21200546622276306 / accuracy: 1.0\n",
      "epoch: 1190 / loss: 0.21160686016082764 / accuracy: 1.0\n",
      "epoch: 1191 / loss: 0.2112094908952713 / accuracy: 1.0\n",
      "epoch: 1192 / loss: 0.21081379055976868 / accuracy: 1.0\n",
      "epoch: 1193 / loss: 0.21041910350322723 / accuracy: 1.0\n",
      "epoch: 1194 / loss: 0.21002604067325592 / accuracy: 1.0\n",
      "epoch: 1195 / loss: 0.20963427424430847 / accuracy: 1.0\n",
      "epoch: 1196 / loss: 0.20924371480941772 / accuracy: 1.0\n",
      "epoch: 1197 / loss: 0.20885445177555084 / accuracy: 1.0\n",
      "epoch: 1198 / loss: 0.20846664905548096 / accuracy: 1.0\n",
      "epoch: 1199 / loss: 0.20808003842830658 / accuracy: 1.0\n",
      "epoch: 1200 / loss: 0.20769499242305756 / accuracy: 1.0\n",
      "epoch: 1201 / loss: 0.20731103420257568 / accuracy: 1.0\n",
      "epoch: 1202 / loss: 0.20692825317382812 / accuracy: 1.0\n",
      "epoch: 1203 / loss: 0.20654723048210144 / accuracy: 1.0\n",
      "epoch: 1204 / loss: 0.206167072057724 / accuracy: 1.0\n",
      "epoch: 1205 / loss: 0.20578840374946594 / accuracy: 1.0\n",
      "epoch: 1206 / loss: 0.20541100203990936 / accuracy: 1.0\n",
      "epoch: 1207 / loss: 0.20503488183021545 / accuracy: 1.0\n",
      "epoch: 1208 / loss: 0.20465990900993347 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1209 / loss: 0.20428620278835297 / accuracy: 1.0\n",
      "epoch: 1210 / loss: 0.20391389727592468 / accuracy: 1.0\n",
      "epoch: 1211 / loss: 0.20354269444942474 / accuracy: 1.0\n",
      "epoch: 1212 / loss: 0.20317280292510986 / accuracy: 1.0\n",
      "epoch: 1213 / loss: 0.20280414819717407 / accuracy: 1.0\n",
      "epoch: 1214 / loss: 0.20243671536445618 / accuracy: 1.0\n",
      "epoch: 1215 / loss: 0.20207053422927856 / accuracy: 1.0\n",
      "epoch: 1216 / loss: 0.20170553028583527 / accuracy: 1.0\n",
      "epoch: 1217 / loss: 0.20134176313877106 / accuracy: 1.0\n",
      "epoch: 1218 / loss: 0.20097924768924713 / accuracy: 1.0\n",
      "epoch: 1219 / loss: 0.20061790943145752 / accuracy: 1.0\n",
      "epoch: 1220 / loss: 0.20025759935379028 / accuracy: 1.0\n",
      "epoch: 1221 / loss: 0.19989866018295288 / accuracy: 1.0\n",
      "epoch: 1222 / loss: 0.19954076409339905 / accuracy: 1.0\n",
      "epoch: 1223 / loss: 0.19918423891067505 / accuracy: 1.0\n",
      "epoch: 1224 / loss: 0.19882889091968536 / accuracy: 1.0\n",
      "epoch: 1225 / loss: 0.1984744518995285 / accuracy: 1.0\n",
      "epoch: 1226 / loss: 0.19812151789665222 / accuracy: 1.0\n",
      "epoch: 1227 / loss: 0.19776935875415802 / accuracy: 1.0\n",
      "epoch: 1228 / loss: 0.19741863012313843 / accuracy: 1.0\n",
      "epoch: 1229 / loss: 0.1970689594745636 / accuracy: 1.0\n",
      "epoch: 1230 / loss: 0.19672051072120667 / accuracy: 1.0\n",
      "epoch: 1231 / loss: 0.19637317955493927 / accuracy: 1.0\n",
      "epoch: 1232 / loss: 0.19602693617343903 / accuracy: 1.0\n",
      "epoch: 1233 / loss: 0.19568181037902832 / accuracy: 1.0\n",
      "epoch: 1234 / loss: 0.19533780217170715 / accuracy: 1.0\n",
      "epoch: 1235 / loss: 0.19499488174915314 / accuracy: 1.0\n",
      "epoch: 1236 / loss: 0.19465327262878418 / accuracy: 1.0\n",
      "epoch: 1237 / loss: 0.19431255757808685 / accuracy: 1.0\n",
      "epoch: 1238 / loss: 0.19397297501564026 / accuracy: 1.0\n",
      "epoch: 1239 / loss: 0.1936345398426056 / accuracy: 1.0\n",
      "epoch: 1240 / loss: 0.19329720735549927 / accuracy: 1.0\n",
      "epoch: 1241 / loss: 0.19296081364154816 / accuracy: 1.0\n",
      "epoch: 1242 / loss: 0.19262564182281494 / accuracy: 1.0\n",
      "epoch: 1243 / loss: 0.19229155778884888 / accuracy: 1.0\n",
      "epoch: 1244 / loss: 0.19195839762687683 / accuracy: 1.0\n",
      "epoch: 1245 / loss: 0.19162653386592865 / accuracy: 1.0\n",
      "epoch: 1246 / loss: 0.19129550457000732 / accuracy: 1.0\n",
      "epoch: 1247 / loss: 0.19096559286117554 / accuracy: 1.0\n",
      "epoch: 1248 / loss: 0.19063688814640045 / accuracy: 1.0\n",
      "epoch: 1249 / loss: 0.190309077501297 / accuracy: 1.0\n",
      "epoch: 1250 / loss: 0.1899823248386383 / accuracy: 1.0\n",
      "epoch: 1251 / loss: 0.1896565556526184 / accuracy: 1.0\n",
      "epoch: 1252 / loss: 0.18933196365833282 / accuracy: 1.0\n",
      "epoch: 1253 / loss: 0.18900834023952484 / accuracy: 1.0\n",
      "epoch: 1254 / loss: 0.18868562579154968 / accuracy: 1.0\n",
      "epoch: 1255 / loss: 0.18836408853530884 / accuracy: 1.0\n",
      "epoch: 1256 / loss: 0.18804356455802917 / accuracy: 1.0\n",
      "epoch: 1257 / loss: 0.18772384524345398 / accuracy: 1.0\n",
      "epoch: 1258 / loss: 0.18740539252758026 / accuracy: 1.0\n",
      "epoch: 1259 / loss: 0.187087744474411 / accuracy: 1.0\n",
      "epoch: 1260 / loss: 0.18677112460136414 / accuracy: 1.0\n",
      "epoch: 1261 / loss: 0.1864555925130844 / accuracy: 1.0\n",
      "epoch: 1262 / loss: 0.1861410290002823 / accuracy: 1.0\n",
      "epoch: 1263 / loss: 0.18582740426063538 / accuracy: 1.0\n",
      "epoch: 1264 / loss: 0.18551477789878845 / accuracy: 1.0\n",
      "epoch: 1265 / loss: 0.1852032095193863 / accuracy: 1.0\n",
      "epoch: 1266 / loss: 0.18489262461662292 / accuracy: 1.0\n",
      "epoch: 1267 / loss: 0.18458279967308044 / accuracy: 1.0\n",
      "epoch: 1268 / loss: 0.18427394330501556 / accuracy: 1.0\n",
      "epoch: 1269 / loss: 0.1839662343263626 / accuracy: 1.0\n",
      "epoch: 1270 / loss: 0.1836593896150589 / accuracy: 1.0\n",
      "epoch: 1271 / loss: 0.18335357308387756 / accuracy: 1.0\n",
      "epoch: 1272 / loss: 0.18304860591888428 / accuracy: 1.0\n",
      "epoch: 1273 / loss: 0.18274447321891785 / accuracy: 1.0\n",
      "epoch: 1274 / loss: 0.1824413239955902 / accuracy: 1.0\n",
      "epoch: 1275 / loss: 0.18213924765586853 / accuracy: 1.0\n",
      "epoch: 1276 / loss: 0.1818380355834961 / accuracy: 1.0\n",
      "epoch: 1277 / loss: 0.18153777718544006 / accuracy: 1.0\n",
      "epoch: 1278 / loss: 0.18123847246170044 / accuracy: 1.0\n",
      "epoch: 1279 / loss: 0.18093997240066528 / accuracy: 1.0\n",
      "epoch: 1280 / loss: 0.18064242601394653 / accuracy: 1.0\n",
      "epoch: 1281 / loss: 0.18034574389457703 / accuracy: 1.0\n",
      "epoch: 1282 / loss: 0.18005013465881348 / accuracy: 1.0\n",
      "epoch: 1283 / loss: 0.1797552853822708 / accuracy: 1.0\n",
      "epoch: 1284 / loss: 0.17946135997772217 / accuracy: 1.0\n",
      "epoch: 1285 / loss: 0.1791682243347168 / accuracy: 1.0\n",
      "epoch: 1286 / loss: 0.1788761019706726 / accuracy: 1.0\n",
      "epoch: 1287 / loss: 0.17858487367630005 / accuracy: 1.0\n",
      "epoch: 1288 / loss: 0.17829445004463196 / accuracy: 1.0\n",
      "epoch: 1289 / loss: 0.1780049055814743 / accuracy: 1.0\n",
      "epoch: 1290 / loss: 0.1777164489030838 / accuracy: 1.0\n",
      "epoch: 1291 / loss: 0.1774284839630127 / accuracy: 1.0\n",
      "epoch: 1292 / loss: 0.17714157700538635 / accuracy: 1.0\n",
      "epoch: 1293 / loss: 0.17685557901859283 / accuracy: 1.0\n",
      "epoch: 1294 / loss: 0.1765703707933426 / accuracy: 1.0\n",
      "epoch: 1295 / loss: 0.17628610134124756 / accuracy: 1.0\n",
      "epoch: 1296 / loss: 0.17600250244140625 / accuracy: 1.0\n",
      "epoch: 1297 / loss: 0.17571991682052612 / accuracy: 1.0\n",
      "epoch: 1298 / loss: 0.17543825507164001 / accuracy: 1.0\n",
      "epoch: 1299 / loss: 0.1751573383808136 / accuracy: 1.0\n",
      "epoch: 1300 / loss: 0.17487721145153046 / accuracy: 1.0\n",
      "epoch: 1301 / loss: 0.17459779977798462 / accuracy: 1.0\n",
      "epoch: 1302 / loss: 0.17431946098804474 / accuracy: 1.0\n",
      "epoch: 1303 / loss: 0.17404180765151978 / accuracy: 1.0\n",
      "epoch: 1304 / loss: 0.17376500368118286 / accuracy: 1.0\n",
      "epoch: 1305 / loss: 0.17348918318748474 / accuracy: 1.0\n",
      "epoch: 1306 / loss: 0.17321382462978363 / accuracy: 1.0\n",
      "epoch: 1307 / loss: 0.17293968796730042 / accuracy: 1.0\n",
      "epoch: 1308 / loss: 0.17266619205474854 / accuracy: 1.0\n",
      "epoch: 1309 / loss: 0.17239326238632202 / accuracy: 1.0\n",
      "epoch: 1310 / loss: 0.1721213161945343 / accuracy: 1.0\n",
      "epoch: 1311 / loss: 0.17185018956661224 / accuracy: 1.0\n",
      "epoch: 1312 / loss: 0.17157989740371704 / accuracy: 1.0\n",
      "epoch: 1313 / loss: 0.17131057381629944 / accuracy: 1.0\n",
      "epoch: 1314 / loss: 0.17104166746139526 / accuracy: 1.0\n",
      "epoch: 1315 / loss: 0.17077374458312988 / accuracy: 1.0\n",
      "epoch: 1316 / loss: 0.17050659656524658 / accuracy: 1.0\n",
      "epoch: 1317 / loss: 0.17024025321006775 / accuracy: 1.0\n",
      "epoch: 1318 / loss: 0.16997459530830383 / accuracy: 1.0\n",
      "epoch: 1319 / loss: 0.16970957815647125 / accuracy: 1.0\n",
      "epoch: 1320 / loss: 0.16944560408592224 / accuracy: 1.0\n",
      "epoch: 1321 / loss: 0.1691821813583374 / accuracy: 1.0\n",
      "epoch: 1322 / loss: 0.16891974210739136 / accuracy: 1.0\n",
      "epoch: 1323 / loss: 0.16865774989128113 / accuracy: 1.0\n",
      "epoch: 1324 / loss: 0.16839677095413208 / accuracy: 1.0\n",
      "epoch: 1325 / loss: 0.16813652217388153 / accuracy: 1.0\n",
      "epoch: 1326 / loss: 0.16787701845169067 / accuracy: 1.0\n",
      "epoch: 1327 / loss: 0.16761824488639832 / accuracy: 1.0\n",
      "epoch: 1328 / loss: 0.16736021637916565 / accuracy: 1.0\n",
      "epoch: 1329 / loss: 0.16710302233695984 / accuracy: 1.0\n",
      "epoch: 1330 / loss: 0.166846364736557 / accuracy: 1.0\n",
      "epoch: 1331 / loss: 0.1665906012058258 / accuracy: 1.0\n",
      "epoch: 1332 / loss: 0.16633552312850952 / accuracy: 1.0\n",
      "epoch: 1333 / loss: 0.166081041097641 / accuracy: 1.0\n",
      "epoch: 1334 / loss: 0.16582739353179932 / accuracy: 1.0\n",
      "epoch: 1335 / loss: 0.1655745655298233 / accuracy: 1.0\n",
      "epoch: 1336 / loss: 0.1653221994638443 / accuracy: 1.0\n",
      "epoch: 1337 / loss: 0.1650708019733429 / accuracy: 1.0\n",
      "epoch: 1338 / loss: 0.16482001543045044 / accuracy: 1.0\n",
      "epoch: 1339 / loss: 0.16457001864910126 / accuracy: 1.0\n",
      "epoch: 1340 / loss: 0.16432058811187744 / accuracy: 1.0\n",
      "epoch: 1341 / loss: 0.16407200694084167 / accuracy: 1.0\n",
      "epoch: 1342 / loss: 0.16382399201393127 / accuracy: 1.0\n",
      "epoch: 1343 / loss: 0.16357681155204773 / accuracy: 1.0\n",
      "epoch: 1344 / loss: 0.16333037614822388 / accuracy: 1.0\n",
      "epoch: 1345 / loss: 0.1630844920873642 / accuracy: 1.0\n",
      "epoch: 1346 / loss: 0.16283929347991943 / accuracy: 1.0\n",
      "epoch: 1347 / loss: 0.1625949740409851 / accuracy: 1.0\n",
      "epoch: 1348 / loss: 0.1623510718345642 / accuracy: 1.0\n",
      "epoch: 1349 / loss: 0.1621079295873642 / accuracy: 1.0\n",
      "epoch: 1350 / loss: 0.16186554729938507 / accuracy: 1.0\n",
      "epoch: 1351 / loss: 0.16162380576133728 / accuracy: 1.0\n",
      "epoch: 1352 / loss: 0.1613827645778656 / accuracy: 1.0\n",
      "epoch: 1353 / loss: 0.16114236414432526 / accuracy: 1.0\n",
      "epoch: 1354 / loss: 0.160902738571167 / accuracy: 1.0\n",
      "epoch: 1355 / loss: 0.1606636792421341 / accuracy: 1.0\n",
      "epoch: 1356 / loss: 0.16042527556419373 / accuracy: 1.0\n",
      "epoch: 1357 / loss: 0.16018755733966827 / accuracy: 1.0\n",
      "epoch: 1358 / loss: 0.15995053946971893 / accuracy: 1.0\n",
      "epoch: 1359 / loss: 0.15971392393112183 / accuracy: 1.0\n",
      "epoch: 1360 / loss: 0.15947812795639038 / accuracy: 1.0\n",
      "epoch: 1361 / loss: 0.15924328565597534 / accuracy: 1.0\n",
      "epoch: 1362 / loss: 0.1590087115764618 / accuracy: 1.0\n",
      "epoch: 1363 / loss: 0.15877491235733032 / accuracy: 1.0\n",
      "epoch: 1364 / loss: 0.1585417240858078 / accuracy: 1.0\n",
      "epoch: 1365 / loss: 0.15830932557582855 / accuracy: 1.0\n",
      "epoch: 1366 / loss: 0.15807723999023438 / accuracy: 1.0\n",
      "epoch: 1367 / loss: 0.15784616768360138 / accuracy: 1.0\n",
      "epoch: 1368 / loss: 0.15761548280715942 / accuracy: 1.0\n",
      "epoch: 1369 / loss: 0.15738557279109955 / accuracy: 1.0\n",
      "epoch: 1370 / loss: 0.15715622901916504 / accuracy: 1.0\n",
      "epoch: 1371 / loss: 0.15692739188671112 / accuracy: 1.0\n",
      "epoch: 1372 / loss: 0.15669915080070496 / accuracy: 1.0\n",
      "epoch: 1373 / loss: 0.15647178888320923 / accuracy: 1.0\n",
      "epoch: 1374 / loss: 0.15624479949474335 / accuracy: 1.0\n",
      "epoch: 1375 / loss: 0.15601864457130432 / accuracy: 1.0\n",
      "epoch: 1376 / loss: 0.15579307079315186 / accuracy: 1.0\n",
      "epoch: 1377 / loss: 0.15556801855564117 / accuracy: 1.0\n",
      "epoch: 1378 / loss: 0.1553436517715454 / accuracy: 1.0\n",
      "epoch: 1379 / loss: 0.1551198661327362 / accuracy: 1.0\n",
      "epoch: 1380 / loss: 0.15489664673805237 / accuracy: 1.0\n",
      "epoch: 1381 / loss: 0.15467393398284912 / accuracy: 1.0\n",
      "epoch: 1382 / loss: 0.15445205569267273 / accuracy: 1.0\n",
      "epoch: 1383 / loss: 0.15423062443733215 / accuracy: 1.0\n",
      "epoch: 1384 / loss: 0.15400996804237366 / accuracy: 1.0\n",
      "epoch: 1385 / loss: 0.15378955006599426 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1386 / loss: 0.1535700261592865 / accuracy: 1.0\n",
      "epoch: 1387 / loss: 0.1533510833978653 / accuracy: 1.0\n",
      "epoch: 1388 / loss: 0.15313252806663513 / accuracy: 1.0\n",
      "epoch: 1389 / loss: 0.15291467308998108 / accuracy: 1.0\n",
      "epoch: 1390 / loss: 0.1526973843574524 / accuracy: 1.0\n",
      "epoch: 1391 / loss: 0.15248066186904907 / accuracy: 1.0\n",
      "epoch: 1392 / loss: 0.15226447582244873 / accuracy: 1.0\n",
      "epoch: 1393 / loss: 0.1520489603281021 / accuracy: 1.0\n",
      "epoch: 1394 / loss: 0.15183402597904205 / accuracy: 1.0\n",
      "epoch: 1395 / loss: 0.15161967277526855 / accuracy: 1.0\n",
      "epoch: 1396 / loss: 0.15140581130981445 / accuracy: 1.0\n",
      "epoch: 1397 / loss: 0.15119260549545288 / accuracy: 1.0\n",
      "epoch: 1398 / loss: 0.15097972750663757 / accuracy: 1.0\n",
      "epoch: 1399 / loss: 0.15076777338981628 / accuracy: 1.0\n",
      "epoch: 1400 / loss: 0.15055616199970245 / accuracy: 1.0\n",
      "epoch: 1401 / loss: 0.150345116853714 / accuracy: 1.0\n",
      "epoch: 1402 / loss: 0.1501346379518509 / accuracy: 1.0\n",
      "epoch: 1403 / loss: 0.14992479979991913 / accuracy: 1.0\n",
      "epoch: 1404 / loss: 0.14971527457237244 / accuracy: 1.0\n",
      "epoch: 1405 / loss: 0.14950650930404663 / accuracy: 1.0\n",
      "epoch: 1406 / loss: 0.1492982804775238 / accuracy: 1.0\n",
      "epoch: 1407 / loss: 0.14909040927886963 / accuracy: 1.0\n",
      "epoch: 1408 / loss: 0.14888343214988708 / accuracy: 1.0\n",
      "epoch: 1409 / loss: 0.14867666363716125 / accuracy: 1.0\n",
      "epoch: 1410 / loss: 0.14847062528133392 / accuracy: 1.0\n",
      "epoch: 1411 / loss: 0.1482650488615036 / accuracy: 1.0\n",
      "epoch: 1412 / loss: 0.1480599194765091 / accuracy: 1.0\n",
      "epoch: 1413 / loss: 0.1478554904460907 / accuracy: 1.0\n",
      "epoch: 1414 / loss: 0.1476515531539917 / accuracy: 1.0\n",
      "epoch: 1415 / loss: 0.14744813740253448 / accuracy: 1.0\n",
      "epoch: 1416 / loss: 0.14724516868591309 / accuracy: 1.0\n",
      "epoch: 1417 / loss: 0.1470429003238678 / accuracy: 1.0\n",
      "epoch: 1418 / loss: 0.1468408852815628 / accuracy: 1.0\n",
      "epoch: 1419 / loss: 0.14663957059383392 / accuracy: 1.0\n",
      "epoch: 1420 / loss: 0.1464388072490692 / accuracy: 1.0\n",
      "epoch: 1421 / loss: 0.1462385058403015 / accuracy: 1.0\n",
      "epoch: 1422 / loss: 0.1460387110710144 / accuracy: 1.0\n",
      "epoch: 1423 / loss: 0.14583943784236908 / accuracy: 1.0\n",
      "epoch: 1424 / loss: 0.14564090967178345 / accuracy: 1.0\n",
      "epoch: 1425 / loss: 0.14544251561164856 / accuracy: 1.0\n",
      "epoch: 1426 / loss: 0.1452447474002838 / accuracy: 1.0\n",
      "epoch: 1427 / loss: 0.14504742622375488 / accuracy: 1.0\n",
      "epoch: 1428 / loss: 0.14485080540180206 / accuracy: 1.0\n",
      "epoch: 1429 / loss: 0.14465445280075073 / accuracy: 1.0\n",
      "epoch: 1430 / loss: 0.14445871114730835 / accuracy: 1.0\n",
      "epoch: 1431 / loss: 0.14426355063915253 / accuracy: 1.0\n",
      "epoch: 1432 / loss: 0.14406883716583252 / accuracy: 1.0\n",
      "epoch: 1433 / loss: 0.1438746154308319 / accuracy: 1.0\n",
      "epoch: 1434 / loss: 0.1436808705329895 / accuracy: 1.0\n",
      "epoch: 1435 / loss: 0.1434876024723053 / accuracy: 1.0\n",
      "epoch: 1436 / loss: 0.1432947963476181 / accuracy: 1.0\n",
      "epoch: 1437 / loss: 0.14310254156589508 / accuracy: 1.0\n",
      "epoch: 1438 / loss: 0.14291080832481384 / accuracy: 1.0\n",
      "epoch: 1439 / loss: 0.14271944761276245 / accuracy: 1.0\n",
      "epoch: 1440 / loss: 0.1425286829471588 / accuracy: 1.0\n",
      "epoch: 1441 / loss: 0.14233826100826263 / accuracy: 1.0\n",
      "epoch: 1442 / loss: 0.14214837551116943 / accuracy: 1.0\n",
      "epoch: 1443 / loss: 0.1419590562582016 / accuracy: 1.0\n",
      "epoch: 1444 / loss: 0.14177009463310242 / accuracy: 1.0\n",
      "epoch: 1445 / loss: 0.141581729054451 / accuracy: 1.0\n",
      "epoch: 1446 / loss: 0.14139360189437866 / accuracy: 1.0\n",
      "epoch: 1447 / loss: 0.141206294298172 / accuracy: 1.0\n",
      "epoch: 1448 / loss: 0.14101925492286682 / accuracy: 1.0\n",
      "epoch: 1449 / loss: 0.1408325433731079 / accuracy: 1.0\n",
      "epoch: 1450 / loss: 0.14064650237560272 / accuracy: 1.0\n",
      "epoch: 1451 / loss: 0.1404610425233841 / accuracy: 1.0\n",
      "epoch: 1452 / loss: 0.14027583599090576 / accuracy: 1.0\n",
      "epoch: 1453 / loss: 0.14009100198745728 / accuracy: 1.0\n",
      "epoch: 1454 / loss: 0.1399068832397461 / accuracy: 1.0\n",
      "epoch: 1455 / loss: 0.139723002910614 / accuracy: 1.0\n",
      "epoch: 1456 / loss: 0.13953974843025208 / accuracy: 1.0\n",
      "epoch: 1457 / loss: 0.1393568217754364 / accuracy: 1.0\n",
      "epoch: 1458 / loss: 0.13917434215545654 / accuracy: 1.0\n",
      "epoch: 1459 / loss: 0.13899242877960205 / accuracy: 1.0\n",
      "epoch: 1460 / loss: 0.13881082832813263 / accuracy: 1.0\n",
      "epoch: 1461 / loss: 0.13862985372543335 / accuracy: 1.0\n",
      "epoch: 1462 / loss: 0.13844895362854004 / accuracy: 1.0\n",
      "epoch: 1463 / loss: 0.1382688730955124 / accuracy: 1.0\n",
      "epoch: 1464 / loss: 0.13808917999267578 / accuracy: 1.0\n",
      "epoch: 1465 / loss: 0.13790979981422424 / accuracy: 1.0\n",
      "epoch: 1466 / loss: 0.13773085176944733 / accuracy: 1.0\n",
      "epoch: 1467 / loss: 0.13755260407924652 / accuracy: 1.0\n",
      "epoch: 1468 / loss: 0.1373743712902069 / accuracy: 1.0\n",
      "epoch: 1469 / loss: 0.1371968686580658 / accuracy: 1.0\n",
      "epoch: 1470 / loss: 0.13701976835727692 / accuracy: 1.0\n",
      "epoch: 1471 / loss: 0.1368432343006134 / accuracy: 1.0\n",
      "epoch: 1472 / loss: 0.13666681945323944 / accuracy: 1.0\n",
      "epoch: 1473 / loss: 0.1364911049604416 / accuracy: 1.0\n",
      "epoch: 1474 / loss: 0.13631564378738403 / accuracy: 1.0\n",
      "epoch: 1475 / loss: 0.1361406147480011 / accuracy: 1.0\n",
      "epoch: 1476 / loss: 0.13596603274345398 / accuracy: 1.0\n",
      "epoch: 1477 / loss: 0.1357918381690979 / accuracy: 1.0\n",
      "epoch: 1478 / loss: 0.1356182098388672 / accuracy: 1.0\n",
      "epoch: 1479 / loss: 0.13544489443302155 / accuracy: 1.0\n",
      "epoch: 1480 / loss: 0.13527202606201172 / accuracy: 1.0\n",
      "epoch: 1481 / loss: 0.13509953022003174 / accuracy: 1.0\n",
      "epoch: 1482 / loss: 0.13492748141288757 / accuracy: 1.0\n",
      "epoch: 1483 / loss: 0.13475582003593445 / accuracy: 1.0\n",
      "epoch: 1484 / loss: 0.13458465039730072 / accuracy: 1.0\n",
      "epoch: 1485 / loss: 0.13441386818885803 / accuracy: 1.0\n",
      "epoch: 1486 / loss: 0.13424338400363922 / accuracy: 1.0\n",
      "epoch: 1487 / loss: 0.13407330214977264 / accuracy: 1.0\n",
      "epoch: 1488 / loss: 0.13390377163887024 / accuracy: 1.0\n",
      "epoch: 1489 / loss: 0.13373462855815887 / accuracy: 1.0\n",
      "epoch: 1490 / loss: 0.13356587290763855 / accuracy: 1.0\n",
      "epoch: 1491 / loss: 0.1333974152803421 / accuracy: 1.0\n",
      "epoch: 1492 / loss: 0.13322965800762177 / accuracy: 1.0\n",
      "epoch: 1493 / loss: 0.13306184113025665 / accuracy: 1.0\n",
      "epoch: 1494 / loss: 0.13289465010166168 / accuracy: 1.0\n",
      "epoch: 1495 / loss: 0.13272790610790253 / accuracy: 1.0\n",
      "epoch: 1496 / loss: 0.13256147503852844 / accuracy: 1.0\n",
      "epoch: 1497 / loss: 0.13239547610282898 / accuracy: 1.0\n",
      "epoch: 1498 / loss: 0.1322299987077713 / accuracy: 1.0\n",
      "epoch: 1499 / loss: 0.13206475973129272 / accuracy: 1.0\n",
      "epoch: 1500 / loss: 0.1319001019001007 / accuracy: 1.0\n",
      "epoch: 1501 / loss: 0.13173550367355347 / accuracy: 1.0\n",
      "epoch: 1502 / loss: 0.13157153129577637 / accuracy: 1.0\n",
      "epoch: 1503 / loss: 0.13140781223773956 / accuracy: 1.0\n",
      "epoch: 1504 / loss: 0.13124454021453857 / accuracy: 1.0\n",
      "epoch: 1505 / loss: 0.13108175992965698 / accuracy: 1.0\n",
      "epoch: 1506 / loss: 0.13091924786567688 / accuracy: 1.0\n",
      "epoch: 1507 / loss: 0.1307571679353714 / accuracy: 1.0\n",
      "epoch: 1508 / loss: 0.13059546053409576 / accuracy: 1.0\n",
      "epoch: 1509 / loss: 0.13043414056301117 / accuracy: 1.0\n",
      "epoch: 1510 / loss: 0.13027307391166687 / accuracy: 1.0\n",
      "epoch: 1511 / loss: 0.13011232018470764 / accuracy: 1.0\n",
      "epoch: 1512 / loss: 0.12995219230651855 / accuracy: 1.0\n",
      "epoch: 1513 / loss: 0.12979243695735931 / accuracy: 1.0\n",
      "epoch: 1514 / loss: 0.12963275611400604 / accuracy: 1.0\n",
      "epoch: 1515 / loss: 0.12947365641593933 / accuracy: 1.0\n",
      "epoch: 1516 / loss: 0.12931515276432037 / accuracy: 1.0\n",
      "epoch: 1517 / loss: 0.12915661931037903 / accuracy: 1.0\n",
      "epoch: 1518 / loss: 0.12899863719940186 / accuracy: 1.0\n",
      "epoch: 1519 / loss: 0.12884114682674408 / accuracy: 1.0\n",
      "epoch: 1520 / loss: 0.1286836713552475 / accuracy: 1.0\n",
      "epoch: 1521 / loss: 0.12852683663368225 / accuracy: 1.0\n",
      "epoch: 1522 / loss: 0.1283702850341797 / accuracy: 1.0\n",
      "epoch: 1523 / loss: 0.12821407616138458 / accuracy: 1.0\n",
      "epoch: 1524 / loss: 0.12805818021297455 / accuracy: 1.0\n",
      "epoch: 1525 / loss: 0.12790289521217346 / accuracy: 1.0\n",
      "epoch: 1526 / loss: 0.12774774432182312 / accuracy: 1.0\n",
      "epoch: 1527 / loss: 0.12759298086166382 / accuracy: 1.0\n",
      "epoch: 1528 / loss: 0.1274385154247284 / accuracy: 1.0\n",
      "epoch: 1529 / loss: 0.12728451192378998 / accuracy: 1.0\n",
      "epoch: 1530 / loss: 0.12713100016117096 / accuracy: 1.0\n",
      "epoch: 1531 / loss: 0.12697742879390717 / accuracy: 1.0\n",
      "epoch: 1532 / loss: 0.12682446837425232 / accuracy: 1.0\n",
      "epoch: 1533 / loss: 0.1266719102859497 / accuracy: 1.0\n",
      "epoch: 1534 / loss: 0.12651953101158142 / accuracy: 1.0\n",
      "epoch: 1535 / loss: 0.12636753916740417 / accuracy: 1.0\n",
      "epoch: 1536 / loss: 0.12621597945690155 / accuracy: 1.0\n",
      "epoch: 1537 / loss: 0.12606479227542877 / accuracy: 1.0\n",
      "epoch: 1538 / loss: 0.1259137988090515 / accuracy: 1.0\n",
      "epoch: 1539 / loss: 0.1257631927728653 / accuracy: 1.0\n",
      "epoch: 1540 / loss: 0.1256130039691925 / accuracy: 1.0\n",
      "epoch: 1541 / loss: 0.12546300888061523 / accuracy: 1.0\n",
      "epoch: 1542 / loss: 0.12531353533267975 / accuracy: 1.0\n",
      "epoch: 1543 / loss: 0.12516430020332336 / accuracy: 1.0\n",
      "epoch: 1544 / loss: 0.1250155121088028 / accuracy: 1.0\n",
      "epoch: 1545 / loss: 0.12486696988344193 / accuracy: 1.0\n",
      "epoch: 1546 / loss: 0.1247188001871109 / accuracy: 1.0\n",
      "epoch: 1547 / loss: 0.12457077205181122 / accuracy: 1.0\n",
      "epoch: 1548 / loss: 0.12442329525947571 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1549 / loss: 0.1242760717868805 / accuracy: 1.0\n",
      "epoch: 1550 / loss: 0.1241292953491211 / accuracy: 1.0\n",
      "epoch: 1551 / loss: 0.12398270517587662 / accuracy: 1.0\n",
      "epoch: 1552 / loss: 0.12383648753166199 / accuracy: 1.0\n",
      "epoch: 1553 / loss: 0.1236906498670578 / accuracy: 1.0\n",
      "epoch: 1554 / loss: 0.12354500591754913 / accuracy: 1.0\n",
      "epoch: 1555 / loss: 0.12339979410171509 / accuracy: 1.0\n",
      "epoch: 1556 / loss: 0.12325489521026611 / accuracy: 1.0\n",
      "epoch: 1557 / loss: 0.12311030924320221 / accuracy: 1.0\n",
      "epoch: 1558 / loss: 0.12296611070632935 / accuracy: 1.0\n",
      "epoch: 1559 / loss: 0.12282209098339081 / accuracy: 1.0\n",
      "epoch: 1560 / loss: 0.12267845124006271 / accuracy: 1.0\n",
      "epoch: 1561 / loss: 0.12253519147634506 / accuracy: 1.0\n",
      "epoch: 1562 / loss: 0.12239212542772293 / accuracy: 1.0\n",
      "epoch: 1563 / loss: 0.12224942445755005 / accuracy: 1.0\n",
      "epoch: 1564 / loss: 0.12210716307163239 / accuracy: 1.0\n",
      "epoch: 1565 / loss: 0.12196516245603561 / accuracy: 1.0\n",
      "epoch: 1566 / loss: 0.12182334065437317 / accuracy: 1.0\n",
      "epoch: 1567 / loss: 0.12168196588754654 / accuracy: 1.0\n",
      "epoch: 1568 / loss: 0.12154083698987961 / accuracy: 1.0\n",
      "epoch: 1569 / loss: 0.12139996886253357 / accuracy: 1.0\n",
      "epoch: 1570 / loss: 0.12125952541828156 / accuracy: 1.0\n",
      "epoch: 1571 / loss: 0.12111934274435043 / accuracy: 1.0\n",
      "epoch: 1572 / loss: 0.1209794133901596 / accuracy: 1.0\n",
      "epoch: 1573 / loss: 0.12083979696035385 / accuracy: 1.0\n",
      "epoch: 1574 / loss: 0.12070073187351227 / accuracy: 1.0\n",
      "epoch: 1575 / loss: 0.12056156247854233 / accuracy: 1.0\n",
      "epoch: 1576 / loss: 0.12042299658060074 / accuracy: 1.0\n",
      "epoch: 1577 / loss: 0.12028476595878601 / accuracy: 1.0\n",
      "epoch: 1578 / loss: 0.12014664709568024 / accuracy: 1.0\n",
      "epoch: 1579 / loss: 0.12000897526741028 / accuracy: 1.0\n",
      "epoch: 1580 / loss: 0.11987142264842987 / accuracy: 1.0\n",
      "epoch: 1581 / loss: 0.1197342574596405 / accuracy: 1.0\n",
      "epoch: 1582 / loss: 0.11959751695394516 / accuracy: 1.0\n",
      "epoch: 1583 / loss: 0.11946085095405579 / accuracy: 1.0\n",
      "epoch: 1584 / loss: 0.11932466924190521 / accuracy: 1.0\n",
      "epoch: 1585 / loss: 0.11918869614601135 / accuracy: 1.0\n",
      "epoch: 1586 / loss: 0.11905315518379211 / accuracy: 1.0\n",
      "epoch: 1587 / loss: 0.11891767382621765 / accuracy: 1.0\n",
      "epoch: 1588 / loss: 0.11878251284360886 / accuracy: 1.0\n",
      "epoch: 1589 / loss: 0.11864784359931946 / accuracy: 1.0\n",
      "epoch: 1590 / loss: 0.11851324141025543 / accuracy: 1.0\n",
      "epoch: 1591 / loss: 0.11837919801473618 / accuracy: 1.0\n",
      "epoch: 1592 / loss: 0.11824522912502289 / accuracy: 1.0\n",
      "epoch: 1593 / loss: 0.11811163276433945 / accuracy: 1.0\n",
      "epoch: 1594 / loss: 0.1179782822728157 / accuracy: 1.0\n",
      "epoch: 1595 / loss: 0.11784518510103226 / accuracy: 1.0\n",
      "epoch: 1596 / loss: 0.11771246790885925 / accuracy: 1.0\n",
      "epoch: 1597 / loss: 0.11757993698120117 / accuracy: 1.0\n",
      "epoch: 1598 / loss: 0.11744783818721771 / accuracy: 1.0\n",
      "epoch: 1599 / loss: 0.1173158586025238 / accuracy: 1.0\n",
      "epoch: 1600 / loss: 0.11718426644802094 / accuracy: 1.0\n",
      "epoch: 1601 / loss: 0.11705286055803299 / accuracy: 1.0\n",
      "epoch: 1602 / loss: 0.11692182719707489 / accuracy: 1.0\n",
      "epoch: 1603 / loss: 0.11679099500179291 / accuracy: 1.0\n",
      "epoch: 1604 / loss: 0.11666040122509003 / accuracy: 1.0\n",
      "epoch: 1605 / loss: 0.11653036624193192 / accuracy: 1.0\n",
      "epoch: 1606 / loss: 0.11640028655529022 / accuracy: 1.0\n",
      "epoch: 1607 / loss: 0.11627069115638733 / accuracy: 1.0\n",
      "epoch: 1608 / loss: 0.1161411702632904 / accuracy: 1.0\n",
      "epoch: 1609 / loss: 0.11601201444864273 / accuracy: 1.0\n",
      "epoch: 1610 / loss: 0.11588312685489655 / accuracy: 1.0\n",
      "epoch: 1611 / loss: 0.11575458943843842 / accuracy: 1.0\n",
      "epoch: 1612 / loss: 0.11562613397836685 / accuracy: 1.0\n",
      "epoch: 1613 / loss: 0.11549822986125946 / accuracy: 1.0\n",
      "epoch: 1614 / loss: 0.11537051945924759 / accuracy: 1.0\n",
      "epoch: 1615 / loss: 0.11524300277233124 / accuracy: 1.0\n",
      "epoch: 1616 / loss: 0.11511579155921936 / accuracy: 1.0\n",
      "epoch: 1617 / loss: 0.11498883366584778 / accuracy: 1.0\n",
      "epoch: 1618 / loss: 0.11486193537712097 / accuracy: 1.0\n",
      "epoch: 1619 / loss: 0.11473560333251953 / accuracy: 1.0\n",
      "epoch: 1620 / loss: 0.11460940539836884 / accuracy: 1.0\n",
      "epoch: 1621 / loss: 0.1144835576415062 / accuracy: 1.0\n",
      "epoch: 1622 / loss: 0.1143578588962555 / accuracy: 1.0\n",
      "epoch: 1623 / loss: 0.11423251777887344 / accuracy: 1.0\n",
      "epoch: 1624 / loss: 0.11410744488239288 / accuracy: 1.0\n",
      "epoch: 1625 / loss: 0.11398248374462128 / accuracy: 1.0\n",
      "epoch: 1626 / loss: 0.11385784298181534 / accuracy: 1.0\n",
      "epoch: 1627 / loss: 0.11373357474803925 / accuracy: 1.0\n",
      "epoch: 1628 / loss: 0.11360950022935867 / accuracy: 1.0\n",
      "epoch: 1629 / loss: 0.11348559707403183 / accuracy: 1.0\n",
      "epoch: 1630 / loss: 0.11336208134889603 / accuracy: 1.0\n",
      "epoch: 1631 / loss: 0.11323869228363037 / accuracy: 1.0\n",
      "epoch: 1632 / loss: 0.11311572790145874 / accuracy: 1.0\n",
      "epoch: 1633 / loss: 0.11299271881580353 / accuracy: 1.0\n",
      "epoch: 1634 / loss: 0.11287038028240204 / accuracy: 1.0\n",
      "epoch: 1635 / loss: 0.11274799704551697 / accuracy: 1.0\n",
      "epoch: 1636 / loss: 0.11262597888708115 / accuracy: 1.0\n",
      "epoch: 1637 / loss: 0.11250408738851547 / accuracy: 1.0\n",
      "epoch: 1638 / loss: 0.11238269507884979 / accuracy: 1.0\n",
      "epoch: 1639 / loss: 0.11226142197847366 / accuracy: 1.0\n",
      "epoch: 1640 / loss: 0.11214034259319305 / accuracy: 1.0\n",
      "epoch: 1641 / loss: 0.11201945692300797 / accuracy: 1.0\n",
      "epoch: 1642 / loss: 0.1118989959359169 / accuracy: 1.0\n",
      "epoch: 1643 / loss: 0.11177860200405121 / accuracy: 1.0\n",
      "epoch: 1644 / loss: 0.11165865510702133 / accuracy: 1.0\n",
      "epoch: 1645 / loss: 0.111538827419281 / accuracy: 1.0\n",
      "epoch: 1646 / loss: 0.11141930520534515 / accuracy: 1.0\n",
      "epoch: 1647 / loss: 0.11129985004663467 / accuracy: 1.0\n",
      "epoch: 1648 / loss: 0.11118078231811523 / accuracy: 1.0\n",
      "epoch: 1649 / loss: 0.11106200516223907 / accuracy: 1.0\n",
      "epoch: 1650 / loss: 0.11094343662261963 / accuracy: 1.0\n",
      "epoch: 1651 / loss: 0.11082497984170914 / accuracy: 1.0\n",
      "epoch: 1652 / loss: 0.11070701479911804 / accuracy: 1.0\n",
      "epoch: 1653 / loss: 0.11058906465768814 / accuracy: 1.0\n",
      "epoch: 1654 / loss: 0.11047142744064331 / accuracy: 1.0\n",
      "epoch: 1655 / loss: 0.11035409569740295 / accuracy: 1.0\n",
      "epoch: 1656 / loss: 0.11023689061403275 / accuracy: 1.0\n",
      "epoch: 1657 / loss: 0.11011999845504761 / accuracy: 1.0\n",
      "epoch: 1658 / loss: 0.11000329256057739 / accuracy: 1.0\n",
      "epoch: 1659 / loss: 0.10988689959049225 / accuracy: 1.0\n",
      "epoch: 1660 / loss: 0.10977063328027725 / accuracy: 1.0\n",
      "epoch: 1661 / loss: 0.10965467244386673 / accuracy: 1.0\n",
      "epoch: 1662 / loss: 0.10953877866268158 / accuracy: 1.0\n",
      "epoch: 1663 / loss: 0.10942356288433075 / accuracy: 1.0\n",
      "epoch: 1664 / loss: 0.10930823534727097 / accuracy: 1.0\n",
      "epoch: 1665 / loss: 0.1091931015253067 / accuracy: 1.0\n",
      "epoch: 1666 / loss: 0.1090783178806305 / accuracy: 1.0\n",
      "epoch: 1667 / loss: 0.10896380245685577 / accuracy: 1.0\n",
      "epoch: 1668 / loss: 0.10884947329759598 / accuracy: 1.0\n",
      "epoch: 1669 / loss: 0.1087353304028511 / accuracy: 1.0\n",
      "epoch: 1670 / loss: 0.1086214929819107 / accuracy: 1.0\n",
      "epoch: 1671 / loss: 0.1085076630115509 / accuracy: 1.0\n",
      "epoch: 1672 / loss: 0.10839438438415527 / accuracy: 1.0\n",
      "epoch: 1673 / loss: 0.10828105360269547 / accuracy: 1.0\n",
      "epoch: 1674 / loss: 0.10816803574562073 / accuracy: 1.0\n",
      "epoch: 1675 / loss: 0.10805532336235046 / accuracy: 1.0\n",
      "epoch: 1676 / loss: 0.10794274508953094 / accuracy: 1.0\n",
      "epoch: 1677 / loss: 0.1078304648399353 / accuracy: 1.0\n",
      "epoch: 1678 / loss: 0.10771843791007996 / accuracy: 1.0\n",
      "epoch: 1679 / loss: 0.10760647058486938 / accuracy: 1.0\n",
      "epoch: 1680 / loss: 0.1074950098991394 / accuracy: 1.0\n",
      "epoch: 1681 / loss: 0.10738342255353928 / accuracy: 1.0\n",
      "epoch: 1682 / loss: 0.10727221518754959 / accuracy: 1.0\n",
      "epoch: 1683 / loss: 0.10716124624013901 / accuracy: 1.0\n",
      "epoch: 1684 / loss: 0.10705053806304932 / accuracy: 1.0\n",
      "epoch: 1685 / loss: 0.10694000869989395 / accuracy: 1.0\n",
      "epoch: 1686 / loss: 0.10682953894138336 / accuracy: 1.0\n",
      "epoch: 1687 / loss: 0.10671951621770859 / accuracy: 1.0\n",
      "epoch: 1688 / loss: 0.10660961270332336 / accuracy: 1.0\n",
      "epoch: 1689 / loss: 0.10650008171796799 / accuracy: 1.0\n",
      "epoch: 1690 / loss: 0.10639049112796783 / accuracy: 1.0\n",
      "epoch: 1691 / loss: 0.10628127306699753 / accuracy: 1.0\n",
      "epoch: 1692 / loss: 0.1061721220612526 / accuracy: 1.0\n",
      "epoch: 1693 / loss: 0.10606339573860168 / accuracy: 1.0\n",
      "epoch: 1694 / loss: 0.10595487058162689 / accuracy: 1.0\n",
      "epoch: 1695 / loss: 0.10584646463394165 / accuracy: 1.0\n",
      "epoch: 1696 / loss: 0.10573811829090118 / accuracy: 1.0\n",
      "epoch: 1697 / loss: 0.10563033074140549 / accuracy: 1.0\n",
      "epoch: 1698 / loss: 0.10552236437797546 / accuracy: 1.0\n",
      "epoch: 1699 / loss: 0.10541488975286484 / accuracy: 1.0\n",
      "epoch: 1700 / loss: 0.10530754178762436 / accuracy: 1.0\n",
      "epoch: 1701 / loss: 0.1052003800868988 / accuracy: 1.0\n",
      "epoch: 1702 / loss: 0.10509346425533295 / accuracy: 1.0\n",
      "epoch: 1703 / loss: 0.10498686134815216 / accuracy: 1.0\n",
      "epoch: 1704 / loss: 0.10488026589155197 / accuracy: 1.0\n",
      "epoch: 1705 / loss: 0.10477398335933685 / accuracy: 1.0\n",
      "epoch: 1706 / loss: 0.10466800630092621 / accuracy: 1.0\n",
      "epoch: 1707 / loss: 0.10456196218729019 / accuracy: 1.0\n",
      "epoch: 1708 / loss: 0.10445629805326462 / accuracy: 1.0\n",
      "epoch: 1709 / loss: 0.10435076057910919 / accuracy: 1.0\n",
      "epoch: 1710 / loss: 0.10424564778804779 / accuracy: 1.0\n",
      "epoch: 1711 / loss: 0.10414042323827744 / accuracy: 1.0\n",
      "epoch: 1712 / loss: 0.1040356308221817 / accuracy: 1.0\n",
      "epoch: 1713 / loss: 0.10393089056015015 / accuracy: 1.0\n",
      "epoch: 1714 / loss: 0.10382647812366486 / accuracy: 1.0\n",
      "epoch: 1715 / loss: 0.10372211784124374 / accuracy: 1.0\n",
      "epoch: 1716 / loss: 0.1036180704832077 / accuracy: 1.0\n",
      "epoch: 1717 / loss: 0.10351433604955673 / accuracy: 1.0\n",
      "epoch: 1718 / loss: 0.10341042280197144 / accuracy: 1.0\n",
      "epoch: 1719 / loss: 0.10330717265605927 / accuracy: 1.0\n",
      "epoch: 1720 / loss: 0.10320375859737396 / accuracy: 1.0\n",
      "epoch: 1721 / loss: 0.10310077667236328 / accuracy: 1.0\n",
      "epoch: 1722 / loss: 0.1029977798461914 / accuracy: 1.0\n",
      "epoch: 1723 / loss: 0.10289517045021057 / accuracy: 1.0\n",
      "epoch: 1724 / loss: 0.10279267281293869 / accuracy: 1.0\n",
      "epoch: 1725 / loss: 0.10269031673669815 / accuracy: 1.0\n",
      "epoch: 1726 / loss: 0.10258826613426208 / accuracy: 1.0\n",
      "epoch: 1727 / loss: 0.1024862676858902 / accuracy: 1.0\n",
      "epoch: 1728 / loss: 0.10238458961248398 / accuracy: 1.0\n",
      "epoch: 1729 / loss: 0.10228309035301208 / accuracy: 1.0\n",
      "epoch: 1730 / loss: 0.10218173265457153 / accuracy: 1.0\n",
      "epoch: 1731 / loss: 0.10208061337471008 / accuracy: 1.0\n",
      "epoch: 1732 / loss: 0.10197979211807251 / accuracy: 1.0\n",
      "epoch: 1733 / loss: 0.10187892615795135 / accuracy: 1.0\n",
      "epoch: 1734 / loss: 0.10177842527627945 / accuracy: 1.0\n",
      "epoch: 1735 / loss: 0.10167805105447769 / accuracy: 1.0\n",
      "epoch: 1736 / loss: 0.10157767683267593 / accuracy: 1.0\n",
      "epoch: 1737 / loss: 0.1014777421951294 / accuracy: 1.0\n",
      "epoch: 1738 / loss: 0.10137799382209778 / accuracy: 1.0\n",
      "epoch: 1739 / loss: 0.10127848386764526 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1740 / loss: 0.10117892175912857 / accuracy: 1.0\n",
      "epoch: 1741 / loss: 0.10107966512441635 / accuracy: 1.0\n",
      "epoch: 1742 / loss: 0.10098066180944443 / accuracy: 1.0\n",
      "epoch: 1743 / loss: 0.10088183730840683 / accuracy: 1.0\n",
      "epoch: 1744 / loss: 0.1007830873131752 / accuracy: 1.0\n",
      "epoch: 1745 / loss: 0.10068458318710327 / accuracy: 1.0\n",
      "epoch: 1746 / loss: 0.10058631747961044 / accuracy: 1.0\n",
      "epoch: 1747 / loss: 0.10048811882734299 / accuracy: 1.0\n",
      "epoch: 1748 / loss: 0.10039018094539642 / accuracy: 1.0\n",
      "epoch: 1749 / loss: 0.10029241442680359 / accuracy: 1.0\n",
      "epoch: 1750 / loss: 0.10019484162330627 / accuracy: 1.0\n",
      "epoch: 1751 / loss: 0.10009732842445374 / accuracy: 1.0\n",
      "epoch: 1752 / loss: 0.1000003069639206 / accuracy: 1.0\n",
      "epoch: 1753 / loss: 0.09990304708480835 / accuracy: 1.0\n",
      "epoch: 1754 / loss: 0.09980621933937073 / accuracy: 1.0\n",
      "epoch: 1755 / loss: 0.09970951080322266 / accuracy: 1.0\n",
      "epoch: 1756 / loss: 0.09961311519145966 / accuracy: 1.0\n",
      "epoch: 1757 / loss: 0.09951677918434143 / accuracy: 1.0\n",
      "epoch: 1758 / loss: 0.09942057728767395 / accuracy: 1.0\n",
      "epoch: 1759 / loss: 0.09932450205087662 / accuracy: 1.0\n",
      "epoch: 1760 / loss: 0.09922872483730316 / accuracy: 1.0\n",
      "epoch: 1761 / loss: 0.09913326054811478 / accuracy: 1.0\n",
      "epoch: 1762 / loss: 0.09903780370950699 / accuracy: 1.0\n",
      "epoch: 1763 / loss: 0.09894251823425293 / accuracy: 1.0\n",
      "epoch: 1764 / loss: 0.09884737432003021 / accuracy: 1.0\n",
      "epoch: 1765 / loss: 0.09875252842903137 / accuracy: 1.0\n",
      "epoch: 1766 / loss: 0.09865788370370865 / accuracy: 1.0\n",
      "epoch: 1767 / loss: 0.09856321662664413 / accuracy: 1.0\n",
      "epoch: 1768 / loss: 0.09846888482570648 / accuracy: 1.0\n",
      "epoch: 1769 / loss: 0.09837466478347778 / accuracy: 1.0\n",
      "epoch: 1770 / loss: 0.09828062355518341 / accuracy: 1.0\n",
      "epoch: 1771 / loss: 0.09818685054779053 / accuracy: 1.0\n",
      "epoch: 1772 / loss: 0.09809300303459167 / accuracy: 1.0\n",
      "epoch: 1773 / loss: 0.0979994609951973 / accuracy: 1.0\n",
      "epoch: 1774 / loss: 0.09790617227554321 / accuracy: 1.0\n",
      "epoch: 1775 / loss: 0.09781289100646973 / accuracy: 1.0\n",
      "epoch: 1776 / loss: 0.09771984815597534 / accuracy: 1.0\n",
      "epoch: 1777 / loss: 0.09762700647115707 / accuracy: 1.0\n",
      "epoch: 1778 / loss: 0.09753434360027313 / accuracy: 1.0\n",
      "epoch: 1779 / loss: 0.0974419116973877 / accuracy: 1.0\n",
      "epoch: 1780 / loss: 0.097349613904953 / accuracy: 1.0\n",
      "epoch: 1781 / loss: 0.0972573310136795 / accuracy: 1.0\n",
      "epoch: 1782 / loss: 0.09716540575027466 / accuracy: 1.0\n",
      "epoch: 1783 / loss: 0.09707342088222504 / accuracy: 1.0\n",
      "epoch: 1784 / loss: 0.09698182344436646 / accuracy: 1.0\n",
      "epoch: 1785 / loss: 0.096890389919281 / accuracy: 1.0\n",
      "epoch: 1786 / loss: 0.09679903090000153 / accuracy: 1.0\n",
      "epoch: 1787 / loss: 0.09670785069465637 / accuracy: 1.0\n",
      "epoch: 1788 / loss: 0.09661686420440674 / accuracy: 1.0\n",
      "epoch: 1789 / loss: 0.09652593731880188 / accuracy: 1.0\n",
      "epoch: 1790 / loss: 0.0964353159070015 / accuracy: 1.0\n",
      "epoch: 1791 / loss: 0.09634481370449066 / accuracy: 1.0\n",
      "epoch: 1792 / loss: 0.0962543934583664 / accuracy: 1.0\n",
      "epoch: 1793 / loss: 0.09616408497095108 / accuracy: 1.0\n",
      "epoch: 1794 / loss: 0.09607414901256561 / accuracy: 1.0\n",
      "epoch: 1795 / loss: 0.09598421305418015 / accuracy: 1.0\n",
      "epoch: 1796 / loss: 0.09589459002017975 / accuracy: 1.0\n",
      "epoch: 1797 / loss: 0.09580489993095398 / accuracy: 1.0\n",
      "epoch: 1798 / loss: 0.09571577608585358 / accuracy: 1.0\n",
      "epoch: 1799 / loss: 0.09562645852565765 / accuracy: 1.0\n",
      "epoch: 1800 / loss: 0.09553738683462143 / accuracy: 1.0\n",
      "epoch: 1801 / loss: 0.0954483300447464 / accuracy: 1.0\n",
      "epoch: 1802 / loss: 0.0953596979379654 / accuracy: 1.0\n",
      "epoch: 1803 / loss: 0.09527100622653961 / accuracy: 1.0\n",
      "epoch: 1804 / loss: 0.09518256038427353 / accuracy: 1.0\n",
      "epoch: 1805 / loss: 0.09509442001581192 / accuracy: 1.0\n",
      "epoch: 1806 / loss: 0.09500622749328613 / accuracy: 1.0\n",
      "epoch: 1807 / loss: 0.09491821378469467 / accuracy: 1.0\n",
      "epoch: 1808 / loss: 0.09483051300048828 / accuracy: 1.0\n",
      "epoch: 1809 / loss: 0.09474276006221771 / accuracy: 1.0\n",
      "epoch: 1810 / loss: 0.09465523064136505 / accuracy: 1.0\n",
      "epoch: 1811 / loss: 0.09456785023212433 / accuracy: 1.0\n",
      "epoch: 1812 / loss: 0.09448069334030151 / accuracy: 1.0\n",
      "epoch: 1813 / loss: 0.09439355880022049 / accuracy: 1.0\n",
      "epoch: 1814 / loss: 0.09430690854787827 / accuracy: 1.0\n",
      "epoch: 1815 / loss: 0.09422007203102112 / accuracy: 1.0\n",
      "epoch: 1816 / loss: 0.09413337707519531 / accuracy: 1.0\n",
      "epoch: 1817 / loss: 0.09404702484607697 / accuracy: 1.0\n",
      "epoch: 1818 / loss: 0.093960702419281 / accuracy: 1.0\n",
      "epoch: 1819 / loss: 0.09387466311454773 / accuracy: 1.0\n",
      "epoch: 1820 / loss: 0.09378869831562042 / accuracy: 1.0\n",
      "epoch: 1821 / loss: 0.09370280057191849 / accuracy: 1.0\n",
      "epoch: 1822 / loss: 0.09361721575260162 / accuracy: 1.0\n",
      "epoch: 1823 / loss: 0.09353179484605789 / accuracy: 1.0\n",
      "epoch: 1824 / loss: 0.09344632923603058 / accuracy: 1.0\n",
      "epoch: 1825 / loss: 0.09336116909980774 / accuracy: 1.0\n",
      "epoch: 1826 / loss: 0.0932760089635849 / accuracy: 1.0\n",
      "epoch: 1827 / loss: 0.09319128096103668 / accuracy: 1.0\n",
      "epoch: 1828 / loss: 0.09310637414455414 / accuracy: 1.0\n",
      "epoch: 1829 / loss: 0.09302172064781189 / accuracy: 1.0\n",
      "epoch: 1830 / loss: 0.09293723106384277 / accuracy: 1.0\n",
      "epoch: 1831 / loss: 0.0928528904914856 / accuracy: 1.0\n",
      "epoch: 1832 / loss: 0.09276877343654633 / accuracy: 1.0\n",
      "epoch: 1833 / loss: 0.0926847979426384 / accuracy: 1.0\n",
      "epoch: 1834 / loss: 0.0926007553935051 / accuracy: 1.0\n",
      "epoch: 1835 / loss: 0.09251707792282104 / accuracy: 1.0\n",
      "epoch: 1836 / loss: 0.09243352711200714 / accuracy: 1.0\n",
      "epoch: 1837 / loss: 0.09235017001628876 / accuracy: 1.0\n",
      "epoch: 1838 / loss: 0.09226667881011963 / accuracy: 1.0\n",
      "epoch: 1839 / loss: 0.09218363463878632 / accuracy: 1.0\n",
      "epoch: 1840 / loss: 0.09210057556629181 / accuracy: 1.0\n",
      "epoch: 1841 / loss: 0.0920177698135376 / accuracy: 1.0\n",
      "epoch: 1842 / loss: 0.09193496406078339 / accuracy: 1.0\n",
      "epoch: 1843 / loss: 0.09185229241847992 / accuracy: 1.0\n",
      "epoch: 1844 / loss: 0.0917699784040451 / accuracy: 1.0\n",
      "epoch: 1845 / loss: 0.09168755263090134 / accuracy: 1.0\n",
      "epoch: 1846 / loss: 0.09160548448562622 / accuracy: 1.0\n",
      "epoch: 1847 / loss: 0.0915234237909317 / accuracy: 1.0\n",
      "epoch: 1848 / loss: 0.0914415493607521 / accuracy: 1.0\n",
      "epoch: 1849 / loss: 0.09135986119508743 / accuracy: 1.0\n",
      "epoch: 1850 / loss: 0.09127822518348694 / accuracy: 1.0\n",
      "epoch: 1851 / loss: 0.09119673073291779 / accuracy: 1.0\n",
      "epoch: 1852 / loss: 0.09111528098583221 / accuracy: 1.0\n",
      "epoch: 1853 / loss: 0.09103427827358246 / accuracy: 1.0\n",
      "epoch: 1854 / loss: 0.09095320850610733 / accuracy: 1.0\n",
      "epoch: 1855 / loss: 0.09087231755256653 / accuracy: 1.0\n",
      "epoch: 1856 / loss: 0.09079156070947647 / accuracy: 1.0\n",
      "epoch: 1857 / loss: 0.09071086347103119 / accuracy: 1.0\n",
      "epoch: 1858 / loss: 0.0906304121017456 / accuracy: 1.0\n",
      "epoch: 1859 / loss: 0.0905500203371048 / accuracy: 1.0\n",
      "epoch: 1860 / loss: 0.09046969562768936 / accuracy: 1.0\n",
      "epoch: 1861 / loss: 0.09038973599672318 / accuracy: 1.0\n",
      "epoch: 1862 / loss: 0.09030978381633759 / accuracy: 1.0\n",
      "epoch: 1863 / loss: 0.09023001044988632 / accuracy: 1.0\n",
      "epoch: 1864 / loss: 0.09015031158924103 / accuracy: 1.0\n",
      "epoch: 1865 / loss: 0.09007078409194946 / accuracy: 1.0\n",
      "epoch: 1866 / loss: 0.08999131619930267 / accuracy: 1.0\n",
      "epoch: 1867 / loss: 0.08991210907697678 / accuracy: 1.0\n",
      "epoch: 1868 / loss: 0.08983296155929565 / accuracy: 1.0\n",
      "epoch: 1869 / loss: 0.08975398540496826 / accuracy: 1.0\n",
      "epoch: 1870 / loss: 0.08967514336109161 / accuracy: 1.0\n",
      "epoch: 1871 / loss: 0.08959630131721497 / accuracy: 1.0\n",
      "epoch: 1872 / loss: 0.08951765298843384 / accuracy: 1.0\n",
      "epoch: 1873 / loss: 0.08943930268287659 / accuracy: 1.0\n",
      "epoch: 1874 / loss: 0.08936095237731934 / accuracy: 1.0\n",
      "epoch: 1875 / loss: 0.08928273618221283 / accuracy: 1.0\n",
      "epoch: 1876 / loss: 0.08920475095510483 / accuracy: 1.0\n",
      "epoch: 1877 / loss: 0.08912678062915802 / accuracy: 1.0\n",
      "epoch: 1878 / loss: 0.08904892951250076 / accuracy: 1.0\n",
      "epoch: 1879 / loss: 0.08897119760513306 / accuracy: 1.0\n",
      "epoch: 1880 / loss: 0.08889378607273102 / accuracy: 1.0\n",
      "epoch: 1881 / loss: 0.08881624042987823 / accuracy: 1.0\n",
      "epoch: 1882 / loss: 0.08873894065618515 / accuracy: 1.0\n",
      "epoch: 1883 / loss: 0.08866177499294281 / accuracy: 1.0\n",
      "epoch: 1884 / loss: 0.08858466148376465 / accuracy: 1.0\n",
      "epoch: 1885 / loss: 0.08850792795419693 / accuracy: 1.0\n",
      "epoch: 1886 / loss: 0.08843100070953369 / accuracy: 1.0\n",
      "epoch: 1887 / loss: 0.08835438638925552 / accuracy: 1.0\n",
      "epoch: 1888 / loss: 0.08827795088291168 / accuracy: 1.0\n",
      "epoch: 1889 / loss: 0.08820153027772903 / accuracy: 1.0\n",
      "epoch: 1890 / loss: 0.08812540769577026 / accuracy: 1.0\n",
      "epoch: 1891 / loss: 0.08804915845394135 / accuracy: 1.0\n",
      "epoch: 1892 / loss: 0.08797304332256317 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1893 / loss: 0.0878971666097641 / accuracy: 1.0\n",
      "epoch: 1894 / loss: 0.08782142400741577 / accuracy: 1.0\n",
      "epoch: 1895 / loss: 0.08774574100971222 / accuracy: 1.0\n",
      "epoch: 1896 / loss: 0.08767016977071762 / accuracy: 1.0\n",
      "epoch: 1897 / loss: 0.08759491890668869 / accuracy: 1.0\n",
      "epoch: 1898 / loss: 0.08751960098743439 / accuracy: 1.0\n",
      "epoch: 1899 / loss: 0.08744452893733978 / accuracy: 1.0\n",
      "epoch: 1900 / loss: 0.08736951649188995 / accuracy: 1.0\n",
      "epoch: 1901 / loss: 0.08729451894760132 / accuracy: 1.0\n",
      "epoch: 1902 / loss: 0.08721975982189178 / accuracy: 1.0\n",
      "epoch: 1903 / loss: 0.08714517951011658 / accuracy: 1.0\n",
      "epoch: 1904 / loss: 0.08707067370414734 / accuracy: 1.0\n",
      "epoch: 1905 / loss: 0.08699627965688705 / accuracy: 1.0\n",
      "epoch: 1906 / loss: 0.08692200481891632 / accuracy: 1.0\n",
      "epoch: 1907 / loss: 0.08684787154197693 / accuracy: 1.0\n",
      "epoch: 1908 / loss: 0.08677385747432709 / accuracy: 1.0\n",
      "epoch: 1909 / loss: 0.0866999626159668 / accuracy: 1.0\n",
      "epoch: 1910 / loss: 0.0866260677576065 / accuracy: 1.0\n",
      "epoch: 1911 / loss: 0.08655253797769547 / accuracy: 1.0\n",
      "epoch: 1912 / loss: 0.08647889643907547 / accuracy: 1.0\n",
      "epoch: 1913 / loss: 0.08640561252832413 / accuracy: 1.0\n",
      "epoch: 1914 / loss: 0.08633209019899368 / accuracy: 1.0\n",
      "epoch: 1915 / loss: 0.08625899255275726 / accuracy: 1.0\n",
      "epoch: 1916 / loss: 0.08618584275245667 / accuracy: 1.0\n",
      "epoch: 1917 / loss: 0.08611300587654114 / accuracy: 1.0\n",
      "epoch: 1918 / loss: 0.08604015409946442 / accuracy: 1.0\n",
      "epoch: 1919 / loss: 0.08596743643283844 / accuracy: 1.0\n",
      "epoch: 1920 / loss: 0.08589496463537216 / accuracy: 1.0\n",
      "epoch: 1921 / loss: 0.08582224696874619 / accuracy: 1.0\n",
      "epoch: 1922 / loss: 0.08575001358985901 / accuracy: 1.0\n",
      "epoch: 1923 / loss: 0.08567778766155243 / accuracy: 1.0\n",
      "epoch: 1924 / loss: 0.085605688393116 / accuracy: 1.0\n",
      "epoch: 1925 / loss: 0.08553358912467957 / accuracy: 1.0\n",
      "epoch: 1926 / loss: 0.08546178787946701 / accuracy: 1.0\n",
      "epoch: 1927 / loss: 0.0853898748755455 / accuracy: 1.0\n",
      "epoch: 1928 / loss: 0.08531825989484787 / accuracy: 1.0\n",
      "epoch: 1929 / loss: 0.08524677902460098 / accuracy: 1.0\n",
      "epoch: 1930 / loss: 0.08517535030841827 / accuracy: 1.0\n",
      "epoch: 1931 / loss: 0.08510398864746094 / accuracy: 1.0\n",
      "epoch: 1932 / loss: 0.0850328803062439 / accuracy: 1.0\n",
      "epoch: 1933 / loss: 0.08496169745922089 / accuracy: 1.0\n",
      "epoch: 1934 / loss: 0.08489077538251877 / accuracy: 1.0\n",
      "epoch: 1935 / loss: 0.08481978625059128 / accuracy: 1.0\n",
      "epoch: 1936 / loss: 0.08474922180175781 / accuracy: 1.0\n",
      "epoch: 1937 / loss: 0.0846785455942154 / accuracy: 1.0\n",
      "epoch: 1938 / loss: 0.08460798114538193 / accuracy: 1.0\n",
      "epoch: 1939 / loss: 0.08453760296106339 / accuracy: 1.0\n",
      "epoch: 1940 / loss: 0.08446735143661499 / accuracy: 1.0\n",
      "epoch: 1941 / loss: 0.08439710736274719 / accuracy: 1.0\n",
      "epoch: 1942 / loss: 0.08432722836732864 / accuracy: 1.0\n",
      "epoch: 1943 / loss: 0.08425716310739517 / accuracy: 1.0\n",
      "epoch: 1944 / loss: 0.08418728411197662 / accuracy: 1.0\n",
      "epoch: 1945 / loss: 0.08411765098571777 / accuracy: 1.0\n",
      "epoch: 1946 / loss: 0.08404795825481415 / accuracy: 1.0\n",
      "epoch: 1947 / loss: 0.08397838473320007 / accuracy: 1.0\n",
      "epoch: 1948 / loss: 0.08390912413597107 / accuracy: 1.0\n",
      "epoch: 1949 / loss: 0.0838397964835167 / accuracy: 1.0\n",
      "epoch: 1950 / loss: 0.08377065509557724 / accuracy: 1.0\n",
      "epoch: 1951 / loss: 0.08370146155357361 / accuracy: 1.0\n",
      "epoch: 1952 / loss: 0.08363256603479385 / accuracy: 1.0\n",
      "epoch: 1953 / loss: 0.08356384932994843 / accuracy: 1.0\n",
      "epoch: 1954 / loss: 0.08349496126174927 / accuracy: 1.0\n",
      "epoch: 1955 / loss: 0.08342644572257996 / accuracy: 1.0\n",
      "epoch: 1956 / loss: 0.0833577960729599 / accuracy: 1.0\n",
      "epoch: 1957 / loss: 0.08328951895236969 / accuracy: 1.0\n",
      "epoch: 1958 / loss: 0.08322112262248993 / accuracy: 1.0\n",
      "epoch: 1959 / loss: 0.08315291255712509 / accuracy: 1.0\n",
      "epoch: 1960 / loss: 0.0830848217010498 / accuracy: 1.0\n",
      "epoch: 1961 / loss: 0.08301697671413422 / accuracy: 1.0\n",
      "epoch: 1962 / loss: 0.08294889330863953 / accuracy: 1.0\n",
      "epoch: 1963 / loss: 0.08288122713565826 / accuracy: 1.0\n",
      "epoch: 1964 / loss: 0.0828135684132576 / accuracy: 1.0\n",
      "epoch: 1965 / loss: 0.08274591714143753 / accuracy: 1.0\n",
      "epoch: 1966 / loss: 0.08267862349748611 / accuracy: 1.0\n",
      "epoch: 1967 / loss: 0.08261115103960037 / accuracy: 1.0\n",
      "epoch: 1968 / loss: 0.08254404366016388 / accuracy: 1.0\n",
      "epoch: 1969 / loss: 0.08247694373130798 / accuracy: 1.0\n",
      "epoch: 1970 / loss: 0.08240972459316254 / accuracy: 1.0\n",
      "epoch: 1971 / loss: 0.08234286308288574 / accuracy: 1.0\n",
      "epoch: 1972 / loss: 0.08227606862783432 / accuracy: 1.0\n",
      "epoch: 1973 / loss: 0.08220939338207245 / accuracy: 1.0\n",
      "epoch: 1974 / loss: 0.08214278519153595 / accuracy: 1.0\n",
      "epoch: 1975 / loss: 0.08207623660564423 / accuracy: 1.0\n",
      "epoch: 1976 / loss: 0.0820099413394928 / accuracy: 1.0\n",
      "epoch: 1977 / loss: 0.081943579018116 / accuracy: 1.0\n",
      "epoch: 1978 / loss: 0.08187739551067352 / accuracy: 1.0\n",
      "epoch: 1979 / loss: 0.0818113461136818 / accuracy: 1.0\n",
      "epoch: 1980 / loss: 0.08174528926610947 / accuracy: 1.0\n",
      "epoch: 1981 / loss: 0.08167953789234161 / accuracy: 1.0\n",
      "epoch: 1982 / loss: 0.08161367475986481 / accuracy: 1.0\n",
      "epoch: 1983 / loss: 0.08154810965061188 / accuracy: 1.0\n",
      "epoch: 1984 / loss: 0.08148236572742462 / accuracy: 1.0\n",
      "epoch: 1985 / loss: 0.08141686767339706 / accuracy: 1.0\n",
      "epoch: 1986 / loss: 0.0813516154885292 / accuracy: 1.0\n",
      "epoch: 1987 / loss: 0.08128630369901657 / accuracy: 1.0\n",
      "epoch: 1988 / loss: 0.08122111111879349 / accuracy: 1.0\n",
      "epoch: 1989 / loss: 0.08115610480308533 / accuracy: 1.0\n",
      "epoch: 1990 / loss: 0.08109109103679657 / accuracy: 1.0\n",
      "epoch: 1991 / loss: 0.08102621883153915 / accuracy: 1.0\n",
      "epoch: 1992 / loss: 0.08096151053905487 / accuracy: 1.0\n",
      "epoch: 1993 / loss: 0.08089675009250641 / accuracy: 1.0\n",
      "epoch: 1994 / loss: 0.08083224296569824 / accuracy: 1.0\n",
      "epoch: 1995 / loss: 0.08076778799295425 / accuracy: 1.0\n",
      "epoch: 1996 / loss: 0.08070340007543564 / accuracy: 1.0\n",
      "epoch: 1997 / loss: 0.08063901215791702 / accuracy: 1.0\n",
      "epoch: 1998 / loss: 0.08057492971420288 / accuracy: 1.0\n",
      "epoch: 1999 / loss: 0.08051096647977829 / accuracy: 1.0\n",
      "epoch: 2000 / loss: 0.08044688403606415 / accuracy: 1.0\n",
      "epoch: 2001 / loss: 0.08038298785686493 / accuracy: 1.0\n",
      "epoch: 2002 / loss: 0.08031921833753586 / accuracy: 1.0\n",
      "epoch: 2003 / loss: 0.08025544881820679 / accuracy: 1.0\n",
      "epoch: 2004 / loss: 0.08019198477268219 / accuracy: 1.0\n",
      "epoch: 2005 / loss: 0.08012845367193222 / accuracy: 1.0\n",
      "epoch: 2006 / loss: 0.08006510883569717 / accuracy: 1.0\n",
      "epoch: 2007 / loss: 0.08000177145004272 / accuracy: 1.0\n",
      "epoch: 2008 / loss: 0.07993854582309723 / accuracy: 1.0\n",
      "epoch: 2009 / loss: 0.07987551391124725 / accuracy: 1.0\n",
      "epoch: 2010 / loss: 0.0798124223947525 / accuracy: 1.0\n",
      "epoch: 2011 / loss: 0.07974962890148163 / accuracy: 1.0\n",
      "epoch: 2012 / loss: 0.0796867236495018 / accuracy: 1.0\n",
      "epoch: 2013 / loss: 0.07962393760681152 / accuracy: 1.0\n",
      "epoch: 2014 / loss: 0.0795612633228302 / accuracy: 1.0\n",
      "epoch: 2015 / loss: 0.079498790204525 / accuracy: 1.0\n",
      "epoch: 2016 / loss: 0.07943637669086456 / accuracy: 1.0\n",
      "epoch: 2017 / loss: 0.07937407493591309 / accuracy: 1.0\n",
      "epoch: 2018 / loss: 0.07931166887283325 / accuracy: 1.0\n",
      "epoch: 2019 / loss: 0.07924967259168625 / accuracy: 1.0\n",
      "epoch: 2020 / loss: 0.0791875571012497 / accuracy: 1.0\n",
      "epoch: 2021 / loss: 0.07912564277648926 / accuracy: 1.0\n",
      "epoch: 2022 / loss: 0.0790637731552124 / accuracy: 1.0\n",
      "epoch: 2023 / loss: 0.07900191843509674 / accuracy: 1.0\n",
      "epoch: 2024 / loss: 0.07894010841846466 / accuracy: 1.0\n",
      "epoch: 2025 / loss: 0.07887867093086243 / accuracy: 1.0\n",
      "epoch: 2026 / loss: 0.07881718128919601 / accuracy: 1.0\n",
      "epoch: 2027 / loss: 0.0787556916475296 / accuracy: 1.0\n",
      "epoch: 2028 / loss: 0.07869431376457214 / accuracy: 1.0\n",
      "epoch: 2029 / loss: 0.07863319665193558 / accuracy: 1.0\n",
      "epoch: 2030 / loss: 0.07857207208871841 / accuracy: 1.0\n",
      "epoch: 2031 / loss: 0.07851094007492065 / accuracy: 1.0\n",
      "epoch: 2032 / loss: 0.07845006883144379 / accuracy: 1.0\n",
      "epoch: 2033 / loss: 0.07838906347751617 / accuracy: 1.0\n",
      "epoch: 2034 / loss: 0.07832825183868408 / accuracy: 1.0\n",
      "epoch: 2035 / loss: 0.07826773822307587 / accuracy: 1.0\n",
      "epoch: 2036 / loss: 0.07820723205804825 / accuracy: 1.0\n",
      "epoch: 2037 / loss: 0.07814672589302063 / accuracy: 1.0\n",
      "epoch: 2038 / loss: 0.07808627933263779 / accuracy: 1.0\n",
      "epoch: 2039 / loss: 0.0780259519815445 / accuracy: 1.0\n",
      "epoch: 2040 / loss: 0.07796575129032135 / accuracy: 1.0\n",
      "epoch: 2041 / loss: 0.07790566980838776 / accuracy: 1.0\n",
      "epoch: 2042 / loss: 0.07784564793109894 / accuracy: 1.0\n",
      "epoch: 2043 / loss: 0.07778564095497131 / accuracy: 1.0\n",
      "epoch: 2044 / loss: 0.07772574573755264 / accuracy: 1.0\n",
      "epoch: 2045 / loss: 0.07766591012477875 / accuracy: 1.0\n",
      "epoch: 2046 / loss: 0.077606201171875 / accuracy: 1.0\n",
      "epoch: 2047 / loss: 0.07754674553871155 / accuracy: 1.0\n",
      "epoch: 2048 / loss: 0.0774870365858078 / accuracy: 1.0\n",
      "epoch: 2049 / loss: 0.07742775976657867 / accuracy: 1.0\n",
      "epoch: 2050 / loss: 0.07736841589212418 / accuracy: 1.0\n",
      "epoch: 2051 / loss: 0.07730919867753983 / accuracy: 1.0\n",
      "epoch: 2052 / loss: 0.0772499293088913 / accuracy: 1.0\n",
      "epoch: 2053 / loss: 0.07719089090824127 / accuracy: 1.0\n",
      "epoch: 2054 / loss: 0.07713185995817184 / accuracy: 1.0\n",
      "epoch: 2055 / loss: 0.07707300782203674 / accuracy: 1.0\n",
      "epoch: 2056 / loss: 0.07701422274112701 / accuracy: 1.0\n",
      "epoch: 2057 / loss: 0.07695544511079788 / accuracy: 1.0\n",
      "epoch: 2058 / loss: 0.07689684629440308 / accuracy: 1.0\n",
      "epoch: 2059 / loss: 0.0768381804227829 / accuracy: 1.0\n",
      "epoch: 2060 / loss: 0.07677976787090302 / accuracy: 1.0\n",
      "epoch: 2061 / loss: 0.07672147452831268 / accuracy: 1.0\n",
      "epoch: 2062 / loss: 0.07666318118572235 / accuracy: 1.0\n",
      "epoch: 2063 / loss: 0.07660476118326187 / accuracy: 1.0\n",
      "epoch: 2064 / loss: 0.07654684036970139 / accuracy: 1.0\n",
      "epoch: 2065 / loss: 0.07648873329162598 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2066 / loss: 0.07643068581819534 / accuracy: 1.0\n",
      "epoch: 2067 / loss: 0.07637288421392441 / accuracy: 1.0\n",
      "epoch: 2068 / loss: 0.07631508260965347 / accuracy: 1.0\n",
      "epoch: 2069 / loss: 0.07625740766525269 / accuracy: 1.0\n",
      "epoch: 2070 / loss: 0.0761997252702713 / accuracy: 1.0\n",
      "epoch: 2071 / loss: 0.07614229619503021 / accuracy: 1.0\n",
      "epoch: 2072 / loss: 0.07608486711978912 / accuracy: 1.0\n",
      "epoch: 2073 / loss: 0.07602749019861221 / accuracy: 1.0\n",
      "epoch: 2074 / loss: 0.0759701281785965 / accuracy: 1.0\n",
      "epoch: 2075 / loss: 0.07591299712657928 / accuracy: 1.0\n",
      "epoch: 2076 / loss: 0.07585587352514267 / accuracy: 1.0\n",
      "epoch: 2077 / loss: 0.0757988691329956 / accuracy: 1.0\n",
      "epoch: 2078 / loss: 0.07574187219142914 / accuracy: 1.0\n",
      "epoch: 2079 / loss: 0.075685054063797 / accuracy: 1.0\n",
      "epoch: 2080 / loss: 0.07562829554080963 / accuracy: 1.0\n",
      "epoch: 2081 / loss: 0.07557160407304764 / accuracy: 1.0\n",
      "epoch: 2082 / loss: 0.07551497220993042 / accuracy: 1.0\n",
      "epoch: 2083 / loss: 0.07545840740203857 / accuracy: 1.0\n",
      "epoch: 2084 / loss: 0.07540188729763031 / accuracy: 1.0\n",
      "epoch: 2085 / loss: 0.07534544169902802 / accuracy: 1.0\n",
      "epoch: 2086 / loss: 0.07528924196958542 / accuracy: 1.0\n",
      "epoch: 2087 / loss: 0.07523304224014282 / accuracy: 1.0\n",
      "epoch: 2088 / loss: 0.07517677545547485 / accuracy: 1.0\n",
      "epoch: 2089 / loss: 0.07512082159519196 / accuracy: 1.0\n",
      "epoch: 2090 / loss: 0.07506480813026428 / accuracy: 1.0\n",
      "epoch: 2091 / loss: 0.07500903308391571 / accuracy: 1.0\n",
      "epoch: 2092 / loss: 0.07495313882827759 / accuracy: 1.0\n",
      "epoch: 2093 / loss: 0.07489742338657379 / accuracy: 1.0\n",
      "epoch: 2094 / loss: 0.07484184205532074 / accuracy: 1.0\n",
      "epoch: 2095 / loss: 0.07478625327348709 / accuracy: 1.0\n",
      "epoch: 2096 / loss: 0.07473066449165344 / accuracy: 1.0\n",
      "epoch: 2097 / loss: 0.07467526197433472 / accuracy: 1.0\n",
      "epoch: 2098 / loss: 0.07461992651224136 / accuracy: 1.0\n",
      "epoch: 2099 / loss: 0.07456471025943756 / accuracy: 1.0\n",
      "epoch: 2100 / loss: 0.07450955361127853 / accuracy: 1.0\n",
      "epoch: 2101 / loss: 0.07445445656776428 / accuracy: 1.0\n",
      "epoch: 2102 / loss: 0.07439954578876495 / accuracy: 1.0\n",
      "epoch: 2103 / loss: 0.07434463500976562 / accuracy: 1.0\n",
      "epoch: 2104 / loss: 0.0742897242307663 / accuracy: 1.0\n",
      "epoch: 2105 / loss: 0.0742349922657013 / accuracy: 1.0\n",
      "epoch: 2106 / loss: 0.07418021559715271 / accuracy: 1.0\n",
      "epoch: 2107 / loss: 0.07412560284137726 / accuracy: 1.0\n",
      "epoch: 2108 / loss: 0.07407118380069733 / accuracy: 1.0\n",
      "epoch: 2109 / loss: 0.07401664555072784 / accuracy: 1.0\n",
      "epoch: 2110 / loss: 0.07396204769611359 / accuracy: 1.0\n",
      "epoch: 2111 / loss: 0.07390786707401276 / accuracy: 1.0\n",
      "epoch: 2112 / loss: 0.07385369390249252 / accuracy: 1.0\n",
      "epoch: 2113 / loss: 0.07379963994026184 / accuracy: 1.0\n",
      "epoch: 2114 / loss: 0.0737454742193222 / accuracy: 1.0\n",
      "epoch: 2115 / loss: 0.0736914798617363 / accuracy: 1.0\n",
      "epoch: 2116 / loss: 0.07363742589950562 / accuracy: 1.0\n",
      "epoch: 2117 / loss: 0.07358356565237045 / accuracy: 1.0\n",
      "epoch: 2118 / loss: 0.07352982461452484 / accuracy: 1.0\n",
      "epoch: 2119 / loss: 0.073476143181324 / accuracy: 1.0\n",
      "epoch: 2120 / loss: 0.07342258095741272 / accuracy: 1.0\n",
      "epoch: 2121 / loss: 0.07336914539337158 / accuracy: 1.0\n",
      "epoch: 2122 / loss: 0.07331552356481552 / accuracy: 1.0\n",
      "epoch: 2123 / loss: 0.07326208800077438 / accuracy: 1.0\n",
      "epoch: 2124 / loss: 0.07320883870124817 / accuracy: 1.0\n",
      "epoch: 2125 / loss: 0.07315540313720703 / accuracy: 1.0\n",
      "epoch: 2126 / loss: 0.07310234010219574 / accuracy: 1.0\n",
      "epoch: 2127 / loss: 0.07304921001195908 / accuracy: 1.0\n",
      "epoch: 2128 / loss: 0.07299619913101196 / accuracy: 1.0\n",
      "epoch: 2129 / loss: 0.07294338196516037 / accuracy: 1.0\n",
      "epoch: 2130 / loss: 0.07289043813943863 / accuracy: 1.0\n",
      "epoch: 2131 / loss: 0.07283755391836166 / accuracy: 1.0\n",
      "epoch: 2132 / loss: 0.07278485596179962 / accuracy: 1.0\n",
      "epoch: 2133 / loss: 0.07273221760988235 / accuracy: 1.0\n",
      "epoch: 2134 / loss: 0.07267957925796509 / accuracy: 1.0\n",
      "epoch: 2135 / loss: 0.07262718677520752 / accuracy: 1.0\n",
      "epoch: 2136 / loss: 0.0725746750831604 / accuracy: 1.0\n",
      "epoch: 2137 / loss: 0.07252222299575806 / accuracy: 1.0\n",
      "epoch: 2138 / loss: 0.07247006893157959 / accuracy: 1.0\n",
      "epoch: 2139 / loss: 0.07241768389940262 / accuracy: 1.0\n",
      "epoch: 2140 / loss: 0.0723656564950943 / accuracy: 1.0\n",
      "epoch: 2141 / loss: 0.07231363654136658 / accuracy: 1.0\n",
      "epoch: 2142 / loss: 0.07226154953241348 / accuracy: 1.0\n",
      "epoch: 2143 / loss: 0.07220959663391113 / accuracy: 1.0\n",
      "epoch: 2144 / loss: 0.07215781509876251 / accuracy: 1.0\n",
      "epoch: 2145 / loss: 0.07210603356361389 / accuracy: 1.0\n",
      "epoch: 2146 / loss: 0.07205431908369064 / accuracy: 1.0\n",
      "epoch: 2147 / loss: 0.07200272381305695 / accuracy: 1.0\n",
      "epoch: 2148 / loss: 0.07195107638835907 / accuracy: 1.0\n",
      "epoch: 2149 / loss: 0.0718996599316597 / accuracy: 1.0\n",
      "epoch: 2150 / loss: 0.07184825837612152 / accuracy: 1.0\n",
      "epoch: 2151 / loss: 0.07179684937000275 / accuracy: 1.0\n",
      "epoch: 2152 / loss: 0.07174555957317352 / accuracy: 1.0\n",
      "epoch: 2153 / loss: 0.07169444859027863 / accuracy: 1.0\n",
      "epoch: 2154 / loss: 0.07164323329925537 / accuracy: 1.0\n",
      "epoch: 2155 / loss: 0.0715920627117157 / accuracy: 1.0\n",
      "epoch: 2156 / loss: 0.07154108583927155 / accuracy: 1.0\n",
      "epoch: 2157 / loss: 0.07149016857147217 / accuracy: 1.0\n",
      "epoch: 2158 / loss: 0.07143925130367279 / accuracy: 1.0\n",
      "epoch: 2159 / loss: 0.07138851284980774 / accuracy: 1.0\n",
      "epoch: 2160 / loss: 0.07133784890174866 / accuracy: 1.0\n",
      "epoch: 2161 / loss: 0.0712871104478836 / accuracy: 1.0\n",
      "epoch: 2162 / loss: 0.07123662531375885 / accuracy: 1.0\n",
      "epoch: 2163 / loss: 0.07118619978427887 / accuracy: 1.0\n",
      "epoch: 2164 / loss: 0.07113570719957352 / accuracy: 1.0\n",
      "epoch: 2165 / loss: 0.07108528167009354 / accuracy: 1.0\n",
      "epoch: 2166 / loss: 0.0710349828004837 / accuracy: 1.0\n",
      "epoch: 2167 / loss: 0.07098467648029327 / accuracy: 1.0\n",
      "epoch: 2168 / loss: 0.07093467563390732 / accuracy: 1.0\n",
      "epoch: 2169 / loss: 0.07088449597358704 / accuracy: 1.0\n",
      "epoch: 2170 / loss: 0.07083437591791153 / accuracy: 1.0\n",
      "epoch: 2171 / loss: 0.07078444212675095 / accuracy: 1.0\n",
      "epoch: 2172 / loss: 0.07073450833559036 / accuracy: 1.0\n",
      "epoch: 2173 / loss: 0.07068482041358948 / accuracy: 1.0\n",
      "epoch: 2174 / loss: 0.07063500583171844 / accuracy: 1.0\n",
      "epoch: 2175 / loss: 0.07058531045913696 / accuracy: 1.0\n",
      "epoch: 2176 / loss: 0.07053567469120026 / accuracy: 1.0\n",
      "epoch: 2177 / loss: 0.07048605382442474 / accuracy: 1.0\n",
      "epoch: 2178 / loss: 0.07043667137622833 / accuracy: 1.0\n",
      "epoch: 2179 / loss: 0.07038722932338715 / accuracy: 1.0\n",
      "epoch: 2180 / loss: 0.07033795863389969 / accuracy: 1.0\n",
      "epoch: 2181 / loss: 0.07028858363628387 / accuracy: 1.0\n",
      "epoch: 2182 / loss: 0.07023938000202179 / accuracy: 1.0\n",
      "epoch: 2183 / loss: 0.07019024342298508 / accuracy: 1.0\n",
      "epoch: 2184 / loss: 0.07014116644859314 / accuracy: 1.0\n",
      "epoch: 2185 / loss: 0.0700920894742012 / accuracy: 1.0\n",
      "epoch: 2186 / loss: 0.07004307955503464 / accuracy: 1.0\n",
      "epoch: 2187 / loss: 0.06999430060386658 / accuracy: 1.0\n",
      "epoch: 2188 / loss: 0.06994547694921494 / accuracy: 1.0\n",
      "epoch: 2189 / loss: 0.06989670544862747 / accuracy: 1.0\n",
      "epoch: 2190 / loss: 0.06984800100326538 / accuracy: 1.0\n",
      "epoch: 2191 / loss: 0.06979928910732269 / accuracy: 1.0\n",
      "epoch: 2192 / loss: 0.0697508230805397 / accuracy: 1.0\n",
      "epoch: 2193 / loss: 0.06970229744911194 / accuracy: 1.0\n",
      "epoch: 2194 / loss: 0.06965401768684387 / accuracy: 1.0\n",
      "epoch: 2195 / loss: 0.06960543245077133 / accuracy: 1.0\n",
      "epoch: 2196 / loss: 0.06955709308385849 / accuracy: 1.0\n",
      "epoch: 2197 / loss: 0.06950905919075012 / accuracy: 1.0\n",
      "epoch: 2198 / loss: 0.06946071982383728 / accuracy: 1.0\n",
      "epoch: 2199 / loss: 0.06941262632608414 / accuracy: 1.0\n",
      "epoch: 2200 / loss: 0.0693647712469101 / accuracy: 1.0\n",
      "epoch: 2201 / loss: 0.06931673735380173 / accuracy: 1.0\n",
      "epoch: 2202 / loss: 0.06926888972520828 / accuracy: 1.0\n",
      "epoch: 2203 / loss: 0.06922103464603424 / accuracy: 1.0\n",
      "epoch: 2204 / loss: 0.0691731870174408 / accuracy: 1.0\n",
      "epoch: 2205 / loss: 0.0691254585981369 / accuracy: 1.0\n",
      "epoch: 2206 / loss: 0.06907785683870316 / accuracy: 1.0\n",
      "epoch: 2207 / loss: 0.06903019547462463 / accuracy: 1.0\n",
      "epoch: 2208 / loss: 0.06898264586925507 / accuracy: 1.0\n",
      "epoch: 2209 / loss: 0.06893523037433624 / accuracy: 1.0\n",
      "epoch: 2210 / loss: 0.06888774782419205 / accuracy: 1.0\n",
      "epoch: 2211 / loss: 0.06884045153856277 / accuracy: 1.0\n",
      "epoch: 2212 / loss: 0.0687931478023529 / accuracy: 1.0\n",
      "epoch: 2213 / loss: 0.06874591112136841 / accuracy: 1.0\n",
      "epoch: 2214 / loss: 0.06869886070489883 / accuracy: 1.0\n",
      "epoch: 2215 / loss: 0.06865174323320389 / accuracy: 1.0\n",
      "epoch: 2216 / loss: 0.06860475242137909 / accuracy: 1.0\n",
      "epoch: 2217 / loss: 0.06855776906013489 / accuracy: 1.0\n",
      "epoch: 2218 / loss: 0.06851089745759964 / accuracy: 1.0\n",
      "epoch: 2219 / loss: 0.06846408545970917 / accuracy: 1.0\n",
      "epoch: 2220 / loss: 0.06841722130775452 / accuracy: 1.0\n",
      "epoch: 2221 / loss: 0.06837065517902374 / accuracy: 1.0\n",
      "epoch: 2222 / loss: 0.06832391023635864 / accuracy: 1.0\n",
      "epoch: 2223 / loss: 0.0682772845029831 / accuracy: 1.0\n",
      "epoch: 2224 / loss: 0.06823084503412247 / accuracy: 1.0\n",
      "epoch: 2225 / loss: 0.06818422675132751 / accuracy: 1.0\n",
      "epoch: 2226 / loss: 0.06813790649175644 / accuracy: 1.0\n",
      "epoch: 2227 / loss: 0.06809165328741074 / accuracy: 1.0\n",
      "epoch: 2228 / loss: 0.06804533302783966 / accuracy: 1.0\n",
      "epoch: 2229 / loss: 0.06799913942813873 / accuracy: 1.0\n",
      "epoch: 2230 / loss: 0.06795300543308258 / accuracy: 1.0\n",
      "epoch: 2231 / loss: 0.06790675222873688 / accuracy: 1.0\n",
      "epoch: 2232 / loss: 0.06786073744297028 / accuracy: 1.0\n",
      "epoch: 2233 / loss: 0.0678146705031395 / accuracy: 1.0\n",
      "epoch: 2234 / loss: 0.06776878237724304 / accuracy: 1.0\n",
      "epoch: 2235 / loss: 0.06772289425134659 / accuracy: 1.0\n",
      "epoch: 2236 / loss: 0.06767712533473969 / accuracy: 1.0\n",
      "epoch: 2237 / loss: 0.06763136386871338 / accuracy: 1.0\n",
      "epoch: 2238 / loss: 0.0675857812166214 / accuracy: 1.0\n",
      "epoch: 2239 / loss: 0.06754013895988464 / accuracy: 1.0\n",
      "epoch: 2240 / loss: 0.06749467551708221 / accuracy: 1.0\n",
      "epoch: 2241 / loss: 0.06744897365570068 / accuracy: 1.0\n",
      "epoch: 2242 / loss: 0.06740370392799377 / accuracy: 1.0\n",
      "epoch: 2243 / loss: 0.06735830008983612 / accuracy: 1.0\n",
      "epoch: 2244 / loss: 0.06731296330690384 / accuracy: 1.0\n",
      "epoch: 2245 / loss: 0.06726769357919693 / accuracy: 1.0\n",
      "epoch: 2246 / loss: 0.06722235679626465 / accuracy: 1.0\n",
      "epoch: 2247 / loss: 0.06717738509178162 / accuracy: 1.0\n",
      "epoch: 2248 / loss: 0.06713223457336426 / accuracy: 1.0\n",
      "epoch: 2249 / loss: 0.06708738207817078 / accuracy: 1.0\n",
      "epoch: 2250 / loss: 0.06704223155975342 / accuracy: 1.0\n",
      "epoch: 2251 / loss: 0.06699726730585098 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2252 / loss: 0.0669524222612381 / accuracy: 1.0\n",
      "epoch: 2253 / loss: 0.06690763682126999 / accuracy: 1.0\n",
      "epoch: 2254 / loss: 0.06686297059059143 / accuracy: 1.0\n",
      "epoch: 2255 / loss: 0.06681831181049347 / accuracy: 1.0\n",
      "epoch: 2256 / loss: 0.06677371263504028 / accuracy: 1.0\n",
      "epoch: 2257 / loss: 0.06672904640436172 / accuracy: 1.0\n",
      "epoch: 2258 / loss: 0.06668456643819809 / accuracy: 1.0\n",
      "epoch: 2259 / loss: 0.06664008647203445 / accuracy: 1.0\n",
      "epoch: 2260 / loss: 0.06659567356109619 / accuracy: 1.0\n",
      "epoch: 2261 / loss: 0.06655137985944748 / accuracy: 1.0\n",
      "epoch: 2262 / loss: 0.06650714576244354 / accuracy: 1.0\n",
      "epoch: 2263 / loss: 0.06646284461021423 / accuracy: 1.0\n",
      "epoch: 2264 / loss: 0.06641867756843567 / accuracy: 1.0\n",
      "epoch: 2265 / loss: 0.06637468934059143 / accuracy: 1.0\n",
      "epoch: 2266 / loss: 0.06633064150810242 / accuracy: 1.0\n",
      "epoch: 2267 / loss: 0.0662865936756134 / accuracy: 1.0\n",
      "epoch: 2268 / loss: 0.06624247878789902 / accuracy: 1.0\n",
      "epoch: 2269 / loss: 0.06619873642921448 / accuracy: 1.0\n",
      "epoch: 2270 / loss: 0.06615486741065979 / accuracy: 1.0\n",
      "epoch: 2271 / loss: 0.06611118465662003 / accuracy: 1.0\n",
      "epoch: 2272 / loss: 0.06606750190258026 / accuracy: 1.0\n",
      "epoch: 2273 / loss: 0.0660238265991211 / accuracy: 1.0\n",
      "epoch: 2274 / loss: 0.06598019599914551 / accuracy: 1.0\n",
      "epoch: 2275 / loss: 0.06593658030033112 / accuracy: 1.0\n",
      "epoch: 2276 / loss: 0.06589320302009583 / accuracy: 1.0\n",
      "epoch: 2277 / loss: 0.06584970653057098 / accuracy: 1.0\n",
      "epoch: 2278 / loss: 0.0658063292503357 / accuracy: 1.0\n",
      "epoch: 2279 / loss: 0.06576307862997055 / accuracy: 1.0\n",
      "epoch: 2280 / loss: 0.06571982055902481 / accuracy: 1.0\n",
      "epoch: 2281 / loss: 0.06567656993865967 / accuracy: 1.0\n",
      "epoch: 2282 / loss: 0.06563331186771393 / accuracy: 1.0\n",
      "epoch: 2283 / loss: 0.06559024751186371 / accuracy: 1.0\n",
      "epoch: 2284 / loss: 0.06554723531007767 / accuracy: 1.0\n",
      "epoch: 2285 / loss: 0.06550417095422745 / accuracy: 1.0\n",
      "epoch: 2286 / loss: 0.06546139717102051 / accuracy: 1.0\n",
      "epoch: 2287 / loss: 0.06541845202445984 / accuracy: 1.0\n",
      "epoch: 2288 / loss: 0.06537550687789917 / accuracy: 1.0\n",
      "epoch: 2289 / loss: 0.06533286720514297 / accuracy: 1.0\n",
      "epoch: 2290 / loss: 0.06529004126787186 / accuracy: 1.0\n",
      "epoch: 2291 / loss: 0.06524740159511566 / accuracy: 1.0\n",
      "epoch: 2292 / loss: 0.06520487368106842 / accuracy: 1.0\n",
      "epoch: 2293 / loss: 0.06516236066818237 / accuracy: 1.0\n",
      "epoch: 2294 / loss: 0.06511972099542618 / accuracy: 1.0\n",
      "epoch: 2295 / loss: 0.06507732719182968 / accuracy: 1.0\n",
      "epoch: 2296 / loss: 0.06503504514694214 / accuracy: 1.0\n",
      "epoch: 2297 / loss: 0.06499253213405609 / accuracy: 1.0\n",
      "epoch: 2298 / loss: 0.06495031714439392 / accuracy: 1.0\n",
      "epoch: 2299 / loss: 0.06490810215473175 / accuracy: 1.0\n",
      "epoch: 2300 / loss: 0.0648658275604248 / accuracy: 1.0\n",
      "epoch: 2301 / loss: 0.064823679625988 / accuracy: 1.0\n",
      "epoch: 2302 / loss: 0.06478171050548553 / accuracy: 1.0\n",
      "epoch: 2303 / loss: 0.06473968178033829 / accuracy: 1.0\n",
      "epoch: 2304 / loss: 0.06469771265983582 / accuracy: 1.0\n",
      "epoch: 2305 / loss: 0.06465569138526917 / accuracy: 1.0\n",
      "epoch: 2306 / loss: 0.0646139606833458 / accuracy: 1.0\n",
      "epoch: 2307 / loss: 0.06457199156284332 / accuracy: 1.0\n",
      "epoch: 2308 / loss: 0.06453033536672592 / accuracy: 1.0\n",
      "epoch: 2309 / loss: 0.06448855251073837 / accuracy: 1.0\n",
      "epoch: 2310 / loss: 0.06444688886404037 / accuracy: 1.0\n",
      "epoch: 2311 / loss: 0.06440529227256775 / accuracy: 1.0\n",
      "epoch: 2312 / loss: 0.06436380743980408 / accuracy: 1.0\n",
      "epoch: 2313 / loss: 0.06432227045297623 / accuracy: 1.0\n",
      "epoch: 2314 / loss: 0.06428085267543793 / accuracy: 1.0\n",
      "epoch: 2315 / loss: 0.06423937529325485 / accuracy: 1.0\n",
      "epoch: 2316 / loss: 0.06419814378023148 / accuracy: 1.0\n",
      "epoch: 2317 / loss: 0.06415678560733795 / accuracy: 1.0\n",
      "epoch: 2318 / loss: 0.06411561369895935 / accuracy: 1.0\n",
      "epoch: 2319 / loss: 0.0640743225812912 / accuracy: 1.0\n",
      "epoch: 2320 / loss: 0.064033143222332 / accuracy: 1.0\n",
      "epoch: 2321 / loss: 0.0639922171831131 / accuracy: 1.0\n",
      "epoch: 2322 / loss: 0.06395110487937927 / accuracy: 1.0\n",
      "epoch: 2323 / loss: 0.06391005963087082 / accuracy: 1.0\n",
      "epoch: 2324 / loss: 0.06386925280094147 / accuracy: 1.0\n",
      "epoch: 2325 / loss: 0.06382820010185242 / accuracy: 1.0\n",
      "epoch: 2326 / loss: 0.06378751993179321 / accuracy: 1.0\n",
      "epoch: 2327 / loss: 0.06374671310186386 / accuracy: 1.0\n",
      "epoch: 2328 / loss: 0.06370602548122406 / accuracy: 1.0\n",
      "epoch: 2329 / loss: 0.06366528570652008 / accuracy: 1.0\n",
      "epoch: 2330 / loss: 0.06362466514110565 / accuracy: 1.0\n",
      "epoch: 2331 / loss: 0.06358404457569122 / accuracy: 1.0\n",
      "epoch: 2332 / loss: 0.06354361027479172 / accuracy: 1.0\n",
      "epoch: 2333 / loss: 0.06350310146808624 / accuracy: 1.0\n",
      "epoch: 2334 / loss: 0.06346254795789719 / accuracy: 1.0\n",
      "epoch: 2335 / loss: 0.06342235207557678 / accuracy: 1.0\n",
      "epoch: 2336 / loss: 0.06338196992874146 / accuracy: 1.0\n",
      "epoch: 2337 / loss: 0.06334160268306732 / accuracy: 1.0\n",
      "epoch: 2338 / loss: 0.06330152601003647 / accuracy: 1.0\n",
      "epoch: 2339 / loss: 0.06326127052307129 / accuracy: 1.0\n",
      "epoch: 2340 / loss: 0.06322114169597626 / accuracy: 1.0\n",
      "epoch: 2341 / loss: 0.06318113207817078 / accuracy: 1.0\n",
      "epoch: 2342 / loss: 0.06314094364643097 / accuracy: 1.0\n",
      "epoch: 2343 / loss: 0.06310093402862549 / accuracy: 1.0\n",
      "epoch: 2344 / loss: 0.06306104362010956 / accuracy: 1.0\n",
      "epoch: 2345 / loss: 0.06302115321159363 / accuracy: 1.0\n",
      "epoch: 2346 / loss: 0.06298144161701202 / accuracy: 1.0\n",
      "epoch: 2347 / loss: 0.06294168531894684 / accuracy: 1.0\n",
      "epoch: 2348 / loss: 0.06290185451507568 / accuracy: 1.0\n",
      "epoch: 2349 / loss: 0.06286215782165527 / accuracy: 1.0\n",
      "epoch: 2350 / loss: 0.06282245367765427 / accuracy: 1.0\n",
      "epoch: 2351 / loss: 0.06278292834758759 / accuracy: 1.0\n",
      "epoch: 2352 / loss: 0.06274335086345673 / accuracy: 1.0\n",
      "epoch: 2353 / loss: 0.06270395219326019 / accuracy: 1.0\n",
      "epoch: 2354 / loss: 0.06266436725854874 / accuracy: 1.0\n",
      "epoch: 2355 / loss: 0.06262490898370743 / accuracy: 1.0\n",
      "epoch: 2356 / loss: 0.0625857561826706 / accuracy: 1.0\n",
      "epoch: 2357 / loss: 0.06254647672176361 / accuracy: 1.0\n",
      "epoch: 2358 / loss: 0.06250713765621185 / accuracy: 1.0\n",
      "epoch: 2359 / loss: 0.06246786192059517 / accuracy: 1.0\n",
      "epoch: 2360 / loss: 0.06242877244949341 / accuracy: 1.0\n",
      "epoch: 2361 / loss: 0.062389738857746124 / accuracy: 1.0\n",
      "epoch: 2362 / loss: 0.06235058605670929 / accuracy: 1.0\n",
      "epoch: 2363 / loss: 0.062311556190252304 / accuracy: 1.0\n",
      "epoch: 2364 / loss: 0.06227252632379532 / accuracy: 1.0\n",
      "epoch: 2365 / loss: 0.06223367527127266 / accuracy: 1.0\n",
      "epoch: 2366 / loss: 0.062194764614105225 / accuracy: 1.0\n",
      "epoch: 2367 / loss: 0.06215598061680794 / accuracy: 1.0\n",
      "epoch: 2368 / loss: 0.062117014080286026 / accuracy: 1.0\n",
      "epoch: 2369 / loss: 0.06207846850156784 / accuracy: 1.0\n",
      "epoch: 2370 / loss: 0.06203986704349518 / accuracy: 1.0\n",
      "epoch: 2371 / loss: 0.06200126186013222 / accuracy: 1.0\n",
      "epoch: 2372 / loss: 0.06196247786283493 / accuracy: 1.0\n",
      "epoch: 2373 / loss: 0.061924055218696594 / accuracy: 1.0\n",
      "epoch: 2374 / loss: 0.06188557296991348 / accuracy: 1.0\n",
      "epoch: 2375 / loss: 0.06184709072113037 / accuracy: 1.0\n",
      "epoch: 2376 / loss: 0.06180855259299278 / accuracy: 1.0\n",
      "epoch: 2377 / loss: 0.061770252883434296 / accuracy: 1.0\n",
      "epoch: 2378 / loss: 0.06173195689916611 / accuracy: 1.0\n",
      "epoch: 2379 / loss: 0.06169365718960762 / accuracy: 1.0\n",
      "epoch: 2380 / loss: 0.06165536493062973 / accuracy: 1.0\n",
      "epoch: 2381 / loss: 0.06161718815565109 / accuracy: 1.0\n",
      "epoch: 2382 / loss: 0.06157901510596275 / accuracy: 1.0\n",
      "epoch: 2383 / loss: 0.06154102087020874 / accuracy: 1.0\n",
      "epoch: 2384 / loss: 0.061502907425165176 / accuracy: 1.0\n",
      "epoch: 2385 / loss: 0.06146491318941116 / accuracy: 1.0\n",
      "epoch: 2386 / loss: 0.061427101492881775 / accuracy: 1.0\n",
      "epoch: 2387 / loss: 0.06138911098241806 / accuracy: 1.0\n",
      "epoch: 2388 / loss: 0.06135118007659912 / accuracy: 1.0\n",
      "epoch: 2389 / loss: 0.061313431710004807 / accuracy: 1.0\n",
      "epoch: 2390 / loss: 0.06127568334341049 / accuracy: 1.0\n",
      "epoch: 2391 / loss: 0.061237938702106476 / accuracy: 1.0\n",
      "epoch: 2392 / loss: 0.061200372874736786 / accuracy: 1.0\n",
      "epoch: 2393 / loss: 0.061162687838077545 / accuracy: 1.0\n",
      "epoch: 2394 / loss: 0.06112506240606308 / accuracy: 1.0\n",
      "epoch: 2395 / loss: 0.06108750030398369 / accuracy: 1.0\n",
      "epoch: 2396 / loss: 0.06105005368590355 / accuracy: 1.0\n",
      "epoch: 2397 / loss: 0.06101255118846893 / accuracy: 1.0\n",
      "epoch: 2398 / loss: 0.06097523495554924 / accuracy: 1.0\n",
      "epoch: 2399 / loss: 0.060937732458114624 / accuracy: 1.0\n",
      "epoch: 2400 / loss: 0.06090047210454941 / accuracy: 1.0\n",
      "epoch: 2401 / loss: 0.060863152146339417 / accuracy: 1.0\n",
      "epoch: 2402 / loss: 0.06082601472735405 / accuracy: 1.0\n",
      "epoch: 2403 / loss: 0.060788754373788834 / accuracy: 1.0\n",
      "epoch: 2404 / loss: 0.06075156107544899 / accuracy: 1.0\n",
      "epoch: 2405 / loss: 0.06071442365646362 / accuracy: 1.0\n",
      "epoch: 2406 / loss: 0.060677409172058105 / accuracy: 1.0\n",
      "epoch: 2407 / loss: 0.06064039468765259 / accuracy: 1.0\n",
      "epoch: 2408 / loss: 0.060603439807891846 / accuracy: 1.0\n",
      "epoch: 2409 / loss: 0.06056654825806618 / accuracy: 1.0\n",
      "epoch: 2410 / loss: 0.06052972003817558 / accuracy: 1.0\n",
      "epoch: 2411 / loss: 0.060492828488349915 / accuracy: 1.0\n",
      "epoch: 2412 / loss: 0.06045611947774887 / accuracy: 1.0\n",
      "epoch: 2413 / loss: 0.060419224202632904 / accuracy: 1.0\n",
      "epoch: 2414 / loss: 0.060382578521966934 / accuracy: 1.0\n",
      "epoch: 2415 / loss: 0.06034599244594574 / accuracy: 1.0\n",
      "epoch: 2416 / loss: 0.06030934303998947 / accuracy: 1.0\n",
      "epoch: 2417 / loss: 0.060272637754678726 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2418 / loss: 0.06023623049259186 / accuracy: 1.0\n",
      "epoch: 2419 / loss: 0.06019970774650574 / accuracy: 1.0\n",
      "epoch: 2420 / loss: 0.06016318127512932 / accuracy: 1.0\n",
      "epoch: 2421 / loss: 0.06012684479355812 / accuracy: 1.0\n",
      "epoch: 2422 / loss: 0.06009061634540558 / accuracy: 1.0\n",
      "epoch: 2423 / loss: 0.06005415692925453 / accuracy: 1.0\n",
      "epoch: 2424 / loss: 0.06001793593168259 / accuracy: 1.0\n",
      "epoch: 2425 / loss: 0.059981655329465866 / accuracy: 1.0\n",
      "epoch: 2426 / loss: 0.059945493936538696 / accuracy: 1.0\n",
      "epoch: 2427 / loss: 0.0599091574549675 / accuracy: 1.0\n",
      "epoch: 2428 / loss: 0.05987323820590973 / accuracy: 1.0\n",
      "epoch: 2429 / loss: 0.05983708053827286 / accuracy: 1.0\n",
      "epoch: 2430 / loss: 0.05980110540986061 / accuracy: 1.0\n",
      "epoch: 2431 / loss: 0.05976507067680359 / accuracy: 1.0\n",
      "epoch: 2432 / loss: 0.05972921848297119 / accuracy: 1.0\n",
      "epoch: 2433 / loss: 0.05969342216849327 / accuracy: 1.0\n",
      "epoch: 2434 / loss: 0.0596575103700161 / accuracy: 1.0\n",
      "epoch: 2435 / loss: 0.059621717780828476 / accuracy: 1.0\n",
      "epoch: 2436 / loss: 0.05958586186170578 / accuracy: 1.0\n",
      "epoch: 2437 / loss: 0.05955019220709801 / accuracy: 1.0\n",
      "epoch: 2438 / loss: 0.05951446294784546 / accuracy: 1.0\n",
      "epoch: 2439 / loss: 0.05947878956794739 / accuracy: 1.0\n",
      "epoch: 2440 / loss: 0.059443242847919464 / accuracy: 1.0\n",
      "epoch: 2441 / loss: 0.05940769240260124 / accuracy: 1.0\n",
      "epoch: 2442 / loss: 0.05937214940786362 / accuracy: 1.0\n",
      "epoch: 2443 / loss: 0.059336721897125244 / accuracy: 1.0\n",
      "epoch: 2444 / loss: 0.05930117145180702 / accuracy: 1.0\n",
      "epoch: 2445 / loss: 0.05926580727100372 / accuracy: 1.0\n",
      "epoch: 2446 / loss: 0.059230439364910126 / accuracy: 1.0\n",
      "epoch: 2447 / loss: 0.059195078909397125 / accuracy: 1.0\n",
      "epoch: 2448 / loss: 0.05915989726781845 / accuracy: 1.0\n",
      "epoch: 2449 / loss: 0.059124656021595 / accuracy: 1.0\n",
      "epoch: 2450 / loss: 0.0590895339846611 / accuracy: 1.0\n",
      "epoch: 2451 / loss: 0.05905423313379288 / accuracy: 1.0\n",
      "epoch: 2452 / loss: 0.059019170701503754 / accuracy: 1.0\n",
      "epoch: 2453 / loss: 0.05898410826921463 / accuracy: 1.0\n",
      "epoch: 2454 / loss: 0.05894911289215088 / accuracy: 1.0\n",
      "epoch: 2455 / loss: 0.058914054185152054 / accuracy: 1.0\n",
      "epoch: 2456 / loss: 0.058879055082798004 / accuracy: 1.0\n",
      "epoch: 2457 / loss: 0.05884423851966858 / accuracy: 1.0\n",
      "epoch: 2458 / loss: 0.0588093027472496 / accuracy: 1.0\n",
      "epoch: 2459 / loss: 0.05877448618412018 / accuracy: 1.0\n",
      "epoch: 2460 / loss: 0.05873973295092583 / accuracy: 1.0\n",
      "epoch: 2461 / loss: 0.058704979717731476 / accuracy: 1.0\n",
      "epoch: 2462 / loss: 0.0586702898144722 / accuracy: 1.0\n",
      "epoch: 2463 / loss: 0.058635592460632324 / accuracy: 1.0\n",
      "epoch: 2464 / loss: 0.05860083922743797 / accuracy: 1.0\n",
      "epoch: 2465 / loss: 0.05856632813811302 / accuracy: 1.0\n",
      "epoch: 2466 / loss: 0.05853182077407837 / accuracy: 1.0\n",
      "epoch: 2467 / loss: 0.05849730595946312 / accuracy: 1.0\n",
      "epoch: 2468 / loss: 0.058462854474782944 / accuracy: 1.0\n",
      "epoch: 2469 / loss: 0.05842834711074829 / accuracy: 1.0\n",
      "epoch: 2470 / loss: 0.058394018560647964 / accuracy: 1.0\n",
      "epoch: 2471 / loss: 0.058359574526548386 / accuracy: 1.0\n",
      "epoch: 2472 / loss: 0.05832524597644806 / accuracy: 1.0\n",
      "epoch: 2473 / loss: 0.05829091742634773 / accuracy: 1.0\n",
      "epoch: 2474 / loss: 0.05825677514076233 / accuracy: 1.0\n",
      "epoch: 2475 / loss: 0.05822256952524185 / accuracy: 1.0\n",
      "epoch: 2476 / loss: 0.05818843096494675 / accuracy: 1.0\n",
      "epoch: 2477 / loss: 0.058154165744781494 / accuracy: 1.0\n",
      "epoch: 2478 / loss: 0.05812026187777519 / accuracy: 1.0\n",
      "epoch: 2479 / loss: 0.05808623880147934 / accuracy: 1.0\n",
      "epoch: 2480 / loss: 0.05805215612053871 / accuracy: 1.0\n",
      "epoch: 2481 / loss: 0.05801837891340256 / accuracy: 1.0\n",
      "epoch: 2482 / loss: 0.05798417702317238 / accuracy: 1.0\n",
      "epoch: 2483 / loss: 0.057950459420681 / accuracy: 1.0\n",
      "epoch: 2484 / loss: 0.0579165555536747 / accuracy: 1.0\n",
      "epoch: 2485 / loss: 0.05788272246718407 / accuracy: 1.0\n",
      "epoch: 2486 / loss: 0.057848937809467316 / accuracy: 1.0\n",
      "epoch: 2487 / loss: 0.05781528353691101 / accuracy: 1.0\n",
      "epoch: 2488 / loss: 0.057781632989645004 / accuracy: 1.0\n",
      "epoch: 2489 / loss: 0.05774785578250885 / accuracy: 1.0\n",
      "epoch: 2490 / loss: 0.05771425738930702 / accuracy: 1.0\n",
      "epoch: 2491 / loss: 0.057680726051330566 / accuracy: 1.0\n",
      "epoch: 2492 / loss: 0.057647012174129486 / accuracy: 1.0\n",
      "epoch: 2493 / loss: 0.05761359632015228 / accuracy: 1.0\n",
      "epoch: 2494 / loss: 0.0575801245868206 / accuracy: 1.0\n",
      "epoch: 2495 / loss: 0.05754677578806877 / accuracy: 1.0\n",
      "epoch: 2496 / loss: 0.05751330032944679 / accuracy: 1.0\n",
      "epoch: 2497 / loss: 0.05748001113533974 / accuracy: 1.0\n",
      "epoch: 2498 / loss: 0.05744660273194313 / accuracy: 1.0\n",
      "epoch: 2499 / loss: 0.057413250207901 / accuracy: 1.0\n",
      "epoch: 2500 / loss: 0.057380083948373795 / accuracy: 1.0\n",
      "epoch: 2501 / loss: 0.057346977293491364 / accuracy: 1.0\n",
      "epoch: 2502 / loss: 0.05731387063860893 / accuracy: 1.0\n",
      "epoch: 2503 / loss: 0.05728064477443695 / accuracy: 1.0\n",
      "epoch: 2504 / loss: 0.05724753811955452 / accuracy: 1.0\n",
      "epoch: 2505 / loss: 0.05721443146467209 / accuracy: 1.0\n",
      "epoch: 2506 / loss: 0.05718150734901428 / accuracy: 1.0\n",
      "epoch: 2507 / loss: 0.0571485236287117 / accuracy: 1.0\n",
      "epoch: 2508 / loss: 0.05711553990840912 / accuracy: 1.0\n",
      "epoch: 2509 / loss: 0.057082679122686386 / accuracy: 1.0\n",
      "epoch: 2510 / loss: 0.057049814611673355 / accuracy: 1.0\n",
      "epoch: 2511 / loss: 0.057016950100660324 / accuracy: 1.0\n",
      "epoch: 2512 / loss: 0.05698409304022789 / accuracy: 1.0\n",
      "epoch: 2513 / loss: 0.05695135146379471 / accuracy: 1.0\n",
      "epoch: 2514 / loss: 0.056918609887361526 / accuracy: 1.0\n",
      "epoch: 2515 / loss: 0.05688605085015297 / accuracy: 1.0\n",
      "epoch: 2516 / loss: 0.056853312999010086 / accuracy: 1.0\n",
      "epoch: 2517 / loss: 0.05682075396180153 / accuracy: 1.0\n",
      "epoch: 2518 / loss: 0.05678807199001312 / accuracy: 1.0\n",
      "epoch: 2519 / loss: 0.05675558000802994 / accuracy: 1.0\n",
      "epoch: 2520 / loss: 0.05672314390540123 / accuracy: 1.0\n",
      "epoch: 2521 / loss: 0.05669058859348297 / accuracy: 1.0\n",
      "epoch: 2522 / loss: 0.056658148765563965 / accuracy: 1.0\n",
      "epoch: 2523 / loss: 0.05662589892745018 / accuracy: 1.0\n",
      "epoch: 2524 / loss: 0.05659346282482147 / accuracy: 1.0\n",
      "epoch: 2525 / loss: 0.05656103044748306 / accuracy: 1.0\n",
      "epoch: 2526 / loss: 0.056528717279434204 / accuracy: 1.0\n",
      "epoch: 2527 / loss: 0.056496646255254745 / accuracy: 1.0\n",
      "epoch: 2528 / loss: 0.05646432936191559 / accuracy: 1.0\n",
      "epoch: 2529 / loss: 0.05643201991915703 / accuracy: 1.0\n",
      "epoch: 2530 / loss: 0.056400008499622345 / accuracy: 1.0\n",
      "epoch: 2531 / loss: 0.05636788159608841 / accuracy: 1.0\n",
      "epoch: 2532 / loss: 0.056335870176553726 / accuracy: 1.0\n",
      "epoch: 2533 / loss: 0.056303802877664566 / accuracy: 1.0\n",
      "epoch: 2534 / loss: 0.05627185478806496 / accuracy: 1.0\n",
      "epoch: 2535 / loss: 0.056239910423755646 / accuracy: 1.0\n",
      "epoch: 2536 / loss: 0.05620790272951126 / accuracy: 1.0\n",
      "epoch: 2537 / loss: 0.05617601424455643 / accuracy: 1.0\n",
      "epoch: 2538 / loss: 0.05614412948489189 / accuracy: 1.0\n",
      "epoch: 2539 / loss: 0.05611218139529228 / accuracy: 1.0\n",
      "epoch: 2540 / loss: 0.05608047917485237 / accuracy: 1.0\n",
      "epoch: 2541 / loss: 0.056048836559057236 / accuracy: 1.0\n",
      "epoch: 2542 / loss: 0.056017011404037476 / accuracy: 1.0\n",
      "epoch: 2543 / loss: 0.055985309183597565 / accuracy: 1.0\n",
      "epoch: 2544 / loss: 0.055953726172447205 / accuracy: 1.0\n",
      "epoch: 2545 / loss: 0.055922023952007294 / accuracy: 1.0\n",
      "epoch: 2546 / loss: 0.05589050427079201 / accuracy: 1.0\n",
      "epoch: 2547 / loss: 0.05585886165499687 / accuracy: 1.0\n",
      "epoch: 2548 / loss: 0.05582740157842636 / accuracy: 1.0\n",
      "epoch: 2549 / loss: 0.055795758962631226 / accuracy: 1.0\n",
      "epoch: 2550 / loss: 0.05576448142528534 / accuracy: 1.0\n",
      "epoch: 2551 / loss: 0.0557330846786499 / accuracy: 1.0\n",
      "epoch: 2552 / loss: 0.055701807141304016 / accuracy: 1.0\n",
      "epoch: 2553 / loss: 0.05567040666937828 / accuracy: 1.0\n",
      "epoch: 2554 / loss: 0.055639006197452545 / accuracy: 1.0\n",
      "epoch: 2555 / loss: 0.05560779199004173 / accuracy: 1.0\n",
      "epoch: 2556 / loss: 0.055576518177986145 / accuracy: 1.0\n",
      "epoch: 2557 / loss: 0.05554530397057533 / accuracy: 1.0\n",
      "epoch: 2558 / loss: 0.05551408603787422 / accuracy: 1.0\n",
      "epoch: 2559 / loss: 0.05548305809497833 / accuracy: 1.0\n",
      "epoch: 2560 / loss: 0.055451899766922 / accuracy: 1.0\n",
      "epoch: 2561 / loss: 0.05542086809873581 / accuracy: 1.0\n",
      "epoch: 2562 / loss: 0.055389776825904846 / accuracy: 1.0\n",
      "epoch: 2563 / loss: 0.055358804762363434 / accuracy: 1.0\n",
      "epoch: 2564 / loss: 0.05532783269882202 / accuracy: 1.0\n",
      "epoch: 2565 / loss: 0.055296797305345535 / accuracy: 1.0\n",
      "epoch: 2566 / loss: 0.05526607111096382 / accuracy: 1.0\n",
      "epoch: 2567 / loss: 0.05523509904742241 / accuracy: 1.0\n",
      "epoch: 2568 / loss: 0.0552043691277504 / accuracy: 1.0\n",
      "epoch: 2569 / loss: 0.05517345666885376 / accuracy: 1.0\n",
      "epoch: 2570 / loss: 0.055142614990472794 / accuracy: 1.0\n",
      "epoch: 2571 / loss: 0.055111948400735855 / accuracy: 1.0\n",
      "epoch: 2572 / loss: 0.05508139729499817 / accuracy: 1.0\n",
      "epoch: 2573 / loss: 0.05505073070526123 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2574 / loss: 0.05502000451087952 / accuracy: 1.0\n",
      "epoch: 2575 / loss: 0.05498940125107765 / accuracy: 1.0\n",
      "epoch: 2576 / loss: 0.05495879799127579 / accuracy: 1.0\n",
      "epoch: 2577 / loss: 0.054928187280893326 / accuracy: 1.0\n",
      "epoch: 2578 / loss: 0.05489776283502579 / accuracy: 1.0\n",
      "epoch: 2579 / loss: 0.05486734211444855 / accuracy: 1.0\n",
      "epoch: 2580 / loss: 0.054836735129356384 / accuracy: 1.0\n",
      "epoch: 2581 / loss: 0.05480649322271347 / accuracy: 1.0\n",
      "epoch: 2582 / loss: 0.054776132106781006 / accuracy: 1.0\n",
      "epoch: 2583 / loss: 0.054745711386203766 / accuracy: 1.0\n",
      "epoch: 2584 / loss: 0.05471552908420563 / accuracy: 1.0\n",
      "epoch: 2585 / loss: 0.05468523129820824 / accuracy: 1.0\n",
      "epoch: 2586 / loss: 0.05465498939156532 / accuracy: 1.0\n",
      "epoch: 2587 / loss: 0.05462481081485748 / accuracy: 1.0\n",
      "epoch: 2588 / loss: 0.05459444969892502 / accuracy: 1.0\n",
      "epoch: 2589 / loss: 0.05456439033150673 / accuracy: 1.0\n",
      "epoch: 2590 / loss: 0.05453433096408844 / accuracy: 1.0\n",
      "epoch: 2591 / loss: 0.0545041561126709 / accuracy: 1.0\n",
      "epoch: 2592 / loss: 0.05447421967983246 / accuracy: 1.0\n",
      "epoch: 2593 / loss: 0.05444404110312462 / accuracy: 1.0\n",
      "epoch: 2594 / loss: 0.054414164274930954 / accuracy: 1.0\n",
      "epoch: 2595 / loss: 0.05438423156738281 / accuracy: 1.0\n",
      "epoch: 2596 / loss: 0.05435435473918915 / accuracy: 1.0\n",
      "epoch: 2597 / loss: 0.05432448163628578 / accuracy: 1.0\n",
      "epoch: 2598 / loss: 0.054294485598802567 / accuracy: 1.0\n",
      "epoch: 2599 / loss: 0.0542648583650589 / accuracy: 1.0\n",
      "epoch: 2600 / loss: 0.05423492193222046 / accuracy: 1.0\n",
      "epoch: 2601 / loss: 0.054205164313316345 / accuracy: 1.0\n",
      "epoch: 2602 / loss: 0.05417535454034805 / accuracy: 1.0\n",
      "epoch: 2603 / loss: 0.05414590239524841 / accuracy: 1.0\n",
      "epoch: 2604 / loss: 0.054116152226924896 / accuracy: 1.0\n",
      "epoch: 2605 / loss: 0.05408652126789093 / accuracy: 1.0\n",
      "epoch: 2606 / loss: 0.05405683070421219 / accuracy: 1.0\n",
      "epoch: 2607 / loss: 0.0540275052189827 / accuracy: 1.0\n",
      "epoch: 2608 / loss: 0.05399775505065918 / accuracy: 1.0\n",
      "epoch: 2609 / loss: 0.053968243300914764 / accuracy: 1.0\n",
      "epoch: 2610 / loss: 0.05393873527646065 / accuracy: 1.0\n",
      "epoch: 2611 / loss: 0.05390940606594086 / accuracy: 1.0\n",
      "epoch: 2612 / loss: 0.053880080580711365 / accuracy: 1.0\n",
      "epoch: 2613 / loss: 0.05385057255625725 / accuracy: 1.0\n",
      "epoch: 2614 / loss: 0.05382118374109268 / accuracy: 1.0\n",
      "epoch: 2615 / loss: 0.053791921585798264 / accuracy: 1.0\n",
      "epoch: 2616 / loss: 0.05376271530985832 / accuracy: 1.0\n",
      "epoch: 2617 / loss: 0.05373338982462883 / accuracy: 1.0\n",
      "epoch: 2618 / loss: 0.05370424687862396 / accuracy: 1.0\n",
      "epoch: 2619 / loss: 0.053674980998039246 / accuracy: 1.0\n",
      "epoch: 2620 / loss: 0.053645774722099304 / accuracy: 1.0\n",
      "epoch: 2621 / loss: 0.05361669510602951 / accuracy: 1.0\n",
      "epoch: 2622 / loss: 0.05358760803937912 / accuracy: 1.0\n",
      "epoch: 2623 / loss: 0.05355846881866455 / accuracy: 1.0\n",
      "epoch: 2624 / loss: 0.05352956801652908 / accuracy: 1.0\n",
      "epoch: 2625 / loss: 0.05350048840045929 / accuracy: 1.0\n",
      "epoch: 2626 / loss: 0.05347152799367905 / accuracy: 1.0\n",
      "epoch: 2627 / loss: 0.053442686796188354 / accuracy: 1.0\n",
      "epoch: 2628 / loss: 0.05341372638940811 / accuracy: 1.0\n",
      "epoch: 2629 / loss: 0.05338470637798309 / accuracy: 1.0\n",
      "epoch: 2630 / loss: 0.053355924785137177 / accuracy: 1.0\n",
      "epoch: 2631 / loss: 0.05332721024751663 / accuracy: 1.0\n",
      "epoch: 2632 / loss: 0.053298432379961014 / accuracy: 1.0\n",
      "epoch: 2633 / loss: 0.053269535303115845 / accuracy: 1.0\n",
      "epoch: 2634 / loss: 0.053240880370140076 / accuracy: 1.0\n",
      "epoch: 2635 / loss: 0.05321216210722923 / accuracy: 1.0\n",
      "epoch: 2636 / loss: 0.053183503448963165 / accuracy: 1.0\n",
      "epoch: 2637 / loss: 0.05315478891134262 / accuracy: 1.0\n",
      "epoch: 2638 / loss: 0.05312613397836685 / accuracy: 1.0\n",
      "epoch: 2639 / loss: 0.05309765785932541 / accuracy: 1.0\n",
      "epoch: 2640 / loss: 0.05306900665163994 / accuracy: 1.0\n",
      "epoch: 2641 / loss: 0.0530405268073082 / accuracy: 1.0\n",
      "epoch: 2642 / loss: 0.053012117743492126 / accuracy: 1.0\n",
      "epoch: 2643 / loss: 0.05298358201980591 / accuracy: 1.0\n",
      "epoch: 2644 / loss: 0.05295516923069954 / accuracy: 1.0\n",
      "epoch: 2645 / loss: 0.05292663723230362 / accuracy: 1.0\n",
      "epoch: 2646 / loss: 0.0528983436524868 / accuracy: 1.0\n",
      "epoch: 2647 / loss: 0.05286999046802521 / accuracy: 1.0\n",
      "epoch: 2648 / loss: 0.05284164100885391 / accuracy: 1.0\n",
      "epoch: 2649 / loss: 0.05281341075897217 / accuracy: 1.0\n",
      "epoch: 2650 / loss: 0.05278506129980087 / accuracy: 1.0\n",
      "epoch: 2651 / loss: 0.05275695398449898 / accuracy: 1.0\n",
      "epoch: 2652 / loss: 0.05272866040468216 / accuracy: 1.0\n",
      "epoch: 2653 / loss: 0.05270049348473549 / accuracy: 1.0\n",
      "epoch: 2654 / loss: 0.05267232656478882 / accuracy: 1.0\n",
      "epoch: 2655 / loss: 0.0526442788541317 / accuracy: 1.0\n",
      "epoch: 2656 / loss: 0.05261611193418503 / accuracy: 1.0\n",
      "epoch: 2657 / loss: 0.05258818715810776 / accuracy: 1.0\n",
      "epoch: 2658 / loss: 0.052560076117515564 / accuracy: 1.0\n",
      "epoch: 2659 / loss: 0.05253221094608307 / accuracy: 1.0\n",
      "epoch: 2660 / loss: 0.05250416696071625 / accuracy: 1.0\n",
      "epoch: 2661 / loss: 0.05247623845934868 / accuracy: 1.0\n",
      "epoch: 2662 / loss: 0.05244837701320648 / accuracy: 1.0\n",
      "epoch: 2663 / loss: 0.052420392632484436 / accuracy: 1.0\n",
      "epoch: 2664 / loss: 0.05239264667034149 / accuracy: 1.0\n",
      "epoch: 2665 / loss: 0.052364904433488846 / accuracy: 1.0\n",
      "epoch: 2666 / loss: 0.0523369163274765 / accuracy: 1.0\n",
      "epoch: 2667 / loss: 0.05230923742055893 / accuracy: 1.0\n",
      "epoch: 2668 / loss: 0.05228155851364136 / accuracy: 1.0\n",
      "epoch: 2669 / loss: 0.05225381255149841 / accuracy: 1.0\n",
      "epoch: 2670 / loss: 0.05222607031464577 / accuracy: 1.0\n",
      "epoch: 2671 / loss: 0.052198510617017746 / accuracy: 1.0\n",
      "epoch: 2672 / loss: 0.0521707683801651 / accuracy: 1.0\n",
      "epoch: 2673 / loss: 0.052143268287181854 / accuracy: 1.0\n",
      "epoch: 2674 / loss: 0.05211570858955383 / accuracy: 1.0\n",
      "epoch: 2675 / loss: 0.05208814889192581 / accuracy: 1.0\n",
      "epoch: 2676 / loss: 0.05206058919429779 / accuracy: 1.0\n",
      "epoch: 2677 / loss: 0.052033212035894394 / accuracy: 1.0\n",
      "epoch: 2678 / loss: 0.05200577527284622 / accuracy: 1.0\n",
      "epoch: 2679 / loss: 0.05197839438915253 / accuracy: 1.0\n",
      "epoch: 2680 / loss: 0.05195101723074913 / accuracy: 1.0\n",
      "epoch: 2681 / loss: 0.051923640072345734 / accuracy: 1.0\n",
      "epoch: 2682 / loss: 0.05189626291394234 / accuracy: 1.0\n",
      "epoch: 2683 / loss: 0.05186895281076431 / accuracy: 1.0\n",
      "epoch: 2684 / loss: 0.05184163153171539 / accuracy: 1.0\n",
      "epoch: 2685 / loss: 0.05181443691253662 / accuracy: 1.0\n",
      "epoch: 2686 / loss: 0.051787182688713074 / accuracy: 1.0\n",
      "epoch: 2687 / loss: 0.051759928464889526 / accuracy: 1.0\n",
      "epoch: 2688 / loss: 0.05173279345035553 / accuracy: 1.0\n",
      "epoch: 2689 / loss: 0.05170571804046631 / accuracy: 1.0\n",
      "epoch: 2690 / loss: 0.05167859047651291 / accuracy: 1.0\n",
      "epoch: 2691 / loss: 0.05165151506662369 / accuracy: 1.0\n",
      "epoch: 2692 / loss: 0.051624443382024765 / accuracy: 1.0\n",
      "epoch: 2693 / loss: 0.051597367972135544 / accuracy: 1.0\n",
      "epoch: 2694 / loss: 0.05157041922211647 / accuracy: 1.0\n",
      "epoch: 2695 / loss: 0.0515434667468071 / accuracy: 1.0\n",
      "epoch: 2696 / loss: 0.05151645466685295 / accuracy: 1.0\n",
      "epoch: 2697 / loss: 0.05148950219154358 / accuracy: 1.0\n",
      "epoch: 2698 / loss: 0.05146261304616928 / accuracy: 1.0\n",
      "epoch: 2699 / loss: 0.05143572390079498 / accuracy: 1.0\n",
      "epoch: 2700 / loss: 0.051408953964710236 / accuracy: 1.0\n",
      "epoch: 2701 / loss: 0.05138206481933594 / accuracy: 1.0\n",
      "epoch: 2702 / loss: 0.05135529488325119 / accuracy: 1.0\n",
      "epoch: 2703 / loss: 0.05132864788174629 / accuracy: 1.0\n",
      "epoch: 2704 / loss: 0.05130188167095184 / accuracy: 1.0\n",
      "epoch: 2705 / loss: 0.05127505213022232 / accuracy: 1.0\n",
      "epoch: 2706 / loss: 0.0512484647333622 / accuracy: 1.0\n",
      "epoch: 2707 / loss: 0.051221758127212524 / accuracy: 1.0\n",
      "epoch: 2708 / loss: 0.05119523033499718 / accuracy: 1.0\n",
      "epoch: 2709 / loss: 0.051168523728847504 / accuracy: 1.0\n",
      "epoch: 2710 / loss: 0.051142122596502304 / accuracy: 1.0\n",
      "epoch: 2711 / loss: 0.05111553519964218 / accuracy: 1.0\n",
      "epoch: 2712 / loss: 0.05108901113271713 / accuracy: 1.0\n",
      "epoch: 2713 / loss: 0.05106242746114731 / accuracy: 1.0\n",
      "epoch: 2714 / loss: 0.05103602260351181 / accuracy: 1.0\n",
      "epoch: 2715 / loss: 0.05100955441594124 / accuracy: 1.0\n",
      "epoch: 2716 / loss: 0.05098327249288559 / accuracy: 1.0\n",
      "epoch: 2717 / loss: 0.050956808030605316 / accuracy: 1.0\n",
      "epoch: 2718 / loss: 0.05093047022819519 / accuracy: 1.0\n",
      "epoch: 2719 / loss: 0.050904128700494766 / accuracy: 1.0\n",
      "epoch: 2720 / loss: 0.05087784677743912 / accuracy: 1.0\n",
      "epoch: 2721 / loss: 0.050851620733737946 / accuracy: 1.0\n",
      "epoch: 2722 / loss: 0.050825219601392746 / accuracy: 1.0\n",
      "epoch: 2723 / loss: 0.050799060612916946 / accuracy: 1.0\n",
      "epoch: 2724 / loss: 0.05077283829450607 / accuracy: 1.0\n",
      "epoch: 2725 / loss: 0.05074667930603027 / accuracy: 1.0\n",
      "epoch: 2726 / loss: 0.05072057992219925 / accuracy: 1.0\n",
      "epoch: 2727 / loss: 0.050694540143013 / accuracy: 1.0\n",
      "epoch: 2728 / loss: 0.0506683811545372 / accuracy: 1.0\n",
      "epoch: 2729 / loss: 0.05064234137535095 / accuracy: 1.0\n",
      "epoch: 2730 / loss: 0.05061624199151993 / accuracy: 1.0\n",
      "epoch: 2731 / loss: 0.05059032887220383 / accuracy: 1.0\n",
      "epoch: 2732 / loss: 0.0505642294883728 / accuracy: 1.0\n",
      "epoch: 2733 / loss: 0.05053824931383133 / accuracy: 1.0\n",
      "epoch: 2734 / loss: 0.050512395799160004 / accuracy: 1.0\n",
      "epoch: 2735 / loss: 0.050486478954553604 / accuracy: 1.0\n",
      "epoch: 2736 / loss: 0.050460562109947205 / accuracy: 1.0\n",
      "epoch: 2737 / loss: 0.05043470859527588 / accuracy: 1.0\n",
      "epoch: 2738 / loss: 0.05040879175066948 / accuracy: 1.0\n",
      "epoch: 2739 / loss: 0.05038299784064293 / accuracy: 1.0\n",
      "epoch: 2740 / loss: 0.050357263535261154 / accuracy: 1.0\n",
      "epoch: 2741 / loss: 0.05033152922987938 / accuracy: 1.0\n",
      "epoch: 2742 / loss: 0.050305794924497604 / accuracy: 1.0\n",
      "epoch: 2743 / loss: 0.05028018355369568 / accuracy: 1.0\n",
      "epoch: 2744 / loss: 0.050254449248313904 / accuracy: 1.0\n",
      "epoch: 2745 / loss: 0.05022871494293213 / accuracy: 1.0\n",
      "epoch: 2746 / loss: 0.05020315945148468 / accuracy: 1.0\n",
      "epoch: 2747 / loss: 0.05017761141061783 / accuracy: 1.0\n",
      "epoch: 2748 / loss: 0.050152115523815155 / accuracy: 1.0\n",
      "epoch: 2749 / loss: 0.05012644827365875 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2750 / loss: 0.05010095238685608 / accuracy: 1.0\n",
      "epoch: 2751 / loss: 0.050075285136699677 / accuracy: 1.0\n",
      "epoch: 2752 / loss: 0.05004997178912163 / accuracy: 1.0\n",
      "epoch: 2753 / loss: 0.05002454295754433 / accuracy: 1.0\n",
      "epoch: 2754 / loss: 0.049999114125967026 / accuracy: 1.0\n",
      "epoch: 2755 / loss: 0.049973685294389725 / accuracy: 1.0\n",
      "epoch: 2756 / loss: 0.049948375672101974 / accuracy: 1.0\n",
      "epoch: 2757 / loss: 0.04992300644516945 / accuracy: 1.0\n",
      "epoch: 2758 / loss: 0.04989763721823692 / accuracy: 1.0\n",
      "epoch: 2759 / loss: 0.04987239092588425 / accuracy: 1.0\n",
      "epoch: 2760 / loss: 0.0498470813035965 / accuracy: 1.0\n",
      "epoch: 2761 / loss: 0.049821894615888596 / accuracy: 1.0\n",
      "epoch: 2762 / loss: 0.049796707928180695 / accuracy: 1.0\n",
      "epoch: 2763 / loss: 0.04977146163582802 / accuracy: 1.0\n",
      "epoch: 2764 / loss: 0.04974615201354027 / accuracy: 1.0\n",
      "epoch: 2765 / loss: 0.049721088260412216 / accuracy: 1.0\n",
      "epoch: 2766 / loss: 0.04969596117734909 / accuracy: 1.0\n",
      "epoch: 2767 / loss: 0.04967083781957626 / accuracy: 1.0\n",
      "epoch: 2768 / loss: 0.04964583367109299 / accuracy: 1.0\n",
      "epoch: 2769 / loss: 0.049620650708675385 / accuracy: 1.0\n",
      "epoch: 2770 / loss: 0.049595702439546585 / accuracy: 1.0\n",
      "epoch: 2771 / loss: 0.04957070201635361 / accuracy: 1.0\n",
      "epoch: 2772 / loss: 0.04954575374722481 / accuracy: 1.0\n",
      "epoch: 2773 / loss: 0.04952075332403183 / accuracy: 1.0\n",
      "epoch: 2774 / loss: 0.04949592798948288 / accuracy: 1.0\n",
      "epoch: 2775 / loss: 0.04947098717093468 / accuracy: 1.0\n",
      "epoch: 2776 / loss: 0.0494462251663208 / accuracy: 1.0\n",
      "epoch: 2777 / loss: 0.0494212806224823 / accuracy: 1.0\n",
      "epoch: 2778 / loss: 0.04939646273851395 / accuracy: 1.0\n",
      "epoch: 2779 / loss: 0.049371637403964996 / accuracy: 1.0\n",
      "epoch: 2780 / loss: 0.049346815794706345 / accuracy: 1.0\n",
      "epoch: 2781 / loss: 0.04932205379009247 / accuracy: 1.0\n",
      "epoch: 2782 / loss: 0.049297355115413666 / accuracy: 1.0\n",
      "epoch: 2783 / loss: 0.049272533506155014 / accuracy: 1.0\n",
      "epoch: 2784 / loss: 0.049247898161411285 / accuracy: 1.0\n",
      "epoch: 2785 / loss: 0.049223314970731735 / accuracy: 1.0\n",
      "epoch: 2786 / loss: 0.04919867962598801 / accuracy: 1.0\n",
      "epoch: 2787 / loss: 0.04917403683066368 / accuracy: 1.0\n",
      "epoch: 2788 / loss: 0.04914945736527443 / accuracy: 1.0\n",
      "epoch: 2789 / loss: 0.0491250604391098 / accuracy: 1.0\n",
      "epoch: 2790 / loss: 0.049100425094366074 / accuracy: 1.0\n",
      "epoch: 2791 / loss: 0.04907596483826637 / accuracy: 1.0\n",
      "epoch: 2792 / loss: 0.04905138909816742 / accuracy: 1.0\n",
      "epoch: 2793 / loss: 0.04902692884206772 / accuracy: 1.0\n",
      "epoch: 2794 / loss: 0.04900241643190384 / accuracy: 1.0\n",
      "epoch: 2795 / loss: 0.04897813871502876 / accuracy: 1.0\n",
      "epoch: 2796 / loss: 0.048953741788864136 / accuracy: 1.0\n",
      "epoch: 2797 / loss: 0.048929229378700256 / accuracy: 1.0\n",
      "epoch: 2798 / loss: 0.04890495166182518 / accuracy: 1.0\n",
      "epoch: 2799 / loss: 0.04888061434030533 / accuracy: 1.0\n",
      "epoch: 2800 / loss: 0.04885634034872055 / accuracy: 1.0\n",
      "epoch: 2801 / loss: 0.048832006752491 / accuracy: 1.0\n",
      "epoch: 2802 / loss: 0.04880785197019577 / accuracy: 1.0\n",
      "epoch: 2803 / loss: 0.04878363758325577 / accuracy: 1.0\n",
      "epoch: 2804 / loss: 0.04875936359167099 / accuracy: 1.0\n",
      "epoch: 2805 / loss: 0.04873520880937576 / accuracy: 1.0\n",
      "epoch: 2806 / loss: 0.048711057752370834 / accuracy: 1.0\n",
      "epoch: 2807 / loss: 0.048686787486076355 / accuracy: 1.0\n",
      "epoch: 2808 / loss: 0.0486626960337162 / accuracy: 1.0\n",
      "epoch: 2809 / loss: 0.048638660460710526 / accuracy: 1.0\n",
      "epoch: 2810 / loss: 0.04861456900835037 / accuracy: 1.0\n",
      "epoch: 2811 / loss: 0.04859047755599022 / accuracy: 1.0\n",
      "epoch: 2812 / loss: 0.048566509038209915 / accuracy: 1.0\n",
      "epoch: 2813 / loss: 0.04854241758584976 / accuracy: 1.0\n",
      "epoch: 2814 / loss: 0.04851856827735901 / accuracy: 1.0\n",
      "epoch: 2815 / loss: 0.04849459230899811 / accuracy: 1.0\n",
      "epoch: 2816 / loss: 0.04847068712115288 / accuracy: 1.0\n",
      "epoch: 2817 / loss: 0.0484466552734375 / accuracy: 1.0\n",
      "epoch: 2818 / loss: 0.0484229251742363 / accuracy: 1.0\n",
      "epoch: 2819 / loss: 0.04839901626110077 / accuracy: 1.0\n",
      "epoch: 2820 / loss: 0.04837510734796524 / accuracy: 1.0\n",
      "epoch: 2821 / loss: 0.04835137724876404 / accuracy: 1.0\n",
      "epoch: 2822 / loss: 0.04832759127020836 / accuracy: 1.0\n",
      "epoch: 2823 / loss: 0.048303864896297455 / accuracy: 1.0\n",
      "epoch: 2824 / loss: 0.04828007519245148 / accuracy: 1.0\n",
      "epoch: 2825 / loss: 0.04825641214847565 / accuracy: 1.0\n",
      "epoch: 2826 / loss: 0.04823274165391922 / accuracy: 1.0\n",
      "epoch: 2827 / loss: 0.04820895567536354 / accuracy: 1.0\n",
      "epoch: 2828 / loss: 0.04818534851074219 / accuracy: 1.0\n",
      "epoch: 2829 / loss: 0.048161622136831284 / accuracy: 1.0\n",
      "epoch: 2830 / loss: 0.04813801869750023 / accuracy: 1.0\n",
      "epoch: 2831 / loss: 0.048114292323589325 / accuracy: 1.0\n",
      "epoch: 2832 / loss: 0.04809080809354782 / accuracy: 1.0\n",
      "epoch: 2833 / loss: 0.04806738346815109 / accuracy: 1.0\n",
      "epoch: 2834 / loss: 0.048043832182884216 / accuracy: 1.0\n",
      "epoch: 2835 / loss: 0.048020415008068085 / accuracy: 1.0\n",
      "epoch: 2836 / loss: 0.04799693077802658 / accuracy: 1.0\n",
      "epoch: 2837 / loss: 0.04797350615262985 / accuracy: 1.0\n",
      "epoch: 2838 / loss: 0.04795002192258835 / accuracy: 1.0\n",
      "epoch: 2839 / loss: 0.04792659729719162 / accuracy: 1.0\n",
      "epoch: 2840 / loss: 0.04790323227643967 / accuracy: 1.0\n",
      "epoch: 2841 / loss: 0.04787987470626831 / accuracy: 1.0\n",
      "epoch: 2842 / loss: 0.04785650968551636 / accuracy: 1.0\n",
      "epoch: 2843 / loss: 0.04783327132463455 / accuracy: 1.0\n",
      "epoch: 2844 / loss: 0.047809846699237823 / accuracy: 1.0\n",
      "epoch: 2845 / loss: 0.04778648167848587 / accuracy: 1.0\n",
      "epoch: 2846 / loss: 0.047763362526893616 / accuracy: 1.0\n",
      "epoch: 2847 / loss: 0.047740064561367035 / accuracy: 1.0\n",
      "epoch: 2848 / loss: 0.04771694168448448 / accuracy: 1.0\n",
      "epoch: 2849 / loss: 0.04769375920295715 / accuracy: 1.0\n",
      "epoch: 2850 / loss: 0.04767052084207535 / accuracy: 1.0\n",
      "epoch: 2851 / loss: 0.04764746129512787 / accuracy: 1.0\n",
      "epoch: 2852 / loss: 0.04762428253889084 / accuracy: 1.0\n",
      "epoch: 2853 / loss: 0.047601163387298584 / accuracy: 1.0\n",
      "epoch: 2854 / loss: 0.04757816344499588 / accuracy: 1.0\n",
      "epoch: 2855 / loss: 0.047555044293403625 / accuracy: 1.0\n",
      "epoch: 2856 / loss: 0.04753204435110092 / accuracy: 1.0\n",
      "epoch: 2857 / loss: 0.04750892519950867 / accuracy: 1.0\n",
      "epoch: 2858 / loss: 0.04748617112636566 / accuracy: 1.0\n",
      "epoch: 2859 / loss: 0.04746311157941818 / accuracy: 1.0\n",
      "epoch: 2860 / loss: 0.047440238296985626 / accuracy: 1.0\n",
      "epoch: 2861 / loss: 0.04741717875003815 / accuracy: 1.0\n",
      "epoch: 2862 / loss: 0.04739435762166977 / accuracy: 1.0\n",
      "epoch: 2863 / loss: 0.04737136512994766 / accuracy: 1.0\n",
      "epoch: 2864 / loss: 0.04734867066144943 / accuracy: 1.0\n",
      "epoch: 2865 / loss: 0.0473257340490818 / accuracy: 1.0\n",
      "epoch: 2866 / loss: 0.04730292037129402 / accuracy: 1.0\n",
      "epoch: 2867 / loss: 0.04728016257286072 / accuracy: 1.0\n",
      "epoch: 2868 / loss: 0.04725734516978264 / accuracy: 1.0\n",
      "epoch: 2869 / loss: 0.04723447188735008 / accuracy: 1.0\n",
      "epoch: 2870 / loss: 0.047211892902851105 / accuracy: 1.0\n",
      "epoch: 2871 / loss: 0.04718920588493347 / accuracy: 1.0\n",
      "epoch: 2872 / loss: 0.04716638848185539 / accuracy: 1.0\n",
      "epoch: 2873 / loss: 0.04714375361800194 / accuracy: 1.0\n",
      "epoch: 2874 / loss: 0.04712118208408356 / accuracy: 1.0\n",
      "epoch: 2875 / loss: 0.04709848761558533 / accuracy: 1.0\n",
      "epoch: 2876 / loss: 0.04707591235637665 / accuracy: 1.0\n",
      "epoch: 2877 / loss: 0.04705340042710304 / accuracy: 1.0\n",
      "epoch: 2878 / loss: 0.04703082889318466 / accuracy: 1.0\n",
      "epoch: 2879 / loss: 0.04700825363397598 / accuracy: 1.0\n",
      "epoch: 2880 / loss: 0.046985745429992676 / accuracy: 1.0\n",
      "epoch: 2881 / loss: 0.04696335643529892 / accuracy: 1.0\n",
      "epoch: 2882 / loss: 0.046940840780735016 / accuracy: 1.0\n",
      "epoch: 2883 / loss: 0.04691826552152634 / accuracy: 1.0\n",
      "epoch: 2884 / loss: 0.04689599946141243 / accuracy: 1.0\n",
      "epoch: 2885 / loss: 0.046873487532138824 / accuracy: 1.0\n",
      "epoch: 2886 / loss: 0.04685109481215477 / accuracy: 1.0\n",
      "epoch: 2887 / loss: 0.04682864993810654 / accuracy: 1.0\n",
      "epoch: 2888 / loss: 0.04680643975734711 / accuracy: 1.0\n",
      "epoch: 2889 / loss: 0.046784043312072754 / accuracy: 1.0\n",
      "epoch: 2890 / loss: 0.046761658042669296 / accuracy: 1.0\n",
      "epoch: 2891 / loss: 0.04673951119184494 / accuracy: 1.0\n",
      "epoch: 2892 / loss: 0.04671718180179596 / accuracy: 1.0\n",
      "epoch: 2893 / loss: 0.04669485241174698 / accuracy: 1.0\n",
      "epoch: 2894 / loss: 0.04667264223098755 / accuracy: 1.0\n",
      "epoch: 2895 / loss: 0.04665055125951767 / accuracy: 1.0\n",
      "epoch: 2896 / loss: 0.04662834480404854 / accuracy: 1.0\n",
      "epoch: 2897 / loss: 0.04660625755786896 / accuracy: 1.0\n",
      "epoch: 2898 / loss: 0.04658398777246475 / accuracy: 1.0\n",
      "epoch: 2899 / loss: 0.04656190425157547 / accuracy: 1.0\n",
      "epoch: 2900 / loss: 0.046539876610040665 / accuracy: 1.0\n",
      "epoch: 2901 / loss: 0.04651772975921631 / accuracy: 1.0\n",
      "epoch: 2902 / loss: 0.04649558290839195 / accuracy: 1.0\n",
      "epoch: 2903 / loss: 0.04647355526685715 / accuracy: 1.0\n",
      "epoch: 2904 / loss: 0.04645153135061264 / accuracy: 1.0\n",
      "epoch: 2905 / loss: 0.046429503709077835 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2906 / loss: 0.04640759900212288 / accuracy: 1.0\n",
      "epoch: 2907 / loss: 0.04638563469052315 / accuracy: 1.0\n",
      "epoch: 2908 / loss: 0.046363793313503265 / accuracy: 1.0\n",
      "epoch: 2909 / loss: 0.046341702342033386 / accuracy: 1.0\n",
      "epoch: 2910 / loss: 0.04631980136036873 / accuracy: 1.0\n",
      "epoch: 2911 / loss: 0.04629801958799362 / accuracy: 1.0\n",
      "epoch: 2912 / loss: 0.04627611115574837 / accuracy: 1.0\n",
      "epoch: 2913 / loss: 0.046254269778728485 / accuracy: 1.0\n",
      "epoch: 2914 / loss: 0.04623248428106308 / accuracy: 1.0\n",
      "epoch: 2915 / loss: 0.0462106391787529 / accuracy: 1.0\n",
      "epoch: 2916 / loss: 0.046188920736312866 / accuracy: 1.0\n",
      "epoch: 2917 / loss: 0.04616713523864746 / accuracy: 1.0\n",
      "epoch: 2918 / loss: 0.0461454764008522 / accuracy: 1.0\n",
      "epoch: 2919 / loss: 0.04612363129854202 / accuracy: 1.0\n",
      "epoch: 2920 / loss: 0.04610202834010124 / accuracy: 1.0\n",
      "epoch: 2921 / loss: 0.04608030617237091 / accuracy: 1.0\n",
      "epoch: 2922 / loss: 0.04605870693922043 / accuracy: 1.0\n",
      "epoch: 2923 / loss: 0.0460369810461998 / accuracy: 1.0\n",
      "epoch: 2924 / loss: 0.04601544141769409 / accuracy: 1.0\n",
      "epoch: 2925 / loss: 0.04599384218454361 / accuracy: 1.0\n",
      "epoch: 2926 / loss: 0.04597212001681328 / accuracy: 1.0\n",
      "epoch: 2927 / loss: 0.045950643718242645 / accuracy: 1.0\n",
      "epoch: 2928 / loss: 0.04592910036444664 / accuracy: 1.0\n",
      "epoch: 2929 / loss: 0.04590756073594093 / accuracy: 1.0\n",
      "epoch: 2930 / loss: 0.0458860844373703 / accuracy: 1.0\n",
      "epoch: 2931 / loss: 0.04586472362279892 / accuracy: 1.0\n",
      "epoch: 2932 / loss: 0.04584318399429321 / accuracy: 1.0\n",
      "epoch: 2933 / loss: 0.04582170397043228 / accuracy: 1.0\n",
      "epoch: 2934 / loss: 0.04580022394657135 / accuracy: 1.0\n",
      "epoch: 2935 / loss: 0.04577898979187012 / accuracy: 1.0\n",
      "epoch: 2936 / loss: 0.04575745016336441 / accuracy: 1.0\n",
      "epoch: 2937 / loss: 0.045736148953437805 / accuracy: 1.0\n",
      "epoch: 2938 / loss: 0.04571473225951195 / accuracy: 1.0\n",
      "epoch: 2939 / loss: 0.04569355398416519 / accuracy: 1.0\n",
      "epoch: 2940 / loss: 0.045672133564949036 / accuracy: 1.0\n",
      "epoch: 2941 / loss: 0.0456508994102478 / accuracy: 1.0\n",
      "epoch: 2942 / loss: 0.045629605650901794 / accuracy: 1.0\n",
      "epoch: 2943 / loss: 0.045608364045619965 / accuracy: 1.0\n",
      "epoch: 2944 / loss: 0.04558712989091873 / accuracy: 1.0\n",
      "epoch: 2945 / loss: 0.045565955340862274 / accuracy: 1.0\n",
      "epoch: 2946 / loss: 0.04554478079080582 / accuracy: 1.0\n",
      "epoch: 2947 / loss: 0.04552353918552399 / accuracy: 1.0\n",
      "epoch: 2948 / loss: 0.045502305030822754 / accuracy: 1.0\n",
      "epoch: 2949 / loss: 0.04548131301999092 / accuracy: 1.0\n",
      "epoch: 2950 / loss: 0.045460134744644165 / accuracy: 1.0\n",
      "epoch: 2951 / loss: 0.04543907940387726 / accuracy: 1.0\n",
      "epoch: 2952 / loss: 0.04541802778840065 / accuracy: 1.0\n",
      "epoch: 2953 / loss: 0.045396849513053894 / accuracy: 1.0\n",
      "epoch: 2954 / loss: 0.04537585377693176 / accuracy: 1.0\n",
      "epoch: 2955 / loss: 0.04535486549139023 / accuracy: 1.0\n",
      "epoch: 2956 / loss: 0.04533392935991287 / accuracy: 1.0\n",
      "epoch: 2957 / loss: 0.045312993228435516 / accuracy: 1.0\n",
      "epoch: 2958 / loss: 0.04529194161295891 / accuracy: 1.0\n",
      "epoch: 2959 / loss: 0.045271072536706924 / accuracy: 1.0\n",
      "epoch: 2960 / loss: 0.045250073075294495 / accuracy: 1.0\n",
      "epoch: 2961 / loss: 0.04522920399904251 / accuracy: 1.0\n",
      "epoch: 2962 / loss: 0.045208267867565155 / accuracy: 1.0\n",
      "epoch: 2963 / loss: 0.04518739879131317 / accuracy: 1.0\n",
      "epoch: 2964 / loss: 0.04516652598977089 / accuracy: 1.0\n",
      "epoch: 2965 / loss: 0.04514583572745323 / accuracy: 1.0\n",
      "epoch: 2966 / loss: 0.04512496292591095 / accuracy: 1.0\n",
      "epoch: 2967 / loss: 0.04510427266359329 / accuracy: 1.0\n",
      "epoch: 2968 / loss: 0.04508328065276146 / accuracy: 1.0\n",
      "epoch: 2969 / loss: 0.04506271332502365 / accuracy: 1.0\n",
      "epoch: 2970 / loss: 0.04504195973277092 / accuracy: 1.0\n",
      "epoch: 2971 / loss: 0.04502115026116371 / accuracy: 1.0\n",
      "epoch: 2972 / loss: 0.045000579208135605 / accuracy: 1.0\n",
      "epoch: 2973 / loss: 0.04497983306646347 / accuracy: 1.0\n",
      "epoch: 2974 / loss: 0.044959135353565216 / accuracy: 1.0\n",
      "epoch: 2975 / loss: 0.04493844881653786 / accuracy: 1.0\n",
      "epoch: 2976 / loss: 0.04491788148880005 / accuracy: 1.0\n",
      "epoch: 2977 / loss: 0.04489719122648239 / accuracy: 1.0\n",
      "epoch: 2978 / loss: 0.044876620173454285 / accuracy: 1.0\n",
      "epoch: 2979 / loss: 0.04485617205500603 / accuracy: 1.0\n",
      "epoch: 2980 / loss: 0.04483548179268837 / accuracy: 1.0\n",
      "epoch: 2981 / loss: 0.04481503367424011 / accuracy: 1.0\n",
      "epoch: 2982 / loss: 0.04479452967643738 / accuracy: 1.0\n",
      "epoch: 2983 / loss: 0.04477401822805405 / accuracy: 1.0\n",
      "epoch: 2984 / loss: 0.04475357383489609 / accuracy: 1.0\n",
      "epoch: 2985 / loss: 0.044733062386512756 / accuracy: 1.0\n",
      "epoch: 2986 / loss: 0.0447126179933548 / accuracy: 1.0\n",
      "epoch: 2987 / loss: 0.04469216614961624 / accuracy: 1.0\n",
      "epoch: 2988 / loss: 0.04467172175645828 / accuracy: 1.0\n",
      "epoch: 2989 / loss: 0.044651515781879425 / accuracy: 1.0\n",
      "epoch: 2990 / loss: 0.04463113099336624 / accuracy: 1.0\n",
      "epoch: 2991 / loss: 0.04461074247956276 / accuracy: 1.0\n",
      "epoch: 2992 / loss: 0.04459047690033913 / accuracy: 1.0\n",
      "epoch: 2993 / loss: 0.04457009211182594 / accuracy: 1.0\n",
      "epoch: 2994 / loss: 0.04454982653260231 / accuracy: 1.0\n",
      "epoch: 2995 / loss: 0.04452943801879883 / accuracy: 1.0\n",
      "epoch: 2996 / loss: 0.04450923204421997 / accuracy: 1.0\n",
      "epoch: 2997 / loss: 0.04448903352022171 / accuracy: 1.0\n",
      "epoch: 2998 / loss: 0.04446882754564285 / accuracy: 1.0\n",
      "epoch: 2999 / loss: 0.044448502361774445 / accuracy: 1.0\n",
      "epoch: 3000 / loss: 0.04442835599184036 / accuracy: 1.0\n",
      "epoch: 3001 / loss: 0.044408150017261505 / accuracy: 1.0\n",
      "epoch: 3002 / loss: 0.04438812658190727 / accuracy: 1.0\n",
      "epoch: 3003 / loss: 0.04436792433261871 / accuracy: 1.0\n",
      "epoch: 3004 / loss: 0.044347841292619705 / accuracy: 1.0\n",
      "epoch: 3005 / loss: 0.04432769492268562 / accuracy: 1.0\n",
      "epoch: 3006 / loss: 0.04430761560797691 / accuracy: 1.0\n",
      "epoch: 3007 / loss: 0.044287532567977905 / accuracy: 1.0\n",
      "epoch: 3008 / loss: 0.04426750913262367 / accuracy: 1.0\n",
      "epoch: 3009 / loss: 0.044247426092624664 / accuracy: 1.0\n",
      "epoch: 3010 / loss: 0.04422752186655998 / accuracy: 1.0\n",
      "epoch: 3011 / loss: 0.04420749843120575 / accuracy: 1.0\n",
      "epoch: 3012 / loss: 0.044187597930431366 / accuracy: 1.0\n",
      "epoch: 3013 / loss: 0.04416757822036743 / accuracy: 1.0\n",
      "epoch: 3014 / loss: 0.04414767771959305 / accuracy: 1.0\n",
      "epoch: 3015 / loss: 0.044127654284238815 / accuracy: 1.0\n",
      "epoch: 3016 / loss: 0.04410780966281891 / accuracy: 1.0\n",
      "epoch: 3017 / loss: 0.04408784955739975 / accuracy: 1.0\n",
      "epoch: 3018 / loss: 0.04406812787055969 / accuracy: 1.0\n",
      "epoch: 3019 / loss: 0.04404817149043083 / accuracy: 1.0\n",
      "epoch: 3020 / loss: 0.0440283864736557 / accuracy: 1.0\n",
      "epoch: 3021 / loss: 0.04400854557752609 / accuracy: 1.0\n",
      "epoch: 3022 / loss: 0.04398876801133156 / accuracy: 1.0\n",
      "epoch: 3023 / loss: 0.0439690463244915 / accuracy: 1.0\n",
      "epoch: 3024 / loss: 0.043949149549007416 / accuracy: 1.0\n",
      "epoch: 3025 / loss: 0.043929487466812134 / accuracy: 1.0\n",
      "epoch: 3026 / loss: 0.043909769505262375 / accuracy: 1.0\n",
      "epoch: 3027 / loss: 0.043890051543712616 / accuracy: 1.0\n",
      "epoch: 3028 / loss: 0.04387027025222778 / accuracy: 1.0\n",
      "epoch: 3029 / loss: 0.043850675225257874 / accuracy: 1.0\n",
      "epoch: 3030 / loss: 0.043830953538417816 / accuracy: 1.0\n",
      "epoch: 3031 / loss: 0.04381147772073746 / accuracy: 1.0\n",
      "epoch: 3032 / loss: 0.043791696429252625 / accuracy: 1.0\n",
      "epoch: 3033 / loss: 0.043772220611572266 / accuracy: 1.0\n",
      "epoch: 3034 / loss: 0.04375249892473221 / accuracy: 1.0\n",
      "epoch: 3035 / loss: 0.04373302683234215 / accuracy: 1.0\n",
      "epoch: 3036 / loss: 0.043713368475437164 / accuracy: 1.0\n",
      "epoch: 3037 / loss: 0.04369395226240158 / accuracy: 1.0\n",
      "epoch: 3038 / loss: 0.04367447271943092 / accuracy: 1.0\n",
      "epoch: 3039 / loss: 0.043654996901750565 / accuracy: 1.0\n",
      "epoch: 3040 / loss: 0.043635401874780655 / accuracy: 1.0\n",
      "epoch: 3041 / loss: 0.04361598193645477 / accuracy: 1.0\n",
      "epoch: 3042 / loss: 0.04359644651412964 / accuracy: 1.0\n",
      "epoch: 3043 / loss: 0.043577030301094055 / accuracy: 1.0\n",
      "epoch: 3044 / loss: 0.04355767369270325 / accuracy: 1.0\n",
      "epoch: 3045 / loss: 0.04353813827037811 / accuracy: 1.0\n",
      "epoch: 3046 / loss: 0.04351884499192238 / accuracy: 1.0\n",
      "epoch: 3047 / loss: 0.043499428778886795 / accuracy: 1.0\n",
      "epoch: 3048 / loss: 0.043480075895786285 / accuracy: 1.0\n",
      "epoch: 3049 / loss: 0.04346071928739548 / accuracy: 1.0\n",
      "epoch: 3050 / loss: 0.04344142600893974 / accuracy: 1.0\n",
      "epoch: 3051 / loss: 0.043422188609838486 / accuracy: 1.0\n",
      "epoch: 3052 / loss: 0.04340295493602753 / accuracy: 1.0\n",
      "epoch: 3053 / loss: 0.04338359832763672 / accuracy: 1.0\n",
      "epoch: 3054 / loss: 0.043364185839891434 / accuracy: 1.0\n",
      "epoch: 3055 / loss: 0.043345071375370026 / accuracy: 1.0\n",
      "epoch: 3056 / loss: 0.043325960636138916 / accuracy: 1.0\n",
      "epoch: 3057 / loss: 0.04330666363239288 / accuracy: 1.0\n",
      "epoch: 3058 / loss: 0.0432874895632267 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3059 / loss: 0.043268319219350815 / accuracy: 1.0\n",
      "epoch: 3060 / loss: 0.04324920475482941 / accuracy: 1.0\n",
      "epoch: 3061 / loss: 0.04323003441095352 / accuracy: 1.0\n",
      "epoch: 3062 / loss: 0.043210919946432114 / accuracy: 1.0\n",
      "epoch: 3063 / loss: 0.04319174587726593 / accuracy: 1.0\n",
      "epoch: 3064 / loss: 0.043172694742679596 / accuracy: 1.0\n",
      "epoch: 3065 / loss: 0.04315364360809326 / accuracy: 1.0\n",
      "epoch: 3066 / loss: 0.04313459247350693 / accuracy: 1.0\n",
      "epoch: 3067 / loss: 0.043115533888339996 / accuracy: 1.0\n",
      "epoch: 3068 / loss: 0.04309660941362381 / accuracy: 1.0\n",
      "epoch: 3069 / loss: 0.043077439069747925 / accuracy: 1.0\n",
      "epoch: 3070 / loss: 0.04305850341916084 / accuracy: 1.0\n",
      "epoch: 3071 / loss: 0.04303957521915436 / accuracy: 1.0\n",
      "epoch: 3072 / loss: 0.0430205799639225 / accuracy: 1.0\n",
      "epoch: 3073 / loss: 0.04300171136856079 / accuracy: 1.0\n",
      "epoch: 3074 / loss: 0.042982783168554306 / accuracy: 1.0\n",
      "epoch: 3075 / loss: 0.04296397045254707 / accuracy: 1.0\n",
      "epoch: 3076 / loss: 0.04294498264789581 / accuracy: 1.0\n",
      "epoch: 3077 / loss: 0.04292605072259903 / accuracy: 1.0\n",
      "epoch: 3078 / loss: 0.042907118797302246 / accuracy: 1.0\n",
      "epoch: 3079 / loss: 0.042888492345809937 / accuracy: 1.0\n",
      "epoch: 3080 / loss: 0.04286962002515793 / accuracy: 1.0\n",
      "epoch: 3081 / loss: 0.04285081475973129 / accuracy: 1.0\n",
      "epoch: 3082 / loss: 0.04283200204372406 / accuracy: 1.0\n",
      "epoch: 3083 / loss: 0.042813196778297424 / accuracy: 1.0\n",
      "epoch: 3084 / loss: 0.04279438406229019 / accuracy: 1.0\n",
      "epoch: 3085 / loss: 0.04277563840150833 / accuracy: 1.0\n",
      "epoch: 3086 / loss: 0.042756885290145874 / accuracy: 1.0\n",
      "epoch: 3087 / loss: 0.04273832216858864 / accuracy: 1.0\n",
      "epoch: 3088 / loss: 0.042719513177871704 / accuracy: 1.0\n",
      "epoch: 3089 / loss: 0.042700886726379395 / accuracy: 1.0\n",
      "epoch: 3090 / loss: 0.042682137340307236 / accuracy: 1.0\n",
      "epoch: 3091 / loss: 0.0426635704934597 / accuracy: 1.0\n",
      "epoch: 3092 / loss: 0.04264488071203232 / accuracy: 1.0\n",
      "epoch: 3093 / loss: 0.04262625426054001 / accuracy: 1.0\n",
      "epoch: 3094 / loss: 0.0426076278090477 / accuracy: 1.0\n",
      "epoch: 3095 / loss: 0.042589180171489716 / accuracy: 1.0\n",
      "epoch: 3096 / loss: 0.042570553719997406 / accuracy: 1.0\n",
      "epoch: 3097 / loss: 0.04255198687314987 / accuracy: 1.0\n",
      "epoch: 3098 / loss: 0.04253336042165756 / accuracy: 1.0\n",
      "epoch: 3099 / loss: 0.0425148531794548 / accuracy: 1.0\n",
      "epoch: 3100 / loss: 0.042496465146541595 / accuracy: 1.0\n",
      "epoch: 3101 / loss: 0.04247789829969406 / accuracy: 1.0\n",
      "epoch: 3102 / loss: 0.042459458112716675 / accuracy: 1.0\n",
      "epoch: 3103 / loss: 0.04244101047515869 / accuracy: 1.0\n",
      "epoch: 3104 / loss: 0.04242250323295593 / accuracy: 1.0\n",
      "epoch: 3105 / loss: 0.04240405932068825 / accuracy: 1.0\n",
      "epoch: 3106 / loss: 0.042385734617710114 / accuracy: 1.0\n",
      "epoch: 3107 / loss: 0.042367346584796906 / accuracy: 1.0\n",
      "epoch: 3108 / loss: 0.04234890639781952 / accuracy: 1.0\n",
      "epoch: 3109 / loss: 0.04233051836490631 / accuracy: 1.0\n",
      "epoch: 3110 / loss: 0.04231225699186325 / accuracy: 1.0\n",
      "epoch: 3111 / loss: 0.04229392856359482 / accuracy: 1.0\n",
      "epoch: 3112 / loss: 0.04227560758590698 / accuracy: 1.0\n",
      "epoch: 3113 / loss: 0.042257342487573624 / accuracy: 1.0\n",
      "epoch: 3114 / loss: 0.042239077389240265 / accuracy: 1.0\n",
      "epoch: 3115 / loss: 0.042220696806907654 / accuracy: 1.0\n",
      "epoch: 3116 / loss: 0.04220249503850937 / accuracy: 1.0\n",
      "epoch: 3117 / loss: 0.04218422621488571 / accuracy: 1.0\n",
      "epoch: 3118 / loss: 0.042165905237197876 / accuracy: 1.0\n",
      "epoch: 3119 / loss: 0.04214770346879959 / accuracy: 1.0\n",
      "epoch: 3120 / loss: 0.04212956130504608 / accuracy: 1.0\n",
      "epoch: 3121 / loss: 0.04211147874593735 / accuracy: 1.0\n",
      "epoch: 3122 / loss: 0.04209315404295921 / accuracy: 1.0\n",
      "epoch: 3123 / loss: 0.042075131088495255 / accuracy: 1.0\n",
      "epoch: 3124 / loss: 0.042056988924741745 / accuracy: 1.0\n",
      "epoch: 3125 / loss: 0.04203878715634346 / accuracy: 1.0\n",
      "epoch: 3126 / loss: 0.0420207679271698 / accuracy: 1.0\n",
      "epoch: 3127 / loss: 0.04200250282883644 / accuracy: 1.0\n",
      "epoch: 3128 / loss: 0.04198460280895233 / accuracy: 1.0\n",
      "epoch: 3129 / loss: 0.04196646064519882 / accuracy: 1.0\n",
      "epoch: 3130 / loss: 0.04194837808609009 / accuracy: 1.0\n",
      "epoch: 3131 / loss: 0.04193054139614105 / accuracy: 1.0\n",
      "epoch: 3132 / loss: 0.04191245511174202 / accuracy: 1.0\n",
      "epoch: 3133 / loss: 0.041894376277923584 / accuracy: 1.0\n",
      "epoch: 3134 / loss: 0.041876357048749924 / accuracy: 1.0\n",
      "epoch: 3135 / loss: 0.041858572512865067 / accuracy: 1.0\n",
      "epoch: 3136 / loss: 0.041840434074401855 / accuracy: 1.0\n",
      "epoch: 3137 / loss: 0.041822537779808044 / accuracy: 1.0\n",
      "epoch: 3138 / loss: 0.041804634034633636 / accuracy: 1.0\n",
      "epoch: 3139 / loss: 0.0417867936193943 / accuracy: 1.0\n",
      "epoch: 3140 / loss: 0.04176889359951019 / accuracy: 1.0\n",
      "epoch: 3141 / loss: 0.04175087437033653 / accuracy: 1.0\n",
      "epoch: 3142 / loss: 0.04173309728503227 / accuracy: 1.0\n",
      "epoch: 3143 / loss: 0.041715316474437714 / accuracy: 1.0\n",
      "epoch: 3144 / loss: 0.04169747605919838 / accuracy: 1.0\n",
      "epoch: 3145 / loss: 0.04167957976460457 / accuracy: 1.0\n",
      "epoch: 3146 / loss: 0.04166179895401001 / accuracy: 1.0\n",
      "epoch: 3147 / loss: 0.04164402186870575 / accuracy: 1.0\n",
      "epoch: 3148 / loss: 0.04162624105811119 / accuracy: 1.0\n",
      "epoch: 3149 / loss: 0.04160846024751663 / accuracy: 1.0\n",
      "epoch: 3150 / loss: 0.04159068688750267 / accuracy: 1.0\n",
      "epoch: 3151 / loss: 0.04157302528619766 / accuracy: 1.0\n",
      "epoch: 3152 / loss: 0.04155537113547325 / accuracy: 1.0\n",
      "epoch: 3153 / loss: 0.04153770953416824 / accuracy: 1.0\n",
      "epoch: 3154 / loss: 0.04151993244886398 / accuracy: 1.0\n",
      "epoch: 3155 / loss: 0.04150233417749405 / accuracy: 1.0\n",
      "epoch: 3156 / loss: 0.04148455709218979 / accuracy: 1.0\n",
      "epoch: 3157 / loss: 0.041467081755399704 / accuracy: 1.0\n",
      "epoch: 3158 / loss: 0.041449304670095444 / accuracy: 1.0\n",
      "epoch: 3159 / loss: 0.04143182560801506 / accuracy: 1.0\n",
      "epoch: 3160 / loss: 0.0414140485227108 / accuracy: 1.0\n",
      "epoch: 3161 / loss: 0.041396573185920715 / accuracy: 1.0\n",
      "epoch: 3162 / loss: 0.041379038244485855 / accuracy: 1.0\n",
      "epoch: 3163 / loss: 0.04136144369840622 / accuracy: 1.0\n",
      "epoch: 3164 / loss: 0.04134390503168106 / accuracy: 1.0\n",
      "epoch: 3165 / loss: 0.0413263663649559 / accuracy: 1.0\n",
      "epoch: 3166 / loss: 0.04130895435810089 / accuracy: 1.0\n",
      "epoch: 3167 / loss: 0.041291359812021255 / accuracy: 1.0\n",
      "epoch: 3168 / loss: 0.04127388447523117 / accuracy: 1.0\n",
      "epoch: 3169 / loss: 0.04125646874308586 / accuracy: 1.0\n",
      "epoch: 3170 / loss: 0.041238993406295776 / accuracy: 1.0\n",
      "epoch: 3171 / loss: 0.04122151434421539 / accuracy: 1.0\n",
      "epoch: 3172 / loss: 0.04120410233736038 / accuracy: 1.0\n",
      "epoch: 3173 / loss: 0.04118668660521507 / accuracy: 1.0\n",
      "epoch: 3174 / loss: 0.041169390082359314 / accuracy: 1.0\n",
      "epoch: 3175 / loss: 0.0411519780755043 / accuracy: 1.0\n",
      "epoch: 3176 / loss: 0.041134562343358994 / accuracy: 1.0\n",
      "epoch: 3177 / loss: 0.04111720621585846 / accuracy: 1.0\n",
      "epoch: 3178 / loss: 0.04109979420900345 / accuracy: 1.0\n",
      "epoch: 3179 / loss: 0.04108262062072754 / accuracy: 1.0\n",
      "epoch: 3180 / loss: 0.04106532782316208 / accuracy: 1.0\n",
      "epoch: 3181 / loss: 0.04104791581630707 / accuracy: 1.0\n",
      "epoch: 3182 / loss: 0.04103061556816101 / accuracy: 1.0\n",
      "epoch: 3183 / loss: 0.041013386100530624 / accuracy: 1.0\n",
      "epoch: 3184 / loss: 0.040996212512254715 / accuracy: 1.0\n",
      "epoch: 3185 / loss: 0.04097903519868851 / accuracy: 1.0\n",
      "epoch: 3186 / loss: 0.04096168652176857 / accuracy: 1.0\n",
      "epoch: 3187 / loss: 0.040944572538137436 / accuracy: 1.0\n",
      "epoch: 3188 / loss: 0.040927402675151825 / accuracy: 1.0\n",
      "epoch: 3189 / loss: 0.040910229086875916 / accuracy: 1.0\n",
      "epoch: 3190 / loss: 0.04089299589395523 / accuracy: 1.0\n",
      "epoch: 3191 / loss: 0.040875762701034546 / accuracy: 1.0\n",
      "epoch: 3192 / loss: 0.04085870832204819 / accuracy: 1.0\n",
      "epoch: 3193 / loss: 0.04084160178899765 / accuracy: 1.0\n",
      "epoch: 3194 / loss: 0.040824487805366516 / accuracy: 1.0\n",
      "epoch: 3195 / loss: 0.04080749303102493 / accuracy: 1.0\n",
      "epoch: 3196 / loss: 0.04079025983810425 / accuracy: 1.0\n",
      "epoch: 3197 / loss: 0.04077315330505371 / accuracy: 1.0\n",
      "epoch: 3198 / loss: 0.04075603932142258 / accuracy: 1.0\n",
      "epoch: 3199 / loss: 0.040739111602306366 / accuracy: 1.0\n",
      "epoch: 3200 / loss: 0.04072217643260956 / accuracy: 1.0\n",
      "epoch: 3201 / loss: 0.040705129504203796 / accuracy: 1.0\n",
      "epoch: 3202 / loss: 0.04068813472986221 / accuracy: 1.0\n",
      "epoch: 3203 / loss: 0.04067114368081093 / accuracy: 1.0\n",
      "epoch: 3204 / loss: 0.04065415635704994 / accuracy: 1.0\n",
      "epoch: 3205 / loss: 0.04063728451728821 / accuracy: 1.0\n",
      "epoch: 3206 / loss: 0.04062023386359215 / accuracy: 1.0\n",
      "epoch: 3207 / loss: 0.04060336574912071 / accuracy: 1.0\n",
      "epoch: 3208 / loss: 0.0405864343047142 / accuracy: 1.0\n",
      "epoch: 3209 / loss: 0.04056950658559799 / accuracy: 1.0\n",
      "epoch: 3210 / loss: 0.04055269807577133 / accuracy: 1.0\n",
      "epoch: 3211 / loss: 0.0405358225107193 / accuracy: 1.0\n",
      "epoch: 3212 / loss: 0.040519073605537415 / accuracy: 1.0\n",
      "epoch: 3213 / loss: 0.040502145886421204 / accuracy: 1.0\n",
      "epoch: 3214 / loss: 0.040485214442014694 / accuracy: 1.0\n",
      "epoch: 3215 / loss: 0.04046835005283356 / accuracy: 1.0\n",
      "epoch: 3216 / loss: 0.040451597422361374 / accuracy: 1.0\n",
      "epoch: 3217 / loss: 0.040434788912534714 / accuracy: 1.0\n",
      "epoch: 3218 / loss: 0.04041804373264313 / accuracy: 1.0\n",
      "epoch: 3219 / loss: 0.04040123522281647 / accuracy: 1.0\n",
      "epoch: 3220 / loss: 0.04038454592227936 / accuracy: 1.0\n",
      "epoch: 3221 / loss: 0.0403677374124527 / accuracy: 1.0\n",
      "epoch: 3222 / loss: 0.04035098850727081 / accuracy: 1.0\n",
      "epoch: 3223 / loss: 0.040334418416023254 / accuracy: 1.0\n",
      "epoch: 3224 / loss: 0.040317609906196594 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3225 / loss: 0.04030092433094978 / accuracy: 1.0\n",
      "epoch: 3226 / loss: 0.0402844175696373 / accuracy: 1.0\n",
      "epoch: 3227 / loss: 0.040267668664455414 / accuracy: 1.0\n",
      "epoch: 3228 / loss: 0.04025092348456383 / accuracy: 1.0\n",
      "epoch: 3229 / loss: 0.04023435711860657 / accuracy: 1.0\n",
      "epoch: 3230 / loss: 0.04021778702735901 / accuracy: 1.0\n",
      "epoch: 3231 / loss: 0.040201157331466675 / accuracy: 1.0\n",
      "epoch: 3232 / loss: 0.040184471756219864 / accuracy: 1.0\n",
      "epoch: 3233 / loss: 0.04016802832484245 / accuracy: 1.0\n",
      "epoch: 3234 / loss: 0.04015152156352997 / accuracy: 1.0\n",
      "epoch: 3235 / loss: 0.040134772658348083 / accuracy: 1.0\n",
      "epoch: 3236 / loss: 0.04011838510632515 / accuracy: 1.0\n",
      "epoch: 3237 / loss: 0.04010188207030296 / accuracy: 1.0\n",
      "epoch: 3238 / loss: 0.04008537530899048 / accuracy: 1.0\n",
      "epoch: 3239 / loss: 0.04006880894303322 / accuracy: 1.0\n",
      "epoch: 3240 / loss: 0.040052421391010284 / accuracy: 1.0\n",
      "epoch: 3241 / loss: 0.0400359183549881 / accuracy: 1.0\n",
      "epoch: 3242 / loss: 0.04001947119832039 / accuracy: 1.0\n",
      "epoch: 3243 / loss: 0.040002964437007904 / accuracy: 1.0\n",
      "epoch: 3244 / loss: 0.03998658061027527 / accuracy: 1.0\n",
      "epoch: 3245 / loss: 0.03997025638818741 / accuracy: 1.0\n",
      "epoch: 3246 / loss: 0.039953749626874924 / accuracy: 1.0\n",
      "epoch: 3247 / loss: 0.03993736952543259 / accuracy: 1.0\n",
      "epoch: 3248 / loss: 0.039921101182699203 / accuracy: 1.0\n",
      "epoch: 3249 / loss: 0.03990459442138672 / accuracy: 1.0\n",
      "epoch: 3250 / loss: 0.03988827019929886 / accuracy: 1.0\n",
      "epoch: 3251 / loss: 0.039872005581855774 / accuracy: 1.0\n",
      "epoch: 3252 / loss: 0.03985568508505821 / accuracy: 1.0\n",
      "epoch: 3253 / loss: 0.039839357137680054 / accuracy: 1.0\n",
      "epoch: 3254 / loss: 0.03982315585017204 / accuracy: 1.0\n",
      "epoch: 3255 / loss: 0.039806708693504333 / accuracy: 1.0\n",
      "epoch: 3256 / loss: 0.03979044780135155 / accuracy: 1.0\n",
      "epoch: 3257 / loss: 0.03977411985397339 / accuracy: 1.0\n",
      "epoch: 3258 / loss: 0.0397578589618206 / accuracy: 1.0\n",
      "epoch: 3259 / loss: 0.03974171727895737 / accuracy: 1.0\n",
      "epoch: 3260 / loss: 0.03972539305686951 / accuracy: 1.0\n",
      "epoch: 3261 / loss: 0.0397091880440712 / accuracy: 1.0\n",
      "epoch: 3262 / loss: 0.03969298675656319 / accuracy: 1.0\n",
      "epoch: 3263 / loss: 0.03967702388763428 / accuracy: 1.0\n",
      "epoch: 3264 / loss: 0.03966081887483597 / accuracy: 1.0\n",
      "epoch: 3265 / loss: 0.03964467719197273 / accuracy: 1.0\n",
      "epoch: 3266 / loss: 0.03962847590446472 / accuracy: 1.0\n",
      "epoch: 3267 / loss: 0.03961227089166641 / accuracy: 1.0\n",
      "epoch: 3268 / loss: 0.03959618881344795 / accuracy: 1.0\n",
      "epoch: 3269 / loss: 0.03958010673522949 / accuracy: 1.0\n",
      "epoch: 3270 / loss: 0.03956402465701103 / accuracy: 1.0\n",
      "epoch: 3271 / loss: 0.0395478829741478 / accuracy: 1.0\n",
      "epoch: 3272 / loss: 0.039531804621219635 / accuracy: 1.0\n",
      "epoch: 3273 / loss: 0.03951577842235565 / accuracy: 1.0\n",
      "epoch: 3274 / loss: 0.03949975594878197 / accuracy: 1.0\n",
      "epoch: 3275 / loss: 0.039483796805143356 / accuracy: 1.0\n",
      "epoch: 3276 / loss: 0.03946777433156967 / accuracy: 1.0\n",
      "epoch: 3277 / loss: 0.039451755583286285 / accuracy: 1.0\n",
      "epoch: 3278 / loss: 0.03943585231900215 / accuracy: 1.0\n",
      "epoch: 3279 / loss: 0.039419710636138916 / accuracy: 1.0\n",
      "epoch: 3280 / loss: 0.03940381109714508 / accuracy: 1.0\n",
      "epoch: 3281 / loss: 0.03938784822821617 / accuracy: 1.0\n",
      "epoch: 3282 / loss: 0.039371829479932785 / accuracy: 1.0\n",
      "epoch: 3283 / loss: 0.039355989545583725 / accuracy: 1.0\n",
      "epoch: 3284 / loss: 0.039340026676654816 / accuracy: 1.0\n",
      "epoch: 3285 / loss: 0.039324186742305756 / accuracy: 1.0\n",
      "epoch: 3286 / loss: 0.039308227598667145 / accuracy: 1.0\n",
      "epoch: 3287 / loss: 0.03929244726896286 / accuracy: 1.0\n",
      "epoch: 3288 / loss: 0.0392766036093235 / accuracy: 1.0\n",
      "epoch: 3289 / loss: 0.039260827004909515 / accuracy: 1.0\n",
      "epoch: 3290 / loss: 0.039244867861270905 / accuracy: 1.0\n",
      "epoch: 3291 / loss: 0.03922908753156662 / accuracy: 1.0\n",
      "epoch: 3292 / loss: 0.039213307201862335 / accuracy: 1.0\n",
      "epoch: 3293 / loss: 0.03919752687215805 / accuracy: 1.0\n",
      "epoch: 3294 / loss: 0.03918180614709854 / accuracy: 1.0\n",
      "epoch: 3295 / loss: 0.039165910333395004 / accuracy: 1.0\n",
      "epoch: 3296 / loss: 0.03915013000369072 / accuracy: 1.0\n",
      "epoch: 3297 / loss: 0.039134349673986435 / accuracy: 1.0\n",
      "epoch: 3298 / loss: 0.03911856934428215 / accuracy: 1.0\n",
      "epoch: 3299 / loss: 0.039102792739868164 / accuracy: 1.0\n",
      "epoch: 3300 / loss: 0.039087194949388504 / accuracy: 1.0\n",
      "epoch: 3301 / loss: 0.03907147794961929 / accuracy: 1.0\n",
      "epoch: 3302 / loss: 0.039055757224559784 / accuracy: 1.0\n",
      "epoch: 3303 / loss: 0.03904009610414505 / accuracy: 1.0\n",
      "epoch: 3304 / loss: 0.03902450203895569 / accuracy: 1.0\n",
      "epoch: 3305 / loss: 0.039008840918540955 / accuracy: 1.0\n",
      "epoch: 3306 / loss: 0.03899312764406204 / accuracy: 1.0\n",
      "epoch: 3307 / loss: 0.03897758573293686 / accuracy: 1.0\n",
      "epoch: 3308 / loss: 0.03896186500787735 / accuracy: 1.0\n",
      "epoch: 3309 / loss: 0.03894639015197754 / accuracy: 1.0\n",
      "epoch: 3310 / loss: 0.03893079608678818 / accuracy: 1.0\n",
      "epoch: 3311 / loss: 0.038915254175662994 / accuracy: 1.0\n",
      "epoch: 3312 / loss: 0.03889960050582886 / accuracy: 1.0\n",
      "epoch: 3313 / loss: 0.038884058594703674 / accuracy: 1.0\n",
      "epoch: 3314 / loss: 0.03886852413415909 / accuracy: 1.0\n",
      "epoch: 3315 / loss: 0.03885304927825928 / accuracy: 1.0\n",
      "epoch: 3316 / loss: 0.03883738815784454 / accuracy: 1.0\n",
      "epoch: 3317 / loss: 0.03882203251123428 / accuracy: 1.0\n",
      "epoch: 3318 / loss: 0.038806378841400146 / accuracy: 1.0\n",
      "epoch: 3319 / loss: 0.03879101946949959 / accuracy: 1.0\n",
      "epoch: 3320 / loss: 0.03877554088830948 / accuracy: 1.0\n",
      "epoch: 3321 / loss: 0.03876000642776489 / accuracy: 1.0\n",
      "epoch: 3322 / loss: 0.03874453157186508 / accuracy: 1.0\n",
      "epoch: 3323 / loss: 0.03872917592525482 / accuracy: 1.0\n",
      "epoch: 3324 / loss: 0.03871375694870949 / accuracy: 1.0\n",
      "epoch: 3325 / loss: 0.03869828209280968 / accuracy: 1.0\n",
      "epoch: 3326 / loss: 0.03868292644619942 / accuracy: 1.0\n",
      "epoch: 3327 / loss: 0.03866757079958916 / accuracy: 1.0\n",
      "epoch: 3328 / loss: 0.0386522114276886 / accuracy: 1.0\n",
      "epoch: 3329 / loss: 0.03863679990172386 / accuracy: 1.0\n",
      "epoch: 3330 / loss: 0.0386214442551136 / accuracy: 1.0\n",
      "epoch: 3331 / loss: 0.03860620781779289 / accuracy: 1.0\n",
      "epoch: 3332 / loss: 0.038590915501117706 / accuracy: 1.0\n",
      "epoch: 3333 / loss: 0.03857538104057312 / accuracy: 1.0\n",
      "epoch: 3334 / loss: 0.038560204207897186 / accuracy: 1.0\n",
      "epoch: 3335 / loss: 0.0385449081659317 / accuracy: 1.0\n",
      "epoch: 3336 / loss: 0.03852967172861099 / accuracy: 1.0\n",
      "epoch: 3337 / loss: 0.03851444274187088 / accuracy: 1.0\n",
      "epoch: 3338 / loss: 0.03849920630455017 / accuracy: 1.0\n",
      "epoch: 3339 / loss: 0.03848385065793991 / accuracy: 1.0\n",
      "epoch: 3340 / loss: 0.038468554615974426 / accuracy: 1.0\n",
      "epoch: 3341 / loss: 0.03845338150858879 / accuracy: 1.0\n",
      "epoch: 3342 / loss: 0.03843814507126808 / accuracy: 1.0\n",
      "epoch: 3343 / loss: 0.03842303156852722 / accuracy: 1.0\n",
      "epoch: 3344 / loss: 0.03840779885649681 / accuracy: 1.0\n",
      "epoch: 3345 / loss: 0.038392625749111176 / accuracy: 1.0\n",
      "epoch: 3346 / loss: 0.03837757185101509 / accuracy: 1.0\n",
      "epoch: 3347 / loss: 0.038362398743629456 / accuracy: 1.0\n",
      "epoch: 3348 / loss: 0.03834734484553337 / accuracy: 1.0\n",
      "epoch: 3349 / loss: 0.038332171738147736 / accuracy: 1.0\n",
      "epoch: 3350 / loss: 0.03831711784005165 / accuracy: 1.0\n",
      "epoch: 3351 / loss: 0.038301944732666016 / accuracy: 1.0\n",
      "epoch: 3352 / loss: 0.03828677162528038 / accuracy: 1.0\n",
      "epoch: 3353 / loss: 0.038271836936473846 / accuracy: 1.0\n",
      "epoch: 3354 / loss: 0.038256727159023285 / accuracy: 1.0\n",
      "epoch: 3355 / loss: 0.0382416732609272 / accuracy: 1.0\n",
      "epoch: 3356 / loss: 0.03822655975818634 / accuracy: 1.0\n",
      "epoch: 3357 / loss: 0.03821168839931488 / accuracy: 1.0\n",
      "epoch: 3358 / loss: 0.03819657489657402 / accuracy: 1.0\n",
      "epoch: 3359 / loss: 0.038181520998477936 / accuracy: 1.0\n",
      "epoch: 3360 / loss: 0.038166530430316925 / accuracy: 1.0\n",
      "epoch: 3361 / loss: 0.03815159946680069 / accuracy: 1.0\n",
      "epoch: 3362 / loss: 0.03813648596405983 / accuracy: 1.0\n",
      "epoch: 3363 / loss: 0.038121674209833145 / accuracy: 1.0\n",
      "epoch: 3364 / loss: 0.038106679916381836 / accuracy: 1.0\n",
      "epoch: 3365 / loss: 0.0380917489528656 / accuracy: 1.0\n",
      "epoch: 3366 / loss: 0.03807675838470459 / accuracy: 1.0\n",
      "epoch: 3367 / loss: 0.03806188702583313 / accuracy: 1.0\n",
      "epoch: 3368 / loss: 0.03804701566696167 / accuracy: 1.0\n",
      "epoch: 3369 / loss: 0.03803202509880066 / accuracy: 1.0\n",
      "epoch: 3370 / loss: 0.03801727294921875 / accuracy: 1.0\n",
      "epoch: 3371 / loss: 0.038002341985702515 / accuracy: 1.0\n",
      "epoch: 3372 / loss: 0.037987589836120605 / accuracy: 1.0\n",
      "epoch: 3373 / loss: 0.03797253966331482 / accuracy: 1.0\n",
      "epoch: 3374 / loss: 0.03795778751373291 / accuracy: 1.0\n",
      "epoch: 3375 / loss: 0.03794291615486145 / accuracy: 1.0\n",
      "epoch: 3376 / loss: 0.03792816400527954 / accuracy: 1.0\n",
      "epoch: 3377 / loss: 0.03791329637169838 / accuracy: 1.0\n",
      "epoch: 3378 / loss: 0.03789854794740677 / accuracy: 1.0\n",
      "epoch: 3379 / loss: 0.03788379579782486 / accuracy: 1.0\n",
      "epoch: 3380 / loss: 0.0378689244389534 / accuracy: 1.0\n",
      "epoch: 3381 / loss: 0.03785417228937149 / accuracy: 1.0\n",
      "epoch: 3382 / loss: 0.03783942386507988 / accuracy: 1.0\n",
      "epoch: 3383 / loss: 0.037824735045433044 / accuracy: 1.0\n",
      "epoch: 3384 / loss: 0.037809982895851135 / accuracy: 1.0\n",
      "epoch: 3385 / loss: 0.037795353680849075 / accuracy: 1.0\n",
      "epoch: 3386 / loss: 0.03778066486120224 / accuracy: 1.0\n",
      "epoch: 3387 / loss: 0.037765972316265106 / accuracy: 1.0\n",
      "epoch: 3388 / loss: 0.03775128722190857 / accuracy: 1.0\n",
      "epoch: 3389 / loss: 0.037736594676971436 / accuracy: 1.0\n",
      "epoch: 3390 / loss: 0.0377219095826149 / accuracy: 1.0\n",
      "epoch: 3391 / loss: 0.037707336246967316 / accuracy: 1.0\n",
      "epoch: 3392 / loss: 0.037692710757255554 / accuracy: 1.0\n",
      "epoch: 3393 / loss: 0.037677958607673645 / accuracy: 1.0\n",
      "epoch: 3394 / loss: 0.037663329392671585 / accuracy: 1.0\n",
      "epoch: 3395 / loss: 0.0376487597823143 / accuracy: 1.0\n",
      "epoch: 3396 / loss: 0.03763413429260254 / accuracy: 1.0\n",
      "epoch: 3397 / loss: 0.03761968016624451 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3398 / loss: 0.03760511428117752 / accuracy: 1.0\n",
      "epoch: 3399 / loss: 0.037590544670820236 / accuracy: 1.0\n",
      "epoch: 3400 / loss: 0.03757597506046295 / accuracy: 1.0\n",
      "epoch: 3401 / loss: 0.03756134957075119 / accuracy: 1.0\n",
      "epoch: 3402 / loss: 0.03754696249961853 / accuracy: 1.0\n",
      "epoch: 3403 / loss: 0.037532392889261246 / accuracy: 1.0\n",
      "epoch: 3404 / loss: 0.03751794248819351 / accuracy: 1.0\n",
      "epoch: 3405 / loss: 0.03750349581241608 / accuracy: 1.0\n",
      "epoch: 3406 / loss: 0.03748892992734909 / accuracy: 1.0\n",
      "epoch: 3407 / loss: 0.03747441992163658 / accuracy: 1.0\n",
      "epoch: 3408 / loss: 0.03746003285050392 / accuracy: 1.0\n",
      "epoch: 3409 / loss: 0.037445466965436935 / accuracy: 1.0\n",
      "epoch: 3410 / loss: 0.03743114322423935 / accuracy: 1.0\n",
      "epoch: 3411 / loss: 0.03741668909788132 / accuracy: 1.0\n",
      "epoch: 3412 / loss: 0.03740224242210388 / accuracy: 1.0\n",
      "epoch: 3413 / loss: 0.03738785535097122 / accuracy: 1.0\n",
      "epoch: 3414 / loss: 0.037373531609773636 / accuracy: 1.0\n",
      "epoch: 3415 / loss: 0.03735914081335068 / accuracy: 1.0\n",
      "epoch: 3416 / loss: 0.03734469413757324 / accuracy: 1.0\n",
      "epoch: 3417 / loss: 0.03733036667108536 / accuracy: 1.0\n",
      "epoch: 3418 / loss: 0.03731610253453255 / accuracy: 1.0\n",
      "epoch: 3419 / loss: 0.03730177879333496 / accuracy: 1.0\n",
      "epoch: 3420 / loss: 0.03728732466697693 / accuracy: 1.0\n",
      "epoch: 3421 / loss: 0.03727300465106964 / accuracy: 1.0\n",
      "epoch: 3422 / loss: 0.03725873678922653 / accuracy: 1.0\n",
      "epoch: 3423 / loss: 0.03724434971809387 / accuracy: 1.0\n",
      "epoch: 3424 / loss: 0.03723008185625076 / accuracy: 1.0\n",
      "epoch: 3425 / loss: 0.03721587359905243 / accuracy: 1.0\n",
      "epoch: 3426 / loss: 0.03720167279243469 / accuracy: 1.0\n",
      "epoch: 3427 / loss: 0.03718734532594681 / accuracy: 1.0\n",
      "epoch: 3428 / loss: 0.037173137068748474 / accuracy: 1.0\n",
      "epoch: 3429 / loss: 0.03715893253684044 / accuracy: 1.0\n",
      "epoch: 3430 / loss: 0.037144728004932404 / accuracy: 1.0\n",
      "epoch: 3431 / loss: 0.03713052347302437 / accuracy: 1.0\n",
      "epoch: 3432 / loss: 0.037116196006536484 / accuracy: 1.0\n",
      "epoch: 3433 / loss: 0.037102051079273224 / accuracy: 1.0\n",
      "epoch: 3434 / loss: 0.037087906152009964 / accuracy: 1.0\n",
      "epoch: 3435 / loss: 0.037073761224746704 / accuracy: 1.0\n",
      "epoch: 3436 / loss: 0.037059612572193146 / accuracy: 1.0\n",
      "epoch: 3437 / loss: 0.037045471370220184 / accuracy: 1.0\n",
      "epoch: 3438 / loss: 0.037031322717666626 / accuracy: 1.0\n",
      "epoch: 3439 / loss: 0.037017177790403366 / accuracy: 1.0\n",
      "epoch: 3440 / loss: 0.03700309246778488 / accuracy: 1.0\n",
      "epoch: 3441 / loss: 0.03698895126581192 / accuracy: 1.0\n",
      "epoch: 3442 / loss: 0.03697480633854866 / accuracy: 1.0\n",
      "epoch: 3443 / loss: 0.03696078062057495 / accuracy: 1.0\n",
      "epoch: 3444 / loss: 0.036946699023246765 / accuracy: 1.0\n",
      "epoch: 3445 / loss: 0.03693260997533798 / accuracy: 1.0\n",
      "epoch: 3446 / loss: 0.03691864758729935 / accuracy: 1.0\n",
      "epoch: 3447 / loss: 0.03690462186932564 / accuracy: 1.0\n",
      "epoch: 3448 / loss: 0.03689054027199745 / accuracy: 1.0\n",
      "epoch: 3449 / loss: 0.03687645494937897 / accuracy: 1.0\n",
      "epoch: 3450 / loss: 0.03686237335205078 / accuracy: 1.0\n",
      "epoch: 3451 / loss: 0.0368485301733017 / accuracy: 1.0\n",
      "epoch: 3452 / loss: 0.036834508180618286 / accuracy: 1.0\n",
      "epoch: 3453 / loss: 0.03682054206728935 / accuracy: 1.0\n",
      "epoch: 3454 / loss: 0.036806635558605194 / accuracy: 1.0\n",
      "epoch: 3455 / loss: 0.03679267317056656 / accuracy: 1.0\n",
      "epoch: 3456 / loss: 0.03677865117788315 / accuracy: 1.0\n",
      "epoch: 3457 / loss: 0.03676474839448929 / accuracy: 1.0\n",
      "epoch: 3458 / loss: 0.03675084188580513 / accuracy: 1.0\n",
      "epoch: 3459 / loss: 0.036736879497766495 / accuracy: 1.0\n",
      "epoch: 3460 / loss: 0.036722976714372635 / accuracy: 1.0\n",
      "epoch: 3461 / loss: 0.03670913353562355 / accuracy: 1.0\n",
      "epoch: 3462 / loss: 0.03669523075222969 / accuracy: 1.0\n",
      "epoch: 3463 / loss: 0.03668144717812538 / accuracy: 1.0\n",
      "epoch: 3464 / loss: 0.036667484790086746 / accuracy: 1.0\n",
      "epoch: 3465 / loss: 0.036653582006692886 / accuracy: 1.0\n",
      "epoch: 3466 / loss: 0.0366397425532341 / accuracy: 1.0\n",
      "epoch: 3467 / loss: 0.03662595525383949 / accuracy: 1.0\n",
      "epoch: 3468 / loss: 0.03661211580038071 / accuracy: 1.0\n",
      "epoch: 3469 / loss: 0.03659821301698685 / accuracy: 1.0\n",
      "epoch: 3470 / loss: 0.03658454865217209 / accuracy: 1.0\n",
      "epoch: 3471 / loss: 0.03657064959406853 / accuracy: 1.0\n",
      "epoch: 3472 / loss: 0.036556925624608994 / accuracy: 1.0\n",
      "epoch: 3473 / loss: 0.03654314577579498 / accuracy: 1.0\n",
      "epoch: 3474 / loss: 0.03652942180633545 / accuracy: 1.0\n",
      "epoch: 3475 / loss: 0.036515578627586365 / accuracy: 1.0\n",
      "epoch: 3476 / loss: 0.036501914262771606 / accuracy: 1.0\n",
      "epoch: 3477 / loss: 0.036488138139247894 / accuracy: 1.0\n",
      "epoch: 3478 / loss: 0.03647453337907791 / accuracy: 1.0\n",
      "epoch: 3479 / loss: 0.0364607535302639 / accuracy: 1.0\n",
      "epoch: 3480 / loss: 0.03644697368144989 / accuracy: 1.0\n",
      "epoch: 3481 / loss: 0.036433249711990356 / accuracy: 1.0\n",
      "epoch: 3482 / loss: 0.03641964867711067 / accuracy: 1.0\n",
      "epoch: 3483 / loss: 0.03640598803758621 / accuracy: 1.0\n",
      "epoch: 3484 / loss: 0.03639232739806175 / accuracy: 1.0\n",
      "epoch: 3485 / loss: 0.03637866675853729 / accuracy: 1.0\n",
      "epoch: 3486 / loss: 0.03636494278907776 / accuracy: 1.0\n",
      "epoch: 3487 / loss: 0.036351464688777924 / accuracy: 1.0\n",
      "epoch: 3488 / loss: 0.036337800323963165 / accuracy: 1.0\n",
      "epoch: 3489 / loss: 0.03632420301437378 / accuracy: 1.0\n",
      "epoch: 3490 / loss: 0.036310479044914246 / accuracy: 1.0\n",
      "epoch: 3491 / loss: 0.036297060549259186 / accuracy: 1.0\n",
      "epoch: 3492 / loss: 0.0362834595143795 / accuracy: 1.0\n",
      "epoch: 3493 / loss: 0.03626979887485504 / accuracy: 1.0\n",
      "epoch: 3494 / loss: 0.03625619783997536 / accuracy: 1.0\n",
      "epoch: 3495 / loss: 0.036242902278900146 / accuracy: 1.0\n",
      "epoch: 3496 / loss: 0.03622917830944061 / accuracy: 1.0\n",
      "epoch: 3497 / loss: 0.036215640604496 / accuracy: 1.0\n",
      "epoch: 3498 / loss: 0.03620215877890587 / accuracy: 1.0\n",
      "epoch: 3499 / loss: 0.036188557744026184 / accuracy: 1.0\n",
      "epoch: 3500 / loss: 0.03617507964372635 / accuracy: 1.0\n",
      "epoch: 3501 / loss: 0.03616165742278099 / accuracy: 1.0\n",
      "epoch: 3502 / loss: 0.036148060113191605 / accuracy: 1.0\n",
      "epoch: 3503 / loss: 0.036134637892246246 / accuracy: 1.0\n",
      "epoch: 3504 / loss: 0.036121219396591187 / accuracy: 1.0\n",
      "epoch: 3505 / loss: 0.03610774129629135 / accuracy: 1.0\n",
      "epoch: 3506 / loss: 0.03609420359134674 / accuracy: 1.0\n",
      "epoch: 3507 / loss: 0.03608084097504616 / accuracy: 1.0\n",
      "epoch: 3508 / loss: 0.0360674224793911 / accuracy: 1.0\n",
      "epoch: 3509 / loss: 0.03605412319302559 / accuracy: 1.0\n",
      "epoch: 3510 / loss: 0.036040764302015305 / accuracy: 1.0\n",
      "epoch: 3511 / loss: 0.03602716326713562 / accuracy: 1.0\n",
      "epoch: 3512 / loss: 0.03601386770606041 / accuracy: 1.0\n",
      "epoch: 3513 / loss: 0.036000385880470276 / accuracy: 1.0\n",
      "epoch: 3514 / loss: 0.03598714619874954 / accuracy: 1.0\n",
      "epoch: 3515 / loss: 0.035973791033029556 / accuracy: 1.0\n",
      "epoch: 3516 / loss: 0.035960372537374496 / accuracy: 1.0\n",
      "epoch: 3517 / loss: 0.03594701364636421 / accuracy: 1.0\n",
      "epoch: 3518 / loss: 0.0359337143599987 / accuracy: 1.0\n",
      "epoch: 3519 / loss: 0.03592047840356827 / accuracy: 1.0\n",
      "epoch: 3520 / loss: 0.03590705990791321 / accuracy: 1.0\n",
      "epoch: 3521 / loss: 0.035893820226192474 / accuracy: 1.0\n",
      "epoch: 3522 / loss: 0.035880520939826965 / accuracy: 1.0\n",
      "epoch: 3523 / loss: 0.03586728125810623 / accuracy: 1.0\n",
      "epoch: 3524 / loss: 0.035854045301675797 / accuracy: 1.0\n",
      "epoch: 3525 / loss: 0.03584086894989014 / accuracy: 1.0\n",
      "epoch: 3526 / loss: 0.03582756966352463 / accuracy: 1.0\n",
      "epoch: 3527 / loss: 0.03581421077251434 / accuracy: 1.0\n",
      "epoch: 3528 / loss: 0.035801030695438385 / accuracy: 1.0\n",
      "epoch: 3529 / loss: 0.03578779473900795 / accuracy: 1.0\n",
      "epoch: 3530 / loss: 0.035774558782577515 / accuracy: 1.0\n",
      "epoch: 3531 / loss: 0.035761378705501556 / accuracy: 1.0\n",
      "epoch: 3532 / loss: 0.03574814274907112 / accuracy: 1.0\n",
      "epoch: 3533 / loss: 0.03573496639728546 / accuracy: 1.0\n",
      "epoch: 3534 / loss: 0.03572184592485428 / accuracy: 1.0\n",
      "epoch: 3535 / loss: 0.03570873290300369 / accuracy: 1.0\n",
      "epoch: 3536 / loss: 0.03569573536515236 / accuracy: 1.0\n",
      "epoch: 3537 / loss: 0.03568243607878685 / accuracy: 1.0\n",
      "epoch: 3538 / loss: 0.03566925972700119 / accuracy: 1.0\n",
      "epoch: 3539 / loss: 0.035656142979860306 / accuracy: 1.0\n",
      "epoch: 3540 / loss: 0.03564302623271942 / accuracy: 1.0\n",
      "epoch: 3541 / loss: 0.03562990576028824 / accuracy: 1.0\n",
      "epoch: 3542 / loss: 0.0356166735291481 / accuracy: 1.0\n",
      "epoch: 3543 / loss: 0.035603735595941544 / accuracy: 1.0\n",
      "epoch: 3544 / loss: 0.035590678453445435 / accuracy: 1.0\n",
      "epoch: 3545 / loss: 0.035577625036239624 / accuracy: 1.0\n",
      "epoch: 3546 / loss: 0.03556450456380844 / accuracy: 1.0\n",
      "epoch: 3547 / loss: 0.03555145114660263 / accuracy: 1.0\n",
      "epoch: 3548 / loss: 0.03553851321339607 / accuracy: 1.0\n",
      "epoch: 3549 / loss: 0.03552539646625519 / accuracy: 1.0\n",
      "epoch: 3550 / loss: 0.03551240265369415 / accuracy: 1.0\n",
      "epoch: 3551 / loss: 0.035499464720487595 / accuracy: 1.0\n",
      "epoch: 3552 / loss: 0.035486288368701935 / accuracy: 1.0\n",
      "epoch: 3553 / loss: 0.03547341376543045 / accuracy: 1.0\n",
      "epoch: 3554 / loss: 0.03546047955751419 / accuracy: 1.0\n",
      "epoch: 3555 / loss: 0.03544742241501808 / accuracy: 1.0\n",
      "epoch: 3556 / loss: 0.03543442487716675 / accuracy: 1.0\n",
      "epoch: 3557 / loss: 0.035421550273895264 / accuracy: 1.0\n",
      "epoch: 3558 / loss: 0.03540867567062378 / accuracy: 1.0\n",
      "epoch: 3559 / loss: 0.03539561852812767 / accuracy: 1.0\n",
      "epoch: 3560 / loss: 0.03538268059492111 / accuracy: 1.0\n",
      "epoch: 3561 / loss: 0.03536974638700485 / accuracy: 1.0\n",
      "epoch: 3562 / loss: 0.03535687178373337 / accuracy: 1.0\n",
      "epoch: 3563 / loss: 0.03534393757581711 / accuracy: 1.0\n",
      "epoch: 3564 / loss: 0.0353311225771904 / accuracy: 1.0\n",
      "epoch: 3565 / loss: 0.035318128764629364 / accuracy: 1.0\n",
      "epoch: 3566 / loss: 0.03530537337064743 / accuracy: 1.0\n",
      "epoch: 3567 / loss: 0.03529231622815132 / accuracy: 1.0\n",
      "epoch: 3568 / loss: 0.03527968376874924 / accuracy: 1.0\n",
      "epoch: 3569 / loss: 0.035266805440187454 / accuracy: 1.0\n",
      "epoch: 3570 / loss: 0.03525387495756149 / accuracy: 1.0\n",
      "epoch: 3571 / loss: 0.035241059958934784 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3572 / loss: 0.03522830456495285 / accuracy: 1.0\n",
      "epoch: 3573 / loss: 0.03521548956632614 / accuracy: 1.0\n",
      "epoch: 3574 / loss: 0.03520261496305466 / accuracy: 1.0\n",
      "epoch: 3575 / loss: 0.0351899191737175 / accuracy: 1.0\n",
      "epoch: 3576 / loss: 0.03517710417509079 / accuracy: 1.0\n",
      "epoch: 3577 / loss: 0.035164352506399155 / accuracy: 1.0\n",
      "epoch: 3578 / loss: 0.03515147790312767 / accuracy: 1.0\n",
      "epoch: 3579 / loss: 0.03513878211379051 / accuracy: 1.0\n",
      "epoch: 3580 / loss: 0.03512609004974365 / accuracy: 1.0\n",
      "epoch: 3581 / loss: 0.03511333465576172 / accuracy: 1.0\n",
      "epoch: 3582 / loss: 0.03510063886642456 / accuracy: 1.0\n",
      "epoch: 3583 / loss: 0.03508788347244263 / accuracy: 1.0\n",
      "epoch: 3584 / loss: 0.03507525473833084 / accuracy: 1.0\n",
      "epoch: 3585 / loss: 0.03506249934434891 / accuracy: 1.0\n",
      "epoch: 3586 / loss: 0.03504980355501175 / accuracy: 1.0\n",
      "epoch: 3587 / loss: 0.03503711149096489 / accuracy: 1.0\n",
      "epoch: 3588 / loss: 0.03502453863620758 / accuracy: 1.0\n",
      "epoch: 3589 / loss: 0.035011906176805496 / accuracy: 1.0\n",
      "epoch: 3590 / loss: 0.03499927371740341 / accuracy: 1.0\n",
      "epoch: 3591 / loss: 0.0349864587187767 / accuracy: 1.0\n",
      "epoch: 3592 / loss: 0.034973885864019394 / accuracy: 1.0\n",
      "epoch: 3593 / loss: 0.03496125340461731 / accuracy: 1.0\n",
      "epoch: 3594 / loss: 0.03494855761528015 / accuracy: 1.0\n",
      "epoch: 3595 / loss: 0.034935928881168365 / accuracy: 1.0\n",
      "epoch: 3596 / loss: 0.03492341190576553 / accuracy: 1.0\n",
      "epoch: 3597 / loss: 0.034910719841718674 / accuracy: 1.0\n",
      "epoch: 3598 / loss: 0.034898146986961365 / accuracy: 1.0\n",
      "epoch: 3599 / loss: 0.03488563373684883 / accuracy: 1.0\n",
      "epoch: 3600 / loss: 0.03487306088209152 / accuracy: 1.0\n",
      "epoch: 3601 / loss: 0.03486049175262451 / accuracy: 1.0\n",
      "epoch: 3602 / loss: 0.034847915172576904 / accuracy: 1.0\n",
      "epoch: 3603 / loss: 0.03483528271317482 / accuracy: 1.0\n",
      "epoch: 3604 / loss: 0.03482288867235184 / accuracy: 1.0\n",
      "epoch: 3605 / loss: 0.03481031954288483 / accuracy: 1.0\n",
      "epoch: 3606 / loss: 0.03479792922735214 / accuracy: 1.0\n",
      "epoch: 3607 / loss: 0.03478541225194931 / accuracy: 1.0\n",
      "epoch: 3608 / loss: 0.034772902727127075 / accuracy: 1.0\n",
      "epoch: 3609 / loss: 0.034760329872369766 / accuracy: 1.0\n",
      "epoch: 3610 / loss: 0.03474781662225723 / accuracy: 1.0\n",
      "epoch: 3611 / loss: 0.0347355455160141 / accuracy: 1.0\n",
      "epoch: 3612 / loss: 0.034723032265901566 / accuracy: 1.0\n",
      "epoch: 3613 / loss: 0.034710582345724106 / accuracy: 1.0\n",
      "epoch: 3614 / loss: 0.03469806909561157 / accuracy: 1.0\n",
      "epoch: 3615 / loss: 0.03468561917543411 / accuracy: 1.0\n",
      "epoch: 3616 / loss: 0.03467322885990143 / accuracy: 1.0\n",
      "epoch: 3617 / loss: 0.03466089442372322 / accuracy: 1.0\n",
      "epoch: 3618 / loss: 0.03464844450354576 / accuracy: 1.0\n",
      "epoch: 3619 / loss: 0.034636110067367554 / accuracy: 1.0\n",
      "epoch: 3620 / loss: 0.03462360054254532 / accuracy: 1.0\n",
      "epoch: 3621 / loss: 0.034611329436302185 / accuracy: 1.0\n",
      "epoch: 3622 / loss: 0.034598879516124725 / accuracy: 1.0\n",
      "epoch: 3623 / loss: 0.03458642587065697 / accuracy: 1.0\n",
      "epoch: 3624 / loss: 0.03457409515976906 / accuracy: 1.0\n",
      "epoch: 3625 / loss: 0.03456176444888115 / accuracy: 1.0\n",
      "epoch: 3626 / loss: 0.03454955667257309 / accuracy: 1.0\n",
      "epoch: 3627 / loss: 0.034537043422460556 / accuracy: 1.0\n",
      "epoch: 3628 / loss: 0.03452489152550697 / accuracy: 1.0\n",
      "epoch: 3629 / loss: 0.034512560814619064 / accuracy: 1.0\n",
      "epoch: 3630 / loss: 0.034500230103731155 / accuracy: 1.0\n",
      "epoch: 3631 / loss: 0.03448783978819847 / accuracy: 1.0\n",
      "epoch: 3632 / loss: 0.03447568789124489 / accuracy: 1.0\n",
      "epoch: 3633 / loss: 0.03446323797106743 / accuracy: 1.0\n",
      "epoch: 3634 / loss: 0.03445102646946907 / accuracy: 1.0\n",
      "epoch: 3635 / loss: 0.034438878297805786 / accuracy: 1.0\n",
      "epoch: 3636 / loss: 0.03442660719156265 / accuracy: 1.0\n",
      "epoch: 3637 / loss: 0.034414276480674744 / accuracy: 1.0\n",
      "epoch: 3638 / loss: 0.03440212458372116 / accuracy: 1.0\n",
      "epoch: 3639 / loss: 0.03438979387283325 / accuracy: 1.0\n",
      "epoch: 3640 / loss: 0.03437764570116997 / accuracy: 1.0\n",
      "epoch: 3641 / loss: 0.03436549752950668 / accuracy: 1.0\n",
      "epoch: 3642 / loss: 0.03435322642326355 / accuracy: 1.0\n",
      "epoch: 3643 / loss: 0.03434113413095474 / accuracy: 1.0\n",
      "epoch: 3644 / loss: 0.03432886302471161 / accuracy: 1.0\n",
      "epoch: 3645 / loss: 0.03431665524840355 / accuracy: 1.0\n",
      "epoch: 3646 / loss: 0.03430456668138504 / accuracy: 1.0\n",
      "epoch: 3647 / loss: 0.03429229557514191 / accuracy: 1.0\n",
      "epoch: 3648 / loss: 0.034280143678188324 / accuracy: 1.0\n",
      "epoch: 3649 / loss: 0.034268055111169815 / accuracy: 1.0\n",
      "epoch: 3650 / loss: 0.034255966544151306 / accuracy: 1.0\n",
      "epoch: 3651 / loss: 0.03424375504255295 / accuracy: 1.0\n",
      "epoch: 3652 / loss: 0.03423166647553444 / accuracy: 1.0\n",
      "epoch: 3653 / loss: 0.034219518303871155 / accuracy: 1.0\n",
      "epoch: 3654 / loss: 0.03420742601156235 / accuracy: 1.0\n",
      "epoch: 3655 / loss: 0.03419540077447891 / accuracy: 1.0\n",
      "epoch: 3656 / loss: 0.03418336808681488 / accuracy: 1.0\n",
      "epoch: 3657 / loss: 0.03417128324508667 / accuracy: 1.0\n",
      "epoch: 3658 / loss: 0.03415913134813309 / accuracy: 1.0\n",
      "epoch: 3659 / loss: 0.03414704278111458 / accuracy: 1.0\n",
      "epoch: 3660 / loss: 0.03413507342338562 / accuracy: 1.0\n",
      "epoch: 3661 / loss: 0.03412292152643204 / accuracy: 1.0\n",
      "epoch: 3662 / loss: 0.03411101549863815 / accuracy: 1.0\n",
      "epoch: 3663 / loss: 0.03409898653626442 / accuracy: 1.0\n",
      "epoch: 3664 / loss: 0.034086838364601135 / accuracy: 1.0\n",
      "epoch: 3665 / loss: 0.03407486900687218 / accuracy: 1.0\n",
      "epoch: 3666 / loss: 0.034062959253787994 / accuracy: 1.0\n",
      "epoch: 3667 / loss: 0.03405105322599411 / accuracy: 1.0\n",
      "epoch: 3668 / loss: 0.0340389646589756 / accuracy: 1.0\n",
      "epoch: 3669 / loss: 0.03402693569660187 / accuracy: 1.0\n",
      "epoch: 3670 / loss: 0.03401508554816246 / accuracy: 1.0\n",
      "epoch: 3671 / loss: 0.03400299698114395 / accuracy: 1.0\n",
      "epoch: 3672 / loss: 0.03399115055799484 / accuracy: 1.0\n",
      "epoch: 3673 / loss: 0.033979300409555435 / accuracy: 1.0\n",
      "epoch: 3674 / loss: 0.03396715223789215 / accuracy: 1.0\n",
      "epoch: 3675 / loss: 0.03395536541938782 / accuracy: 1.0\n",
      "epoch: 3676 / loss: 0.03394351899623871 / accuracy: 1.0\n",
      "epoch: 3677 / loss: 0.033931367099285126 / accuracy: 1.0\n",
      "epoch: 3678 / loss: 0.03391958028078079 / accuracy: 1.0\n",
      "epoch: 3679 / loss: 0.03390767425298691 / accuracy: 1.0\n",
      "epoch: 3680 / loss: 0.033895887434482574 / accuracy: 1.0\n",
      "epoch: 3681 / loss: 0.03388385474681854 / accuracy: 1.0\n",
      "epoch: 3682 / loss: 0.03387207165360451 / accuracy: 1.0\n",
      "epoch: 3683 / loss: 0.033860161900520325 / accuracy: 1.0\n",
      "epoch: 3684 / loss: 0.033848315477371216 / accuracy: 1.0\n",
      "epoch: 3685 / loss: 0.03383664786815643 / accuracy: 1.0\n",
      "epoch: 3686 / loss: 0.033824678510427475 / accuracy: 1.0\n",
      "epoch: 3687 / loss: 0.033812955021858215 / accuracy: 1.0\n",
      "epoch: 3688 / loss: 0.03380104526877403 / accuracy: 1.0\n",
      "epoch: 3689 / loss: 0.0337892547249794 / accuracy: 1.0\n",
      "epoch: 3690 / loss: 0.033777348697185516 / accuracy: 1.0\n",
      "epoch: 3691 / loss: 0.03376556187868118 / accuracy: 1.0\n",
      "epoch: 3692 / loss: 0.0337538942694664 / accuracy: 1.0\n",
      "epoch: 3693 / loss: 0.03374216705560684 / accuracy: 1.0\n",
      "epoch: 3694 / loss: 0.03373026102781296 / accuracy: 1.0\n",
      "epoch: 3695 / loss: 0.033718593418598175 / accuracy: 1.0\n",
      "epoch: 3696 / loss: 0.033706746995449066 / accuracy: 1.0\n",
      "epoch: 3697 / loss: 0.03369496017694473 / accuracy: 1.0\n",
      "epoch: 3698 / loss: 0.03368329629302025 / accuracy: 1.0\n",
      "epoch: 3699 / loss: 0.033671505749225616 / accuracy: 1.0\n",
      "epoch: 3700 / loss: 0.03365990146994591 / accuracy: 1.0\n",
      "epoch: 3701 / loss: 0.033648233860731125 / accuracy: 1.0\n",
      "epoch: 3702 / loss: 0.03363632783293724 / accuracy: 1.0\n",
      "epoch: 3703 / loss: 0.03362471982836723 / accuracy: 1.0\n",
      "epoch: 3704 / loss: 0.03361305594444275 / accuracy: 1.0\n",
      "epoch: 3705 / loss: 0.03360132873058319 / accuracy: 1.0\n",
      "epoch: 3706 / loss: 0.03358966112136841 / accuracy: 1.0\n",
      "epoch: 3707 / loss: 0.03357817605137825 / accuracy: 1.0\n",
      "epoch: 3708 / loss: 0.03356638923287392 / accuracy: 1.0\n",
      "epoch: 3709 / loss: 0.03355478495359421 / accuracy: 1.0\n",
      "epoch: 3710 / loss: 0.03354305773973465 / accuracy: 1.0\n",
      "epoch: 3711 / loss: 0.03353139013051987 / accuracy: 1.0\n",
      "epoch: 3712 / loss: 0.03351978212594986 / accuracy: 1.0\n",
      "epoch: 3713 / loss: 0.0335080586373806 / accuracy: 1.0\n",
      "epoch: 3714 / loss: 0.033496513962745667 / accuracy: 1.0\n",
      "epoch: 3715 / loss: 0.03348490595817566 / accuracy: 1.0\n",
      "epoch: 3716 / loss: 0.03347330540418625 / accuracy: 1.0\n",
      "epoch: 3717 / loss: 0.03346169367432594 / accuracy: 1.0\n",
      "epoch: 3718 / loss: 0.03345014899969101 / accuracy: 1.0\n",
      "epoch: 3719 / loss: 0.03343872353434563 / accuracy: 1.0\n",
      "epoch: 3720 / loss: 0.03342711925506592 / accuracy: 1.0\n",
      "epoch: 3721 / loss: 0.03341563045978546 / accuracy: 1.0\n",
      "epoch: 3722 / loss: 0.03340396657586098 / accuracy: 1.0\n",
      "epoch: 3723 / loss: 0.033392421901226044 / accuracy: 1.0\n",
      "epoch: 3724 / loss: 0.03338093310594559 / accuracy: 1.0\n",
      "epoch: 3725 / loss: 0.03336939215660095 / accuracy: 1.0\n",
      "epoch: 3726 / loss: 0.03335784375667572 / accuracy: 1.0\n",
      "epoch: 3727 / loss: 0.033346422016620636 / accuracy: 1.0\n",
      "epoch: 3728 / loss: 0.033334873616695404 / accuracy: 1.0\n",
      "epoch: 3729 / loss: 0.03332332894206047 / accuracy: 1.0\n",
      "epoch: 3730 / loss: 0.03331184387207031 / accuracy: 1.0\n",
      "epoch: 3731 / loss: 0.03330041840672493 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3732 / loss: 0.03328893333673477 / accuracy: 1.0\n",
      "epoch: 3733 / loss: 0.03327738866209984 / accuracy: 1.0\n",
      "epoch: 3734 / loss: 0.03326614201068878 / accuracy: 1.0\n",
      "epoch: 3735 / loss: 0.03325460106134415 / accuracy: 1.0\n",
      "epoch: 3736 / loss: 0.03324311599135399 / accuracy: 1.0\n",
      "epoch: 3737 / loss: 0.03323163092136383 / accuracy: 1.0\n",
      "epoch: 3738 / loss: 0.03322026506066322 / accuracy: 1.0\n",
      "epoch: 3739 / loss: 0.033208779990673065 / accuracy: 1.0\n",
      "epoch: 3740 / loss: 0.03319753706455231 / accuracy: 1.0\n",
      "epoch: 3741 / loss: 0.03318605199456215 / accuracy: 1.0\n",
      "epoch: 3742 / loss: 0.03317456692457199 / accuracy: 1.0\n",
      "epoch: 3743 / loss: 0.03316332399845123 / accuracy: 1.0\n",
      "epoch: 3744 / loss: 0.03315189853310585 / accuracy: 1.0\n",
      "epoch: 3745 / loss: 0.03314053267240524 / accuracy: 1.0\n",
      "epoch: 3746 / loss: 0.033129047602415085 / accuracy: 1.0\n",
      "epoch: 3747 / loss: 0.033117808401584625 / accuracy: 1.0\n",
      "epoch: 3748 / loss: 0.03310643881559372 / accuracy: 1.0\n",
      "epoch: 3749 / loss: 0.03309519588947296 / accuracy: 1.0\n",
      "epoch: 3750 / loss: 0.03308377042412758 / accuracy: 1.0\n",
      "epoch: 3751 / loss: 0.033072471618652344 / accuracy: 1.0\n",
      "epoch: 3752 / loss: 0.03306116536259651 / accuracy: 1.0\n",
      "epoch: 3753 / loss: 0.033049799501895905 / accuracy: 1.0\n",
      "epoch: 3754 / loss: 0.03303861618041992 / accuracy: 1.0\n",
      "epoch: 3755 / loss: 0.03302713483572006 / accuracy: 1.0\n",
      "epoch: 3756 / loss: 0.033016011118888855 / accuracy: 1.0\n",
      "epoch: 3757 / loss: 0.03300464525818825 / accuracy: 1.0\n",
      "epoch: 3758 / loss: 0.03299340233206749 / accuracy: 1.0\n",
      "epoch: 3759 / loss: 0.03298215940594673 / accuracy: 1.0\n",
      "epoch: 3760 / loss: 0.032970916479825974 / accuracy: 1.0\n",
      "epoch: 3761 / loss: 0.032959550619125366 / accuracy: 1.0\n",
      "epoch: 3762 / loss: 0.032948486506938934 / accuracy: 1.0\n",
      "epoch: 3763 / loss: 0.03293706476688385 / accuracy: 1.0\n",
      "epoch: 3764 / loss: 0.03292594105005264 / accuracy: 1.0\n",
      "epoch: 3765 / loss: 0.032914817333221436 / accuracy: 1.0\n",
      "epoch: 3766 / loss: 0.03290345519781113 / accuracy: 1.0\n",
      "epoch: 3767 / loss: 0.032892391085624695 / accuracy: 1.0\n",
      "epoch: 3768 / loss: 0.032881028950214386 / accuracy: 1.0\n",
      "epoch: 3769 / loss: 0.03286990523338318 / accuracy: 1.0\n",
      "epoch: 3770 / loss: 0.03285884112119675 / accuracy: 1.0\n",
      "epoch: 3771 / loss: 0.03284747898578644 / accuracy: 1.0\n",
      "epoch: 3772 / loss: 0.032836414873600006 / accuracy: 1.0\n",
      "epoch: 3773 / loss: 0.03282517194747925 / accuracy: 1.0\n",
      "epoch: 3774 / loss: 0.03281404823064804 / accuracy: 1.0\n",
      "epoch: 3775 / loss: 0.032802924513816833 / accuracy: 1.0\n",
      "epoch: 3776 / loss: 0.032791804522275925 / accuracy: 1.0\n",
      "epoch: 3777 / loss: 0.03278068080544472 / accuracy: 1.0\n",
      "epoch: 3778 / loss: 0.032769616693258286 / accuracy: 1.0\n",
      "epoch: 3779 / loss: 0.0327584370970726 / accuracy: 1.0\n",
      "epoch: 3780 / loss: 0.03274737298488617 / accuracy: 1.0\n",
      "epoch: 3781 / loss: 0.03273624926805496 / accuracy: 1.0\n",
      "epoch: 3782 / loss: 0.032725125551223755 / accuracy: 1.0\n",
      "epoch: 3783 / loss: 0.032714128494262695 / accuracy: 1.0\n",
      "epoch: 3784 / loss: 0.03270294517278671 / accuracy: 1.0\n",
      "epoch: 3785 / loss: 0.03269200026988983 / accuracy: 1.0\n",
      "epoch: 3786 / loss: 0.0326809398829937 / accuracy: 1.0\n",
      "epoch: 3787 / loss: 0.03266981989145279 / accuracy: 1.0\n",
      "epoch: 3788 / loss: 0.03265875577926636 / accuracy: 1.0\n",
      "epoch: 3789 / loss: 0.032647691667079926 / accuracy: 1.0\n",
      "epoch: 3790 / loss: 0.03263656795024872 / accuracy: 1.0\n",
      "epoch: 3791 / loss: 0.032625630497932434 / accuracy: 1.0\n",
      "epoch: 3792 / loss: 0.03261450678110123 / accuracy: 1.0\n",
      "epoch: 3793 / loss: 0.032603684812784195 / accuracy: 1.0\n",
      "epoch: 3794 / loss: 0.03259262442588806 / accuracy: 1.0\n",
      "epoch: 3795 / loss: 0.032581619918346405 / accuracy: 1.0\n",
      "epoch: 3796 / loss: 0.03257061913609505 / accuracy: 1.0\n",
      "epoch: 3797 / loss: 0.03255961835384369 / accuracy: 1.0\n",
      "epoch: 3798 / loss: 0.03254867345094681 / accuracy: 1.0\n",
      "epoch: 3799 / loss: 0.032537855207920074 / accuracy: 1.0\n",
      "epoch: 3800 / loss: 0.03252673149108887 / accuracy: 1.0\n",
      "epoch: 3801 / loss: 0.032515786588191986 / accuracy: 1.0\n",
      "epoch: 3802 / loss: 0.032504789531230927 / accuracy: 1.0\n",
      "epoch: 3803 / loss: 0.03249390795826912 / accuracy: 1.0\n",
      "epoch: 3804 / loss: 0.03248302638530731 / accuracy: 1.0\n",
      "epoch: 3805 / loss: 0.03247196227312088 / accuracy: 1.0\n",
      "epoch: 3806 / loss: 0.03246120363473892 / accuracy: 1.0\n",
      "epoch: 3807 / loss: 0.032450199127197266 / accuracy: 1.0\n",
      "epoch: 3808 / loss: 0.03243926167488098 / accuracy: 1.0\n",
      "epoch: 3809 / loss: 0.032428495585918427 / accuracy: 1.0\n",
      "epoch: 3810 / loss: 0.03241761773824692 / accuracy: 1.0\n",
      "epoch: 3811 / loss: 0.03240667283535004 / accuracy: 1.0\n",
      "epoch: 3812 / loss: 0.03239573538303375 / accuracy: 1.0\n",
      "epoch: 3813 / loss: 0.032384853810071945 / accuracy: 1.0\n",
      "epoch: 3814 / loss: 0.03237421438097954 / accuracy: 1.0\n",
      "epoch: 3815 / loss: 0.03236320987343788 / accuracy: 1.0\n",
      "epoch: 3816 / loss: 0.03235239163041115 / accuracy: 1.0\n",
      "epoch: 3817 / loss: 0.03234151005744934 / accuracy: 1.0\n",
      "epoch: 3818 / loss: 0.03233063220977783 / accuracy: 1.0\n",
      "epoch: 3819 / loss: 0.0323198065161705 / accuracy: 1.0\n",
      "epoch: 3820 / loss: 0.03230910748243332 / accuracy: 1.0\n",
      "epoch: 3821 / loss: 0.032298289239406586 / accuracy: 1.0\n",
      "epoch: 3822 / loss: 0.03228740766644478 / accuracy: 1.0\n",
      "epoch: 3823 / loss: 0.03227664530277252 / accuracy: 1.0\n",
      "epoch: 3824 / loss: 0.03226582333445549 / accuracy: 1.0\n",
      "epoch: 3825 / loss: 0.03225506469607353 / accuracy: 1.0\n",
      "epoch: 3826 / loss: 0.03224436193704605 / accuracy: 1.0\n",
      "epoch: 3827 / loss: 0.03223348408937454 / accuracy: 1.0\n",
      "epoch: 3828 / loss: 0.03222278505563736 / accuracy: 1.0\n",
      "epoch: 3829 / loss: 0.03221208602190018 / accuracy: 1.0\n",
      "epoch: 3830 / loss: 0.03220120444893837 / accuracy: 1.0\n",
      "epoch: 3831 / loss: 0.03219062089920044 / accuracy: 1.0\n",
      "epoch: 3832 / loss: 0.032179802656173706 / accuracy: 1.0\n",
      "epoch: 3833 / loss: 0.03216910362243652 / accuracy: 1.0\n",
      "epoch: 3834 / loss: 0.03215828537940979 / accuracy: 1.0\n",
      "epoch: 3835 / loss: 0.032147761434316635 / accuracy: 1.0\n",
      "epoch: 3836 / loss: 0.03213682025671005 / accuracy: 1.0\n",
      "epoch: 3837 / loss: 0.032126180827617645 / accuracy: 1.0\n",
      "epoch: 3838 / loss: 0.03211566060781479 / accuracy: 1.0\n",
      "epoch: 3839 / loss: 0.032104842364788055 / accuracy: 1.0\n",
      "epoch: 3840 / loss: 0.03209414333105087 / accuracy: 1.0\n",
      "epoch: 3841 / loss: 0.03208356350660324 / accuracy: 1.0\n",
      "epoch: 3842 / loss: 0.03207286447286606 / accuracy: 1.0\n",
      "epoch: 3843 / loss: 0.03206222504377365 / accuracy: 1.0\n",
      "epoch: 3844 / loss: 0.03205164521932602 / accuracy: 1.0\n",
      "epoch: 3845 / loss: 0.03204082325100899 / accuracy: 1.0\n",
      "epoch: 3846 / loss: 0.03203018382191658 / accuracy: 1.0\n",
      "epoch: 3847 / loss: 0.03201960399746895 / accuracy: 1.0\n",
      "epoch: 3848 / loss: 0.03200908377766609 / accuracy: 1.0\n",
      "epoch: 3849 / loss: 0.03199838474392891 / accuracy: 1.0\n",
      "epoch: 3850 / loss: 0.031987808644771576 / accuracy: 1.0\n",
      "epoch: 3851 / loss: 0.031977228820323944 / accuracy: 1.0\n",
      "epoch: 3852 / loss: 0.03196670860052109 / accuracy: 1.0\n",
      "epoch: 3853 / loss: 0.03195618838071823 / accuracy: 1.0\n",
      "epoch: 3854 / loss: 0.03194542974233627 / accuracy: 1.0\n",
      "epoch: 3855 / loss: 0.03193490952253342 / accuracy: 1.0\n",
      "epoch: 3856 / loss: 0.031924329698085785 / accuracy: 1.0\n",
      "epoch: 3857 / loss: 0.031913869082927704 / accuracy: 1.0\n",
      "epoch: 3858 / loss: 0.03190329298377037 / accuracy: 1.0\n",
      "epoch: 3859 / loss: 0.031892772763967514 / accuracy: 1.0\n",
      "epoch: 3860 / loss: 0.03188219666481018 / accuracy: 1.0\n",
      "epoch: 3861 / loss: 0.031871553510427475 / accuracy: 1.0\n",
      "epoch: 3862 / loss: 0.03186109662055969 / accuracy: 1.0\n",
      "epoch: 3863 / loss: 0.031850576400756836 / accuracy: 1.0\n",
      "epoch: 3864 / loss: 0.031839996576309204 / accuracy: 1.0\n",
      "epoch: 3865 / loss: 0.031829480081796646 / accuracy: 1.0\n",
      "epoch: 3866 / loss: 0.031819019466638565 / accuracy: 1.0\n",
      "epoch: 3867 / loss: 0.03180850297212601 / accuracy: 1.0\n",
      "epoch: 3868 / loss: 0.03179798275232315 / accuracy: 1.0\n",
      "epoch: 3869 / loss: 0.03178764134645462 / accuracy: 1.0\n",
      "epoch: 3870 / loss: 0.03177712857723236 / accuracy: 1.0\n",
      "epoch: 3871 / loss: 0.031766608357429504 / accuracy: 1.0\n",
      "epoch: 3872 / loss: 0.03175608813762665 / accuracy: 1.0\n",
      "epoch: 3873 / loss: 0.03174569085240364 / accuracy: 1.0\n",
      "epoch: 3874 / loss: 0.03173547238111496 / accuracy: 1.0\n",
      "epoch: 3875 / loss: 0.0317249521613121 / accuracy: 1.0\n",
      "epoch: 3876 / loss: 0.031714435666799545 / accuracy: 1.0\n",
      "epoch: 3877 / loss: 0.03170397877693176 / accuracy: 1.0\n",
      "epoch: 3878 / loss: 0.03169363737106323 / accuracy: 1.0\n",
      "epoch: 3879 / loss: 0.03168318048119545 / accuracy: 1.0\n",
      "epoch: 3880 / loss: 0.03167266026139259 / accuracy: 1.0\n",
      "epoch: 3881 / loss: 0.03166238218545914 / accuracy: 1.0\n",
      "epoch: 3882 / loss: 0.03165210783481598 / accuracy: 1.0\n",
      "epoch: 3883 / loss: 0.03164170682430267 / accuracy: 1.0\n",
      "epoch: 3884 / loss: 0.031631190329790115 / accuracy: 1.0\n",
      "epoch: 3885 / loss: 0.031620852649211884 / accuracy: 1.0\n",
      "epoch: 3886 / loss: 0.03161057457327843 / accuracy: 1.0\n",
      "epoch: 3887 / loss: 0.031600236892700195 / accuracy: 1.0\n",
      "epoch: 3888 / loss: 0.03158983960747719 / accuracy: 1.0\n",
      "epoch: 3889 / loss: 0.03157931938767433 / accuracy: 1.0\n",
      "epoch: 3890 / loss: 0.0315692238509655 / accuracy: 1.0\n",
      "epoch: 3891 / loss: 0.03155876696109772 / accuracy: 1.0\n",
      "epoch: 3892 / loss: 0.031548548489809036 / accuracy: 1.0\n",
      "epoch: 3893 / loss: 0.031538210809230804 / accuracy: 1.0\n",
      "epoch: 3894 / loss: 0.03152793273329735 / accuracy: 1.0\n",
      "epoch: 3895 / loss: 0.03151753544807434 / accuracy: 1.0\n",
      "epoch: 3896 / loss: 0.03150719404220581 / accuracy: 1.0\n",
      "epoch: 3897 / loss: 0.03149691969156265 / accuracy: 1.0\n",
      "epoch: 3898 / loss: 0.03148682042956352 / accuracy: 1.0\n",
      "epoch: 3899 / loss: 0.03147636353969574 / accuracy: 1.0\n",
      "epoch: 3900 / loss: 0.03146608918905258 / accuracy: 1.0\n",
      "epoch: 3901 / loss: 0.03145592659711838 / accuracy: 1.0\n",
      "epoch: 3902 / loss: 0.03144565224647522 / accuracy: 1.0\n",
      "epoch: 3903 / loss: 0.03143537789583206 / accuracy: 1.0\n",
      "epoch: 3904 / loss: 0.03142509609460831 / accuracy: 1.0\n",
      "epoch: 3905 / loss: 0.031414881348609924 / accuracy: 1.0\n",
      "epoch: 3906 / loss: 0.031404606997966766 / accuracy: 1.0\n",
      "epoch: 3907 / loss: 0.031394507735967636 / accuracy: 1.0\n",
      "epoch: 3908 / loss: 0.031384170055389404 / accuracy: 1.0\n",
      "epoch: 3909 / loss: 0.0313740149140358 / accuracy: 1.0\n",
      "epoch: 3910 / loss: 0.03136379271745682 / accuracy: 1.0\n",
      "epoch: 3911 / loss: 0.031353577971458435 / accuracy: 1.0\n",
      "epoch: 3912 / loss: 0.03134342283010483 / accuracy: 1.0\n",
      "epoch: 3913 / loss: 0.03133326396346092 / accuracy: 1.0\n",
      "epoch: 3914 / loss: 0.031322985887527466 / accuracy: 1.0\n",
      "epoch: 3915 / loss: 0.03131283074617386 / accuracy: 1.0\n",
      "epoch: 3916 / loss: 0.03130273520946503 / accuracy: 1.0\n",
      "epoch: 3917 / loss: 0.031292516738176346 / accuracy: 1.0\n",
      "epoch: 3918 / loss: 0.031282421201467514 / accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3919 / loss: 0.03127226233482361 / accuracy: 1.0\n",
      "epoch: 3920 / loss: 0.031262047588825226 / accuracy: 1.0\n",
      "epoch: 3921 / loss: 0.03125201165676117 / accuracy: 1.0\n",
      "epoch: 3922 / loss: 0.03124191425740719 / accuracy: 1.0\n",
      "epoch: 3923 / loss: 0.031231757253408432 / accuracy: 1.0\n",
      "epoch: 3924 / loss: 0.0312215406447649 / accuracy: 1.0\n",
      "epoch: 3925 / loss: 0.031211504712700844 / accuracy: 1.0\n",
      "epoch: 3926 / loss: 0.031201409175992012 / accuracy: 1.0\n",
      "epoch: 3927 / loss: 0.03119131177663803 / accuracy: 1.0\n",
      "epoch: 3928 / loss: 0.0311812162399292 / accuracy: 1.0\n",
      "epoch: 3929 / loss: 0.031171180307865143 / accuracy: 1.0\n",
      "epoch: 3930 / loss: 0.031161144375801086 / accuracy: 1.0\n",
      "epoch: 3931 / loss: 0.03115110844373703 / accuracy: 1.0\n",
      "epoch: 3932 / loss: 0.031140830367803574 / accuracy: 1.0\n",
      "epoch: 3933 / loss: 0.031130915507674217 / accuracy: 1.0\n",
      "epoch: 3934 / loss: 0.03112087771296501 / accuracy: 1.0\n",
      "epoch: 3935 / loss: 0.03111066296696663 / accuracy: 1.0\n",
      "epoch: 3936 / loss: 0.031100748106837273 / accuracy: 1.0\n",
      "epoch: 3937 / loss: 0.03109065070748329 / accuracy: 1.0\n",
      "epoch: 3938 / loss: 0.031080856919288635 / accuracy: 1.0\n",
      "epoch: 3939 / loss: 0.031070638447999954 / accuracy: 1.0\n",
      "epoch: 3940 / loss: 0.031060723587870598 / accuracy: 1.0\n",
      "epoch: 3941 / loss: 0.031050628051161766 / accuracy: 1.0\n",
      "epoch: 3942 / loss: 0.03104071132838726 / accuracy: 1.0\n",
      "epoch: 3943 / loss: 0.03103061579167843 / accuracy: 1.0\n",
      "epoch: 3944 / loss: 0.031020700931549072 / accuracy: 1.0\n",
      "epoch: 3945 / loss: 0.031010664999485016 / accuracy: 1.0\n",
      "epoch: 3946 / loss: 0.031000688672065735 / accuracy: 1.0\n",
      "epoch: 3947 / loss: 0.030990714207291603 / accuracy: 1.0\n",
      "epoch: 3948 / loss: 0.030980737879872322 / accuracy: 1.0\n",
      "epoch: 3949 / loss: 0.03097076341509819 / accuracy: 1.0\n",
      "epoch: 3950 / loss: 0.03096090815961361 / accuracy: 1.0\n",
      "epoch: 3951 / loss: 0.03095093183219433 / accuracy: 1.0\n",
      "epoch: 3952 / loss: 0.030941015109419823 / accuracy: 1.0\n",
      "epoch: 3953 / loss: 0.030931100249290466 / accuracy: 1.0\n",
      "epoch: 3954 / loss: 0.030921004712581635 / accuracy: 1.0\n",
      "epoch: 3955 / loss: 0.030911389738321304 / accuracy: 1.0\n",
      "epoch: 3956 / loss: 0.030901353806257248 / accuracy: 1.0\n",
      "epoch: 3957 / loss: 0.030891500413417816 / accuracy: 1.0\n",
      "epoch: 3958 / loss: 0.030881524085998535 / accuracy: 1.0\n",
      "epoch: 3959 / loss: 0.03087172843515873 / accuracy: 1.0\n",
      "epoch: 3960 / loss: 0.030861694365739822 / accuracy: 1.0\n",
      "epoch: 3961 / loss: 0.030851837247610092 / accuracy: 1.0\n",
      "epoch: 3962 / loss: 0.030841922387480736 / accuracy: 1.0\n",
      "epoch: 3963 / loss: 0.03083224967122078 / accuracy: 1.0\n",
      "epoch: 3964 / loss: 0.030822213739156723 / accuracy: 1.0\n",
      "epoch: 3965 / loss: 0.030812358483672142 / accuracy: 1.0\n",
      "epoch: 3966 / loss: 0.03080250322818756 / accuracy: 1.0\n",
      "epoch: 3967 / loss: 0.03079276904463768 / accuracy: 1.0\n",
      "epoch: 3968 / loss: 0.030782852321863174 / accuracy: 1.0\n",
      "epoch: 3969 / loss: 0.030773060396313667 / accuracy: 1.0\n",
      "epoch: 3970 / loss: 0.030763205140829086 / accuracy: 1.0\n",
      "epoch: 3971 / loss: 0.03075353056192398 / accuracy: 1.0\n",
      "epoch: 3972 / loss: 0.03074355609714985 / accuracy: 1.0\n",
      "epoch: 3973 / loss: 0.030733700841665268 / accuracy: 1.0\n",
      "epoch: 3974 / loss: 0.030723966658115387 / accuracy: 1.0\n",
      "epoch: 3975 / loss: 0.03071417100727558 / accuracy: 1.0\n",
      "epoch: 3976 / loss: 0.030704375356435776 / accuracy: 1.0\n",
      "epoch: 3977 / loss: 0.030694641172885895 / accuracy: 1.0\n",
      "epoch: 3978 / loss: 0.030684787780046463 / accuracy: 1.0\n",
      "epoch: 3979 / loss: 0.030675053596496582 / accuracy: 1.0\n",
      "epoch: 3980 / loss: 0.030665379017591476 / accuracy: 1.0\n",
      "epoch: 3981 / loss: 0.03065546415746212 / accuracy: 1.0\n",
      "epoch: 3982 / loss: 0.03064584918320179 / accuracy: 1.0\n",
      "epoch: 3983 / loss: 0.03063599392771721 / accuracy: 1.0\n",
      "epoch: 3984 / loss: 0.030626259744167328 / accuracy: 1.0\n",
      "epoch: 3985 / loss: 0.030616648495197296 / accuracy: 1.0\n",
      "epoch: 3986 / loss: 0.030606914311647415 / accuracy: 1.0\n",
      "epoch: 3987 / loss: 0.030597178265452385 / accuracy: 1.0\n",
      "epoch: 3988 / loss: 0.030587445944547653 / accuracy: 1.0\n",
      "epoch: 3989 / loss: 0.030577709898352623 / accuracy: 1.0\n",
      "epoch: 3990 / loss: 0.030568037182092667 / accuracy: 1.0\n",
      "epoch: 3991 / loss: 0.030558301135897636 / accuracy: 1.0\n",
      "epoch: 3992 / loss: 0.03054862841963768 / accuracy: 1.0\n",
      "epoch: 3993 / loss: 0.0305388942360878 / accuracy: 1.0\n",
      "epoch: 3994 / loss: 0.03052927926182747 / accuracy: 1.0\n",
      "epoch: 3995 / loss: 0.030519606545567513 / accuracy: 1.0\n",
      "epoch: 3996 / loss: 0.03050999343395233 / accuracy: 1.0\n",
      "epoch: 3997 / loss: 0.0305002573877573 / accuracy: 1.0\n",
      "epoch: 3998 / loss: 0.030490705743432045 / accuracy: 1.0\n",
      "epoch: 3999 / loss: 0.030480969697237015 / accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X,\n",
    "                    Y,\n",
    "                    epochs=4000,\n",
    "                    batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00621678],\n",
       "       [ 0.99318296],\n",
       "       [ 0.99315345],\n",
       "       [ 0.01047902]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd67c716e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.show_graph_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd67c276ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.show_graph_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
